# Base configuration for SBIR ETL Pipeline
# This file contains default settings that are version controlled
# Environment-specific overrides go in dev.yaml, prod.yaml, etc.

pipeline:
  name: "sbir-etl"
  version: "0.1.0"
  environment: "development" # overridden by environment

data_quality:
  sbir_awards:
    # SBIR-specific validation thresholds
    pass_rate_threshold: 0.95 # 95% of records must pass validation
    completeness_threshold: 0.90 # 90% completeness for individual fields
    uniqueness_threshold: 0.99 # 99% unique Contract IDs (allowing phase progressions)

  uspto_patents:
    # USPTO Patent Assignment validation thresholds
    pass_rate_threshold: 0.99 # 99% of records must pass validation
    completeness_threshold: 0.95 # 95% completeness for critical fields
    uniqueness_threshold: 0.98 # 98% unique grant document numbers

  # Completeness thresholds (percentage of non-null values required)
  completeness:
    award_id: 1.00 # 100% required
    company_name: 0.95 # 95% required
    award_amount: 0.90 # 90% required
    award_date: 0.95 # 95% required
    program: 0.98 # 98% required

  # Uniqueness requirements (no duplicates allowed)
  uniqueness:
    award_id: 1.00 # No duplicate award IDs

  # Value range validation
  validity:
    award_amount_min: 0.0
    award_amount_max: 5000000.0 # $5M max SBIR award
    award_year_min: 1983 # SBIR program start
    award_year_max: 2030 # Future limit

  # Enrichment success rates
  enrichment:
    sam_gov_success_rate: 0.85 # 85% of companies should enrich successfully
    usaspending_match_rate: 0.70 # 70% of awards should match USAspending data
    regression_threshold_percent: 5.0 # Fail asset if match rate declines >5 percentage points

enrichment:
  performance:
    # Processing chunk size (records to process in memory at once)
    # Adjust based on available RAM and data characteristics
    # Higher = faster but more memory; Lower = slower but memory-efficient
    chunk_size: 25000 # Default: 10000-50000 depending on hardware

    # Memory threshold before triggering spill-to-disk (MB)
    # System will reduce chunk size or spill to disk if exceeded
    memory_threshold_mb: 2048 # Default: 2048 (2 GB)

    # Enrichment quality gate threshold
    # Asset fails if match_rate falls below this percentage
    match_rate_threshold: 0.70 # Default: 0.70 (70%)

    # Processing timeout per chunk (seconds)
    # Fails chunk if processing takes longer than this
    timeout_seconds: 300 # Default: 300 (5 minutes)

    # Retry backoff strategy for failed operations
    # Options: "fixed", "linear", "exponential"
    retry_backoff: exponential

    # Fuzzy matching confidence thresholds
    # Affects match quality and speed tradeoff
    high_confidence_threshold: 90 # Min score for high-confidence match
    low_confidence_threshold: 75 # Min score for low-confidence match

    # Enable/disable features
    enable_fuzzy_matching: true # Enable fuzzy name matching
    enable_memory_monitoring: true # Track memory usage
    enable_progress_tracking: true # Track per-chunk progress

    # Alert thresholds for performance/quality monitoring
    duration_warning_seconds: 5.0 # WARNING if > 5s per record
    memory_delta_warning_mb: 500.0 # WARNING if memory delta > 500MB
    memory_pressure_warn_percent: 80.0 # WARNING if memory > 80% available
    memory_pressure_critical_percent: 95.0 # CRITICAL if memory > 95% available
  sam_gov:
    # SAM.gov API configuration
    base_url: "https://api.sam.gov/entity-information/v3"
    api_key_env_var: "SAM_GOV_API_KEY" # pragma: allowlist secret
    rate_limit_per_minute: 60
    timeout_seconds: 30
    retry_attempts: 3
    retry_backoff_seconds: 1

  usaspending_api:
    # USAspending API configuration (for additional enrichment)
    base_url: "https://api.usaspending.gov/api/v2"
    timeout_seconds: 30
    retry_attempts: 3
    retry_backoff_seconds: 2

extraction:
  sbir:
    # SBIR CSV configuration
    csv_path: "data/raw/sbir/awards_data.csv"
    duckdb:
      database_path: ":memory:" # Use in-memory database (or path for persistent)
      table_name: "sbir_awards"
    batch_size: 10000 # Chunk size for processing
    encoding: "utf-8"

  usaspending:
    # USAspending PostgreSQL dump configuration
    database_name: "usaspending"
    table_name: "awards"
    # DuckDB will import the PostgreSQL dump file
    import_chunk_size: 50000

  uspto:
    # USPTO Patent Assignment Data configuration
    csv_path: "data/raw/uspto/patent_assignments.csv"
    batch_size: 5000 # Chunk size for extraction
    encoding: "utf-8"
    sample_limit: null # Set to integer to limit extraction for testing (null = no limit)

    # Extraction quality thresholds
    quality:
      min_fields_required: 8 # Minimum required fields per record
      max_malformed_percentage: 0.01 # Allow up to 1% malformed records

    # Transform output configuration
    transform_output_dir: "data/transformed/uspto"
    transform_batch_size: 1000
    normalize_entity_names: true
    detect_conveyances: true

validation:
  # Schema validation settings
  strict_schema: true
  fail_on_first_error: false # Collect all errors before failing

  # Quality check settings
  sample_size_for_checks: 1000 # Sample size for expensive checks
  max_error_percentage: 0.05 # 5% error rate allowed

transformation:
  # Business logic configuration
  company_deduplication:
    similarity_threshold: 0.85 # Fuzzy matching threshold
    min_company_name_length: 3

  award_normalization:
    currency: "USD"
    standardize_program_names: true
    program_mappings:
      "SBIR": "SBIR"
      "STTR": "STTR"
      "SBIR/STTR": "SBIR/STTR"

  graph_preparation:
    # Node and relationship preparation
    batch_size: 1000
    relationship_types:
      - "AWARDED_TO"
      - "FUNDED_BY"
      - "SUBMITTED_BY"

loading:
  neo4j:
    # Neo4j connection configuration
    uri_env_var: "NEO4J_URI"
    user_env_var: "NEO4J_USER"
    password_env_var: "NEO4J_PASSWORD" # pragma: allowlist secret
    output_dir: "data/loaded/uspto"
    load_success_threshold: 0.99 # 99% success rate required
    create_indexes: true
    create_constraints: true
    batch_operations: true

    # Batch loading configuration
    batch_size: 1000
    parallel_threads: 4

    # Transaction settings
    transaction_timeout_seconds: 300
    retry_on_deadlock: true
    max_deadlock_retries: 3

logging:
  # Logging configuration
  level: "INFO"
  format: "json" # "json" or "pretty"
  file_path: "logs/sbir-etl.log"
  max_file_size_mb: 100
  backup_count: 5

  # Context variables
  include_stage: true
  include_run_id: true
  include_timestamps: true

metrics:
  # Metrics collection
  enabled: true
  collection_interval_seconds: 30
  persist_to_file: true
  metrics_file_path: "logs/metrics.json"

  # Performance thresholds
  warning_thresholds:
    stage_duration_seconds: 3600 # 1 hour
    memory_usage_mb: 2048 # 2GB
    error_rate_percentage: 5.0

duckdb:
  # DuckDB configuration
  database_path: "data/processed/sbir.duckdb"
  memory_limit_gb: 4
  threads: 4

  # Query optimization
  enable_object_cache: true
  enable_query_profiler: false

transition:
  # Transition detection & analytics configuration
  contracts:
    # Coverage thresholds for contracts_sample asset
    date_coverage_min: 0.90 # ≥90% rows must have action_date
    ident_coverage_min: 0.60 # ≥60% rows must have any of UEI|DUNS|PIID|FAIN
    sample_size_min: 1000 # Expected sample size lower bound
    sample_size_max: 10000 # Expected sample size upper bound
    enforce_sample_size: false # Enforce sample size gates (default off for tiny runs)

  analytics:
    # Thresholds for analytics computations and CI gates
    score_threshold: 0.60 # Minimum score to count as transitioned
    min_award_rate: 0.0 # Optional CI gate: minimum award transition rate
    min_company_rate: 0.0 # Optional CI gate: minimum company transition rate

  detections:
    # Quality thresholds for consolidated detections
    min_valid_rate: 0.99 # ≥99% rows valid (required fields + score bounds)
    min_highconf_rate: 0.0 # Optional rate of high-confidence (score ≥ threshold)

  vendor_resolution:
    # Vendor resolution quality thresholds and fuzzy config
    min_rate: 0.60 # Minimum resolution rate (uei/duns/fuzzy)
    fuzzy_threshold: 0.85 # RapidFuzz partial_ratio threshold for name_fuzzy

statistical_reporting:
  # Report generation settings
  generation:
    enabled: true
    formats: ["html", "json", "markdown", "executive"]
    output_directory: "reports/statistical"
    template_directory: "templates/reports"
    
  # Module-specific reporting
  modules:
    sbir_enrichment:
      enabled: true
      include_coverage_analysis: true
      include_source_breakdown: true
    patent_analysis:
      enabled: true
      include_validation_details: true
      include_loading_statistics: true
    cet_classification:
      enabled: true
      include_confidence_distribution: true
      include_taxonomy_breakdown: true
    transition_detection:
      enabled: true
      include_success_stories: true
      include_trend_analysis: true
      
  # Insight generation
  insights:
    anomaly_detection:
      enabled: true
      sensitivity: "medium"  # low, medium, high
      lookback_periods: 5
    recommendations:
      enabled: true
      include_actionable_steps: true
    success_stories:
      enabled: true
      min_impact_threshold: 0.8
      
  # Output format configuration
  formats:
    html:
      include_interactive_charts: true
      chart_library: "plotly"
      theme: "default"
    json:
      include_raw_data: false
      pretty_print: true
    markdown:
      max_length: 2000
      include_links: true
    executive:
      include_visualizations: true
      focus_areas: ["impact", "quality", "trends"]
      
  # CI/CD integration
  cicd:
    github_actions:
      enabled: true
      upload_artifacts: true
      post_pr_comments: true
      artifact_retention_days: 30
    report_comparison:
      enabled: true
      baseline_comparison: true
      trend_analysis_periods: [7, 30, 90]
      
  # Quality thresholds for reporting
  quality_thresholds:
    data_completeness_warning: 0.90
    data_completeness_error: 0.80
    enrichment_success_warning: 0.85
    enrichment_success_error: 0.70
    performance_degradation_warning: 1.5  # 50% slower
    performance_degradation_error: 2.0    # 100% slower
