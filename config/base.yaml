# Base configuration for SBIR ETL Pipeline
# This file contains default settings that are version controlled
# Environment-specific overrides go in dev.yaml, prod.yaml, etc.

pipeline:
  name: "sbir-etl"
  version: "0.1.0"
  environment: "development" # overridden by environment

# File system paths configuration
# All paths are relative to project root by default
# Override via environment variables: SBIR_ETL__PATHS__<KEY>
# Examples:
#   export SBIR_ETL__PATHS__DATA_ROOT=/mnt/data
#   export SBIR_ETL__PATHS__USASPENDING_DUMP_FILE=/path/to/dump.zip
paths:
  # Root data directory (all other paths relative to this)
  data_root: "data"
  raw_data: "data/raw"

  # USAspending database dumps
  usaspending_dump_dir: "data/usaspending"
  usaspending_dump_file: "data/usaspending/usaspending-db_20251006.zip"

  # Transition detection outputs
  transition_contracts_output: "data/transition/contracts_ingestion.parquet"
  transition_dump_dir: "data/transition/pruned_data_store_api_dump"
  transition_vendor_filters: "data/transition/sbir_vendor_filters.json"

  # Scripts output
  scripts_output: "data/scripts_output"

data_quality:
  sbir_awards:
    # SBIR-specific validation thresholds
    pass_rate_threshold: 0.95 # 95% of records must pass validation
    completeness_threshold: 0.90 # 90% completeness for individual fields
    uniqueness_threshold: 0.99 # 99% unique Contract IDs (allowing phase progressions)

  uspto_patents:
    # USPTO Patent Assignment validation thresholds
    pass_rate_threshold: 0.99 # 99% of records must pass validation
    completeness_threshold: 0.95 # 95% completeness for critical fields
    uniqueness_threshold: 0.98 # 98% unique grant document numbers

  # Completeness thresholds (percentage of non-null values required)
  completeness:
    award_id: 1.00 # 100% required
    company_name: 0.95 # 95% required
    award_amount: 0.90 # 90% required
    award_date: 0.95 # 95% required
    program: 0.98 # 98% required

  # Uniqueness requirements (no duplicates allowed)
  uniqueness:
    award_id: 1.00 # No duplicate award IDs

  # Value range validation
  validity:
    award_amount_min: 0.0
    award_amount_max: 5000000.0 # $5M max SBIR award
    award_year_min: 1983 # SBIR program start
    award_year_max: 2030 # Future limit

  # Enrichment success rates
  enrichment:
    sam_gov_success_rate: 0.85 # 85% of companies should enrich successfully
    usaspending_match_rate: 0.70 # 70% of awards should match USAspending data
    regression_threshold_percent: 0.05 # Fail asset if match rate declines >5 percentage points

enrichment:
  performance:
    # Processing chunk size (records to process in memory at once)
    # Adjust based on available RAM and data characteristics
    # Higher = faster but more memory; Lower = slower but memory-efficient
    chunk_size: 25000 # Default: 10000-50000 depending on hardware

    # Memory threshold before triggering spill-to-disk (MB)
    # System will reduce chunk size or spill to disk if exceeded
    memory_threshold_mb: 2048 # Default: 2048 (2 GB)

    # Enrichment quality gate threshold
    # Asset fails if match_rate falls below this percentage
    match_rate_threshold: 0.70 # Default: 0.70 (70%)

    # Processing timeout per chunk (seconds)
    # Fails chunk if processing takes longer than this
    timeout_seconds: 300 # Default: 300 (5 minutes)

    # Retry backoff strategy for failed operations
    # Options: "fixed", "linear", "exponential"
    retry_backoff: exponential

    # Fuzzy matching confidence thresholds
    # Affects match quality and speed tradeoff
    high_confidence_threshold: 90 # Min score for high-confidence match
    low_confidence_threshold: 75 # Min score for low-confidence match

    # Enable/disable features
    enable_fuzzy_matching: true # Enable fuzzy name matching
    enable_memory_monitoring: true # Track memory usage
    enable_progress_tracking: true # Track per-chunk progress

    # Alert thresholds for performance/quality monitoring
    duration_warning_seconds: 5.0 # WARNING if > 5s per record
    memory_delta_warning_mb: 500.0 # WARNING if memory delta > 500MB
    memory_pressure_warn_percent: 80.0 # WARNING if memory > 80% available
    memory_pressure_critical_percent: 95.0 # CRITICAL if memory > 95% available

  # Enhanced matching features for company name data cleaning
  enhanced_matching:
    # Phonetic matching - catches misspellings that sound similar
    # Examples: "Smyth" vs "Smith", "Mikrosystems" vs "Microsystems"
    enable_phonetic_matching: false # Enable phonetic (sound-alike) matching
    phonetic_algorithm: "metaphone" # Options: "metaphone", "double_metaphone", "soundex"
    phonetic_boost: 5 # Score boost (0-10) when phonetic codes match

    # Jaro-Winkler matching - gives extra weight to matching prefixes
    # Particularly good for company names where first word is distinctive
    enable_jaro_winkler: false # Enable Jaro-Winkler distance metric
    jaro_winkler_prefix_weight: 0.1 # Weight for matching prefix (0.0-0.25)
    jaro_winkler_threshold: 90 # Min score to consider a match (0-100)
    jaro_winkler_use_as_primary: false # Use Jaro-Winkler as primary scorer (instead of token_set_ratio)

    # Enhanced abbreviations - normalizes common terms to standard abbreviations
    # Examples: "Technologies" -> "tech", "International" -> "intl"
    enable_enhanced_abbreviations: false # Enable enhanced abbreviation dictionary
    custom_abbreviations: {} # Additional custom abbreviations (dict format)

  # Researcher matching configuration
  researcher_matching:
    # ORCID-first matching strategy
    enable_orcid_matching: true # Match by ORCID identifier (highest confidence)
    orcid_confidence: 100 # Confidence score for ORCID matches

    # Email matching
    enable_email_matching: true # Match by email address
    email_confidence: 95 # Confidence score for email matches

    # Affiliation + last name matching
    enable_affiliation_matching: true # Match by affiliation + last name
    affiliation_confidence: 80 # Confidence score for affiliation matches
  sam_gov:
    # SAM.gov API configuration
    base_url: "https://api.sam.gov/entity-information/v3"
    api_key_env_var: "SAM_GOV_API_KEY" # pragma: allowlist secret
    rate_limit_per_minute: 60
    timeout_seconds: 30
    retry_attempts: 3
    retry_backoff_seconds: 1

  usaspending_api:
    # USAspending API configuration (for additional enrichment)
    base_url: "https://api.usaspending.gov/api/v2"
    timeout_seconds: 30
    retry_attempts: 3
    retry_backoff_seconds: 2

  patentsview_api:
    # PatentsView API configuration (PatentSearch API)
    base_url: "https://search.patentsview.org/api"
    api_key_env_var: "PATENTSVIEW_API_KEY" # pragma: allowlist secret
    rate_limit_per_minute: 60  # Conservative default for polite-use policy
    timeout_seconds: 30
    retry_attempts: 3
    retry_backoff_seconds: 2
    # API response caching
    cache:
      enabled: true  # Enable caching of API responses
      cache_dir: "data/cache/patentsview"  # Directory for cache files
      ttl_hours: 24  # Time-to-live for cache entries (hours)

  enrichment_refresh:
    # Iterative refresh configuration for enrichment APIs
    # Phase 1: USAspending API only. Other APIs (SAM.gov, NIH RePORTER, PatentsView, etc.) will be evaluated in Phase 2+
    usaspending:
      # Refresh cadence and SLA
      cadence_days: 1  # Daily refresh
      sla_staleness_days: 1  # Max 24 hours old before considered stale

      # Batch processing configuration
      batch_size: 100  # Awards per batch
      max_concurrent_requests: 5  # Rate limit respect
      rate_limit_per_minute: 120  # Conservative default based on API documentation

      # Delta detection
      enable_delta_detection: true  # Skip unchanged records based on payload hash
      hash_algorithm: "sha256"  # Algorithm for payload hashing

      # Retry configuration
      retry_attempts: 3
      retry_backoff_seconds: 2
      retry_backoff_multiplier: 2.0  # Exponential backoff multiplier

      # Timeout configuration
      timeout_seconds: 30
      connection_timeout_seconds: 10

      # State management
      checkpoint_interval: 50  # Save checkpoint every N records
      state_file: "data/state/enrichment_refresh_state.json"

      # Metrics
      enable_metrics: true
      metrics_file: "reports/metrics/enrichment_freshness.json"

      # API response caching
      cache:
        enabled: true  # Enable caching of API responses
        cache_dir: "data/cache/usaspending"  # Directory for cache files
        ttl_hours: 24  # Time-to-live for cache entries (hours)

extraction:
  sbir:
    # SBIR CSV configuration
    csv_path: "data/raw/sbir/awards_data.csv"
    duckdb:
      database_path: ":memory:" # Use in-memory database (or path for persistent)
      table_name: "sbir_awards"
    batch_size: 10000 # Chunk size for processing
    encoding: "utf-8"

  usaspending:
    # USAspending PostgreSQL dump configuration
    database_name: "usaspending"
    table_name: "awards"
    # DuckDB will import the PostgreSQL dump file
    import_chunk_size: 50000

  uspto:
    # USPTO Patent Assignment Data configuration
    csv_path: "data/raw/uspto/patent_assignments.csv"
    batch_size: 5000 # Chunk size for extraction
    encoding: "utf-8"
    sample_limit: null # Set to integer to limit extraction for testing (null = no limit)

    # Extraction quality thresholds
    quality:
      min_fields_required: 8 # Minimum required fields per record
      max_malformed_percentage: 0.01 # Allow up to 1% malformed records

    # Transform output configuration
    transform_output_dir: "data/transformed/uspto"
    transform_batch_size: 1000
    normalize_entity_names: true
    detect_conveyances: true

validation:
  # Schema validation settings
  strict_schema: true
  fail_on_first_error: false # Collect all errors before failing

  # Quality check settings
  sample_size_for_checks: 1000 # Sample size for expensive checks
  max_error_percentage: 0.05 # 5% error rate allowed

transformation:
  # Business logic configuration
  company_deduplication:
    similarity_threshold: 0.85 # Fuzzy matching threshold
    min_company_name_length: 3

  award_normalization:
    currency: "USD"
    standardize_program_names: true
    program_mappings:
      "SBIR": "SBIR"
      "STTR": "STTR"
      "SBIR/STTR": "SBIR/STTR"

  graph_preparation:
    # Node and relationship preparation
    batch_size: 1000
    relationship_types:
      - "AWARDED_TO"
      - "FUNDED_BY"
      - "SUBMITTED_BY"

# Company categorization configuration
company_categorization:
  # Classification thresholds
  product_leaning_pct: 51.0 # 51% threshold for Product-leaning
  service_leaning_pct: 51.0 # 51% threshold for Service-leaning
  rd_leaning_pct: 51.0 # 51% threshold for R&D-leaning
  psc_family_diversity_threshold: 6 # >6 PSC families → Mixed

  # Confidence levels
  low_max_awards: 2 # <=2 awards → Low confidence
  medium_max_awards: 5 # 2-5 awards → Medium confidence
  # >5 awards → High confidence

  # Processing
  batch_size: 100 # Companies per batch
  parallel_workers: 4 # Parallel processing workers

  # USAspending query settings
  usaspending_table_name: "usaspending_awards"
  usaspending_timeout_seconds: 30
  usaspending_retry_attempts: 3

  # Output options
  include_contract_details: true
  include_metadata: true

loading:
  neo4j:
    # Neo4j connection configuration
    uri_env_var: "NEO4J_URI"
    user_env_var: "NEO4J_USER"
    password_env_var: "NEO4J_PASSWORD" # pragma: allowlist secret
    output_dir: "data/loaded/uspto"
    load_success_threshold: 0.99 # 99% success rate required
    create_indexes: true
    create_constraints: true
    batch_operations: true

    # Batch loading configuration
    batch_size: 1000
    parallel_threads: 4

    # Transaction settings
    transaction_timeout_seconds: 300
    retry_on_deadlock: true
    max_deadlock_retries: 3

logging:
  # Logging configuration
  level: "INFO"
  format: "json" # "json" or "pretty"
  file_path: "logs/sbir-etl.log"
  max_file_size_mb: 100
  backup_count: 5

  # Context variables
  include_stage: true
  include_run_id: true
  include_timestamps: true

metrics:
  # Metrics collection
  enabled: true
  collection_interval_seconds: 30
  persist_to_file: true
  metrics_file_path: "logs/metrics.json"

  # Performance thresholds
  warning_thresholds:
    stage_duration_seconds: 3600 # 1 hour
    memory_usage_mb: 2048 # 2GB
    error_rate_percentage: 5.0

duckdb:
  # DuckDB configuration
  database_path: "data/processed/sbir.duckdb"
  memory_limit_gb: 4
  threads: 4

  # Query optimization
  enable_object_cache: true
  enable_query_profiler: false

transition:
  # Transition detection & analytics configuration
  contracts:
    # Coverage thresholds for contracts_sample asset
    date_coverage_min: 0.90 # ≥90% rows must have action_date
    ident_coverage_min: 0.60 # ≥60% rows must have any of UEI|DUNS|PIID|FAIN
    sample_size_min: 1000 # Expected sample size lower bound
    sample_size_max: 10000 # Expected sample size upper bound
    enforce_sample_size: false # Enforce sample size gates (default off for tiny runs)

  analytics:
    # Thresholds for analytics computations and CI gates
    score_threshold: 0.60 # Minimum score to count as transitioned
    min_award_rate: 0.0 # Optional CI gate: minimum award transition rate
    min_company_rate: 0.0 # Optional CI gate: minimum company transition rate

  detections:
    # Quality thresholds for consolidated detections
    min_valid_rate: 0.99 # ≥99% rows valid (required fields + score bounds)
    min_highconf_rate: 0.0 # Optional rate of high-confidence (score ≥ threshold)

  vendor_resolution:
    # Vendor resolution quality thresholds and fuzzy config
    min_rate: 0.60 # Minimum resolution rate (uei/duns/fuzzy)
    fuzzy_threshold: 0.85 # RapidFuzz partial_ratio threshold for name_fuzzy

statistical_reporting:
  # Report generation settings
  generation:
    enabled: true
    formats: ["html", "json", "markdown", "executive"]
    output_directory: "reports/statistical"
    template_directory: "templates/reports"

  # Module-specific reporting
  modules:
    sbir_enrichment:
      enabled: true
      include_coverage_analysis: true
      include_source_breakdown: true
    patent_analysis:
      enabled: true
      include_validation_details: true
      include_loading_statistics: true
    cet_classification:
      enabled: true
      include_confidence_distribution: true
      include_taxonomy_breakdown: true
    transition_detection:
      enabled: true
      include_success_stories: true
      include_trend_analysis: true

  # Insight generation
  insights:
    anomaly_detection:
      enabled: true
      sensitivity: "medium"  # low, medium, high
      lookback_periods: 5
    recommendations:
      enabled: true
      include_actionable_steps: true
    success_stories:
      enabled: true
      min_impact_threshold: 0.8

  # Output format configuration
  formats:
    html:
      include_interactive_charts: true
      chart_library: "plotly"
      theme: "default"
    json:
      include_raw_data: false
      pretty_print: true
    markdown:
      max_length: 2000
      include_links: true
    executive:
      include_visualizations: true
      focus_areas: ["impact", "quality", "trends"]

  # CI/CD integration
  cicd:
    github_actions:
      enabled: true
      upload_artifacts: true
      post_pr_comments: true
      artifact_retention_days: 30
    report_comparison:
      enabled: true
      baseline_comparison: true
      trend_analysis_periods: [7, 30, 90]

  # Quality thresholds for reporting
  quality_thresholds:
    data_completeness_warning: 0.90
    data_completeness_error: 0.80
    enrichment_success_warning: 0.85
    enrichment_success_error: 0.70
    performance_degradation_warning: 1.5  # 50% slower
    performance_degradation_error: 2.0    # 100% slower

fiscal_analysis:
  # Base analysis parameters
  base_year: 2023
  inflation_source: "bea_gdp_deflator"
  naics_crosswalk_version: "2017"
  stateio_model_version: "v2.1"

  # NAICS-to-BEA sector mapping configuration
  naics_to_bea:
    crosswalk_version: "2017"
    crosswalk_path: "data/reference/bea/naics_to_bea_crosswalk_2017.csv"
    fallback_path: "config/fiscal/naics_bea_mappings.yaml"
    min_confidence_threshold: 0.50
    enable_weighted_allocation: true
    hierarchical_fallback: true  # 6→4→3→2 digit

  # Tax calculation parameters
  tax_parameters:
    # Individual income tax parameters
    individual_income_tax:
      effective_rate: 0.22  # Average effective federal income tax rate
      progressive_rates:
        "10_percent": 0.10
        "12_percent": 0.12
        "22_percent": 0.22
        "24_percent": 0.24
        "32_percent": 0.32
        "35_percent": 0.35
        "37_percent": 0.37
      standard_deduction: 13850  # 2023 standard deduction (single)

    # Payroll tax parameters
    payroll_tax:
      social_security_rate: 0.062  # Employee portion
      medicare_rate: 0.0145  # Employee portion
      unemployment_rate: 0.006  # FUTA rate
      wage_base_limit: 160200  # 2023 Social Security wage base

    # Corporate income tax parameters
    corporate_income_tax:
      federal_rate: 0.21  # Federal corporate tax rate
      effective_rate: 0.18  # Average effective rate accounting for deductions

    # Excise tax parameters
    excise_tax:
      fuel_tax_rate: 0.184  # Federal gasoline tax per gallon
      general_rate: 0.03  # General excise tax rate on goods

  # Sensitivity analysis parameters
  sensitivity_parameters:
    # Parameter sweep configuration
    parameter_sweep:
      enabled: true
      method: "monte_carlo"  # monte_carlo, latin_hypercube, grid_search
      num_scenarios: 1000
      random_seed: 42

    # Uncertainty parameters
    uncertainty_parameters:
      tax_rates:
        variation_percent: 0.10  # ±10% variation
        distribution: "normal"
      multipliers:
        variation_percent: 0.15  # ±15% variation
        distribution: "normal"
      inflation_adjustment:
        variation_percent: 0.05  # ±5% variation
        distribution: "normal"

    # Confidence interval configuration
    confidence_intervals:
      levels: [0.90, 0.95, 0.99]  # 90%, 95%, 99% confidence intervals
      method: "percentile"  # percentile, bootstrap
      bootstrap_samples: 1000

    # Performance thresholds
    performance:
      max_scenarios_parallel: 10
      timeout_seconds: 3600  # 1 hour timeout
      memory_limit_gb: 8

  # Data quality thresholds
  quality_thresholds:
    naics_coverage_rate: 0.85  # 85% of awards must have NAICS codes
    geographic_resolution_rate: 0.90  # 90% must resolve to state level
    inflation_adjustment_success: 0.95  # 95% must have valid inflation data
    bea_sector_mapping_rate: 0.90  # 90% must map to BEA sectors

  # Performance configuration
  performance:
    chunk_size: 10000
    parallel_processing: true
    max_workers: 4
    memory_limit_gb: 4
    timeout_seconds: 1800  # 30 minutes

  # Output configuration
  output:
    formats: ["json", "csv", "html"]
    include_audit_trail: true
    include_sensitivity_analysis: true
    output_directory: "reports/fiscal_returns"

# CLI interface configuration
cli:
  # Display settings
  theme: "default"  # "default", "dark", "light"
  progress_refresh_rate: 0.1  # seconds
  dashboard_refresh_rate: 10  # seconds

  # Output settings
  max_table_rows: 50
  truncate_long_text: true
  show_timestamps: true

  # Performance settings
  api_timeout_seconds: 30
  max_concurrent_operations: 4
  cache_metrics_seconds: 60
