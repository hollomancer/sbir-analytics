import os
import yaml
from pathlib import Path
from typing import List

import pandas as pd
import pytest

from src.models.cet_models import CETArea
from src.ml.config.taxonomy_loader import TaxonomyLoader
import src.assets.cet_assets as cet_assets


def _write_yaml(path: Path, data: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        yaml.safe_dump(data, f)


def test_taxonomy_loader_loads_and_reads_configs(tmp_path: Path) -> None:
    """
    Create a temporary config/cet directory with taxonomy.yaml and classification.yaml,
    then validate TaxonomyLoader can read and expose expected values.
    """
    config_dir = tmp_path / "config" / "cet"
    config_dir.mkdir(parents=True, exist_ok=True)

    # Minimal taxonomy with two CET areas (the loader warns if !=21 but does not fail)
    taxonomy_yaml = {
        "version": "TEST-2025Q1",
        "last_updated": "2025-01-01",
        "description": "Test taxonomy",
        "cet_areas": [
            {
                "cet_id": "artificial_intelligence",
                "name": "Artificial Intelligence",
                "definition": "AI definition",
                "keywords": ["ai", "machine learning"],
                "parent_cet_id": None,
            },
            {
                "cet_id": "quantum_information_science",
                "name": "Quantum Info Science",
                "definition": "Quantum definition",
                "keywords": ["quantum", "qubits"],
                "parent_cet_id": None,
            },
        ],
    }

    # Minimal classification.yaml satisfying required keys for the Pydantic model
    classification_yaml = {
        "model_version": "v0-test",
        "created_date": "2025-01-01",
        "confidence_thresholds": {"high": 70.0, "medium": 40.0, "low": 0.0},
        "tfidf": {"max_features": 100},
        "logistic_regression": {"C": 1.0},
        "calibration": {"method": "sigmoid", "cv": 3},
        "feature_selection": {"enabled": False},
        "evidence": {"max_statements": 3},
        "supporting": {"max_supporting_areas": 3},
        "batch": {"size": 100},
        "performance": {},
        "quality": {},
        "analytics": {},
    }

    _write_yaml(config_dir / "taxonomy.yaml", taxonomy_yaml)
    _write_yaml(config_dir / "classification.yaml", classification_yaml)

    # Instantiate loader with the temp config dir
    loader = TaxonomyLoader(config_dir=config_dir)
    taxonomy = loader.load_taxonomy()

    # Basic assertions about taxonomy content
    assert taxonomy.version == "TEST-2025Q1"
    assert taxonomy.last_updated == "2025-01-01"
    assert taxonomy.description == "Test taxonomy"
    assert len(taxonomy.cet_areas) == 2

    # get_all_cet_ids and get_cet_keywords should behave as expected
    ids = loader.get_all_cet_ids()
    assert set(ids) == {"artificial_intelligence", "quantum_information_science"}

    kw_ai = loader.get_cet_keywords("artificial_intelligence")
    assert "ai" in kw_ai and "machine learning" in kw_ai

    # Classification config
    class_cfg = loader.load_classification_config()
    assert class_cfg.model_version == "v0-test"
    assert class_cfg.confidence_thresholds["high"] == 70.0


def test_cet_taxonomy_asset_writes_parquet(tmp_path: Path, monkeypatch) -> None:
    """
    Test the `cet_taxonomy` asset writes a parquet file and contains expected rows.

    This test monkeypatches the TaxonomyLoader used in the asset module so the asset
    can run in isolation and write to a temp directory.
    """

    # Create a few CETArea instances to simulate a loaded taxonomy
    cet_areas: List[CETArea] = [
        CETArea(
            cet_id="artificial_intelligence",
            name="Artificial Intelligence",
            definition="AI definition",
            keywords=["ai", "ml"],
            parent_cet_id=None,
            taxonomy_version="TEST-2025Q1",
        ),
        CETArea(
            cet_id="quantum_information_science",
            name="Quantum Information Science",
            definition="QIS definition",
            keywords=["quantum", "qubits"],
            parent_cet_id=None,
            taxonomy_version="TEST-2025Q1",
        ),
        CETArea(
            cet_id="advanced_manufacturing",
            name="Advanced Manufacturing",
            definition="Manufacturing definition",
            keywords=["3d printing", "additive manufacturing"],
            parent_cet_id=None,
            taxonomy_version="TEST-2025Q1",
        ),
    ]

    # Create a fake loader class that returns an object with the attributes used by the asset
    class FakeTaxonomy:
        def __init__(
            self, version: str, last_updated: str, description: str, cet_areas: List[CETArea]
        ):
            self.version = version
            self.last_updated = last_updated
            self.description = description
            self.cet_areas = cet_areas

    class FakeLoader:
        def __init__(self, *args, **kwargs):
            # ignore args; keep simple
            pass

        def load_taxonomy(self):
            return FakeTaxonomy(
                version="TEST-2025Q1",
                last_updated="2025-01-01",
                description="Fake taxonomy for tests",
                cet_areas=cet_areas,
            )

    # Monkeypatch the TaxonomyLoader in the asset module to our fake loader
    monkeypatch.setattr(cet_assets, "TaxonomyLoader", FakeLoader)

    # Redirect DEFAULT_OUTPUT_PATH to a temp location so we don't write to repo paths
    temp_output = tmp_path / "data" / "processed" / "cet_taxonomy.parquet"
    monkeypatch.setattr(cet_assets, "DEFAULT_OUTPUT_PATH", temp_output)

    # Run the asset function. The decorated asset is a normal function that returns a dagster.Output
    result = cet_assets.cet_taxonomy()

    # The asset returns a dagster Output containing a string path as value (per implementation)
    # We expect the file (or a fallback) to have been written at temp_output
    assert temp_output.exists(), f"Expected parquet file at {temp_output} to exist"

    # Attempt to read the parquet file. In test environments where a parquet
    # engine (pyarrow/fastparquet) is not available, the asset writes a JSON
    # fallback next to the parquet path. Accept either format.
    try:
        df = pd.read_parquet(temp_output)
    except Exception:
        # Fallback to NDJSON (same basename, .json) produced by the asset when
        # parquet engines are missing. Also allow reading the companion checks JSON
        # if present for basic validation.
        json_fallback = temp_output.with_suffix(".json")
        checks_fallback = temp_output.with_suffix(".checks.json")
        if json_fallback.exists():
            df = pd.read_json(json_fallback, orient="records", lines=True)
        elif checks_fallback.exists():
            # The checks JSON contains metadata rather than rows; validate basic keys
            with open(checks_fallback, "r", encoding="utf-8") as fh:
                checks = json.load(fh)
            # Synthesize a minimal DataFrame for assertions from checks if possible
            total = (
                checks.get("total_areas") or checks.get("cet_count") or checks.get("total", None)
            )
            if total is None:
                # Not much we can assert about rows, but ensure checks file is present
                pytest.skip("Parquet unavailable; checks JSON present but does not expose row data")
            df = pd.DataFrame([{"cet_id": f"dummy_{i}"} for i in range(total)])
        else:
            # No readable fallback found â€” re-raise the original error to expose environment problem
            raise

    assert "cet_id" in df.columns
    assert len(df) == len(cet_areas)
    # Check key columns exist
    for col in ["cet_id", "name", "definition", "keywords", "keywords_joined", "taxonomy_version"]:
        assert col in df.columns

    # Verify that one known CET ID is present
    assert "artificial_intelligence" in df["cet_id"].values
