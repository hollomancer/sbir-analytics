# Test Suite Documentation

## Overview
This repository contains a comprehensive test suite for the **SBIR Analytics** project.  The tests are organized under the `tests/` directory and are split into three main groups:

- **unit** – Fast, isolated tests that focus on individual functions and models.
- **integration** – Tests that interact with external services (e.g., Neo4j) or larger data flows.
- **e2e / slow** – End‑to‑end scenarios that run the full pipeline (run on CI only).

## Running the Tests
```bash
# Ensure the development environment is set up (Python 3.11)
source .venv/bin/activate

# Run the entire test suite
python -m pytest

# Run only unit tests (fast)
python -m pytest tests/unit

# Run a specific test file
python -m pytest tests/unit/test_sbir_extractor.py -k "rawaward"
```

The helper script `scripts/test_new_tests.sh` also runs the new tests inside the virtual environment.

## Test Utilities
### Factories (`tests/factories.py`)
We use **custom factories** to generate realistic model instances without pulling data from CSV files.

**Core Models:**
```python
from tests.factories import RawAwardFactory, AwardFactory
raw = RawAwardFactory.create(award_amount="1,000,000.00")
award = raw.to_award()
```

**ML Models:**
```python
from tests.factories import CETClassificationFactory, CETAssessmentFactory
classification = CETClassificationFactory.create(score=85.0, primary=True)
assessment = CETAssessmentFactory.create(primary_cet=classification)
```

The factories provide sensible defaults and handle data cleaning.

### Custom Assertions (`tests/assertions.py`)
Common validation logic lives here to keep tests DRY:
- `assert_valid_award(award)` – checks required fields and value types.
- `assert_award_fields_equal(actual, expected)` – deep field‑by‑field comparison.
- `assert_dict_subset(subset, superset)` – useful for partial JSON checks.

## Integration Tests
Integration tests (marked with `@pytest.mark.integration`) use a dedicated `conftest.py` in `tests/integration/`.

### Neo4j Helper
Use the `neo4j_helper` fixture to simplify node creation in tests:
```python
def test_relationship(neo4j_client, neo4j_helper):
    # Setup using helper
    neo4j_helper.create_company(uei="UEI001")
    neo4j_helper.create_award(award_id="AWARD001")

    # Test logic...
```

## Adding New Tests
1. **Prefer factories** – Use factories to build test data.
2. **Parametrize** – Leverage `@pytest.mark.parametrize` for multiple input scenarios.
3. **Use custom assertions** – Call helpers from `tests/assertions.py` instead of repeating `assert` statements.
4. **Keep tests fast** – Place heavy‑weight setup (e.g., Neo4j) in `tests/integration/` and mark with `@pytest.mark.integration`.

## CI/CD and Guardrails
We use **GitHub Actions** and **pre-commit** to enforce quality standards.

### Pre-commit Hooks
Runs automatically on commit to check formatting and static analysis.
```bash
# Install hooks (done automatically by setup_dev.sh)
pre-commit install

# Run manually
pre-commit run --all-files
```

### CI Workflow
The `.github/workflows/ci.yml` pipeline runs on every PR:
1. **Verifies Setup**: Runs `scripts/setup_dev.sh` to ensure the dev environment is reproducible.
2. **Runs Tests**: Executes `pytest` with coverage reporting.
3. **Checks Taxonomy**: Validates CET taxonomy configuration.
5. **Document intent** – Add a short docstring describing the purpose of each test.

## Future Improvements
- Add more factory methods for other models (e.g., `Company`, `Contract`).
- Introduce a shared `conftest.py` fixture for the Neo4j client to reduce duplication.
- Expand parametrization for edge‑case date formats and large numeric values.

---
*Generated by Antigravity – your AI‑powered coding assistant.*
