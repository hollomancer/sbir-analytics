# Requirements Document

## Introduction

This feature integrates PaECTER (Patent-specific Encoder with Examiner Citations) to create a robust, explainable, and retrieval-friendly analysis layer that complements our existing CET-based patent and award classifiers. PaECTER provides high-quality patent embeddings that enable semantic search, prior-art style similarity, cohort cohesion checks, and award↔patent cross-linking to improve discovery, validation, and downstream analytics.

The system will use Hugging Face's hosted Inference API by default (no local GPU required) to accelerate adoption, with optional support for Hugging Face Inference Endpoints for dedicated capacity and tighter performance SLOs.

## Glossary

- **PaECTER_System**: The patent-specific encoder fine-tuned with examiner citations that generates embeddings for patent documents and SBIR awards
- **Hugging_Face_API**: External service for computing embeddings via the mpi-inno-comp/paecter model
- **Embedding_Vector**: Dense numerical representation of text content generated by the PaECTER model
- **Cosine_Similarity**: Metric used to measure semantic similarity between embedding vectors
- **FAISS_Index**: Fast Approximate Nearest Neighbor search library for efficient similarity computation
- **Neo4j_Similarity_Edge**: Graph database relationship connecting awards and patents based on semantic similarity
- **Cohesion_Metrics**: Statistical measures of how well embeddings cluster within CET classification groups
- **Quality_Gates**: Validation thresholds that must be met for pipeline execution to continue
- **Dagster_Asset**: Data processing unit in the pipeline orchestration system
- **Parquet_Output**: Columnar storage format for embedding and similarity data
- **Bayesian_Router**: A probabilistic routing mechanism that infers distributions over experts and routes inputs stochastically with posterior weights
- **Expert_Pool**: A collection of specialized lightweight adapters or heads trained on specific technology domains
- **Uncertainty_Head**: A calibrated confidence component that provides abstention capabilities on diffuse posteriors or low evidence
- **Technology_Cluster**: Emergent groupings of patents and SBIR awards discovered through Bayesian routing without preset categories
- **CPC_Section**: Cooperative Patent Classification sections (A, B, C, D, E, F, G, H, Y) used for patent categorization
- **CET_Area**: Critical and Emerging Technologies areas defined by government agencies for strategic technology assessment
- **Commercialization_Signal**: Indicators that a patent or SBIR award has transitioned from research to commercial application
- **ECE**: Expected Calibration Error, a metric for measuring calibration quality of probabilistic predictions
- **LoRA_Head**: Low-Rank Adaptation heads that serve as lightweight expert modules
- **DP_Means**: Dirichlet Process means clustering algorithm for dynamic expert creation

## Requirements

### Requirement 1

**User Story:** As a data scientist, I want to compute dense embeddings for patents and awards using PaECTER, so that I can perform semantic analysis and similarity matching across the SBIR ecosystem.

#### Acceptance Criteria

1. THE PaECTER_System SHALL generate embeddings for patent documents using title and abstract fields
2. THE PaECTER_System SHALL generate embeddings for SBIR awards using solicitation_title and abstract fields
3. THE system SHALL use the Hugging_Face_API with the mpi-inno-comp/paecter model for embedding computation
4. THE system SHALL store embeddings in Parquet format with metadata including model version and computation timestamp
5. THE system SHALL implement rate limiting and retry logic for API calls to ensure reliable processing

### Requirement 2

**User Story:** As a researcher, I want to identify semantic similarity between awards and patents, so that I can discover potential technology transfer relationships and prior art connections.

#### Acceptance Criteria

1. THE system SHALL compute cosine similarity between award and patent embeddings
2. THE system SHALL identify top-k most similar patents for each award based on configurable thresholds
3. THE system SHALL support both brute-force and FAISS_Index backends for similarity computation
4. THE system SHALL apply minimum similarity score thresholds to filter low-quality matches
5. THE system SHALL limit the number of similarity pairs per award to prevent excessive relationships

### Requirement 3

**User Story:** As a system administrator, I want quality validation for the PaECTER analysis pipeline, so that I can ensure data quality and system reliability.

#### Acceptance Criteria

1. THE system SHALL validate embedding coverage meets minimum thresholds for patents (≥98%) and awards (≥95%)
2. THE system SHALL compute cohesion metrics to validate that embeddings cluster appropriately within CET classifications
3. THE system SHALL implement Quality_Gates that block downstream processing when validation fails
4. THE system SHALL generate performance baselines and alert on significant drift in embedding quality
5. THE system SHALL provide detailed error reporting and recovery guidance when validation thresholds are not met

### Requirement 4

**User Story:** As a graph database user, I want semantic similarity relationships loaded into Neo4j, so that I can query and visualize award-patent connections alongside existing graph data.

#### Acceptance Criteria

1. WHERE Neo4j loading is enabled, THE system SHALL create SIMILAR_TO relationships between Award and Patent nodes
2. THE system SHALL include similarity scores, ranking, and metadata on Neo4j_Similarity_Edge relationships
3. THE system SHALL support incremental updates with options to prune previous relationships or mark current runs
4. THE system SHALL validate that referenced Award and Patent nodes exist before creating relationships
5. THE system SHALL provide dry-run capabilities for testing relationship creation without committing changes

### Requirement 5

**User Story:** As a policy researcher, I want Bayesian MoE classification routing for CET/CPC categories, so that I can quickly categorize patents and SBIR awards into technology domains with calibrated uncertainty before detailed analysis.

#### Acceptance Criteria

1. THE Bayesian_Router SHALL route documents to specialized Expert_Pool classifiers for initial CPC_Section and CET_Area predictions based on document characteristics
2. THE system SHALL use variational inference to model uncertainty in expert selection and provide probability distributions over classification experts
3. THE Uncertainty_Head SHALL compute calibrated confidence scores for classifications using Expected Calibration Error validation
4. THE system SHALL maintain separate expert classifiers for different technology domains with dynamic expert pool expansion
5. THE system SHALL flag low-confidence classifications requiring human review based on routing uncertainty thresholds

### Requirement 6

**User Story:** As a researcher, I want Bayesian MoE similarity computation routing, so that I can get robust semantic similarity measures within and across technology categories with uncertainty quantification.

#### Acceptance Criteria

1. THE Bayesian_Router SHALL route document pairs to specialized Expert_Pool similarity computation methods based on their classified technology categories
2. THE system SHALL use multiple similarity expert methods (intra-category, cross-category, domain-specific) with probabilistic routing
3. THE system SHALL aggregate similarity scores from multiple experts using Bayesian model averaging with uncertainty propagation
4. THE Uncertainty_Head SHALL provide confidence intervals for similarity scores and flag uncertain matches for review
5. THE system SHALL adapt expert routing based on technology category combinations and historical performance patterns

### Requirement 7

**User Story:** As a technology analyst, I want Bayesian MoE embedding generation routing, so that patents and SBIR awards can be processed by specialized embedding experts optimized for their identified technology domains.

#### Acceptance Criteria

1. THE Bayesian_Router SHALL route documents to specialized PaECTER expert models based on their classified technology categories and similarity analysis results
2. THE system SHALL use variational inference to model uncertainty in expert selection for domain-specific embedding generation
3. THE system SHALL provide uncertainty quantification that incorporates embedding generation uncertainty in downstream analysis
4. THE Uncertainty_Head SHALL compute calibrated confidence scores for embedding quality using Expected Calibration Error validation
5. THE system SHALL generate specialized embeddings that are optimized for the identified technology domains and similarity patterns

### Requirement 8

**User Story:** As a developer, I want the PaECTER analysis layer to be strictly additive, so that existing CET classification and pipeline functionality remains unchanged.

#### Acceptance Criteria

1. THE system SHALL implement all PaECTER functionality as new Dagster_Asset components without modifying existing assets
2. THE system SHALL not alter existing data schemas, API endpoints, or user interfaces
3. THE system SHALL be configurable to enable or disable without affecting other pipeline operations
4. THE system SHALL maintain backward compatibility with existing configuration and deployment processes
5. THE system SHALL provide clear separation between PaECTER outputs and existing CET classification results
