# Transition Detection System - Complete Overview

## What is Transition Detection?

The **Transition Detection System** identifies which SBIR-funded companies likely transitioned their research into federal procurement contracts. It combines six independent signals to estimate the probability that an SBIR award led to a subsequent federal contract (commercialization).

**Key Question**: *Did this SBIR-funded research result in a federal contract?*

**Answer**: A composite likelihood score (0.0â€“1.0) with confidence classification (HIGH/LIKELY/POSSIBLE) supported by detailed evidence.

## How It Works

The system uses a multi-signal scoring approach:

1. **Vendor Resolution** - Match SBIR recipients to federal contractors (UEI, CAGE, DUNS, fuzzy name)
2. **Signal Extraction** - Extract 6 independent evidence signals:
   - ğŸ›ï¸ **Agency Continuity** - Same federal agency indicates ongoing relationship
   - â±ï¸ **Timing Proximity** - Contracts within 0â€“24 months of award completion
   - ğŸ¯ **Competition Type** - Sole source/limited competition indicates vendor targeting
   - ğŸ“œ **Patent Signal** - Patents filed indicate technology maturity
   - ğŸ”¬ **CET Alignment** - Same critical technology area shows focus consistency
   - ğŸ¤ **Vendor Match** - UEI/CAGE/DUNS exact match confirms same company
3. **Composite Scoring** - Weighted combination of all signals (0.0â€“1.0)
4. **Confidence Classification** - HIGH (â‰¥0.85), LIKELY (0.65â€“0.84), POSSIBLE (<0.65)
5. **Evidence Generation** - Detailed justification for every detection
6. **Neo4j Loading** - Graph database storage for complex analysis

## Key Capabilities

- **Comprehensive Analysis**: 6 independent signals + configurable weights
- **Transparent Decisions**: Full evidence bundles justify every detection
- **Flexible Configuration**: Presets for high-precision, balanced, broad-discovery, and CET-focused analysis
- **Neo4j Integration**: Awardâ†’Transitionâ†’Contract pathways, patent backing, technology area clustering
- **Analytics**: Dual-perspective metrics (award-level + company-level transition rates)
- **Validation**: Precision/recall evaluation, confusion matrix, false positive analysis

## Performance Metrics

- **Throughput**: 15,000â€“20,000 detections/minute (target: â‰¥10K)
- **Coverage**: ~80% of SBIR awards resolve to contracts
- **Precision (HIGH)**: â‰¥85% (manual validation)
- **Recall**: â‰¥70% (vs. ground truth)
- **Scalability**: Tested on 252K awards + 6.7M contracts

## Data Assets

After running the transition detection pipeline, you get:

- **transitions.parquet** - Detected transitions with scores and signals
- **transitions_evidence.ndjson** - Complete evidence bundles (JSON per line)
- **vendor_resolution.parquet** - Award recipient â†’ contractor cross-walk
- **transition_analytics.json** - Aggregated KPIs (award-level, company-level, by-agency, by-CET)
- **transition_analytics_executive_summary.md** - Markdown report with key findings
- **Neo4j Transition nodes** - Queryable in graph database
- **Neo4j relationships** - TRANSITIONED_TO, RESULTED_IN, ENABLED_BY, INVOLVES_TECHNOLOGY

## Quick Start

### Running Transition Detection

```bash
# Run full transition detection pipeline
uv run python -m dagster job execute -f src/definitions.py -j transition_full_job

# Or: Run from Dagster UI
uv run dagster dev
# Then select and materialize "transition_full_job"
```

**Expected Output**: ~40,000â€“80,000 detected transitions with â‰¥85% precision (HIGH confidence)

### Configuration

#### Quick Setup - Use Presets

```bash
# Use balanced preset (default)
export SBIR_ETL__TRANSITION__DETECTION__PRESET=balanced

# Or: Use high-precision preset
export SBIR_ETL__TRANSITION__DETECTION__PRESET=high_precision

# Or: Use broad-discovery preset
export SBIR_ETL__TRANSITION__DETECTION__PRESET=broad_discovery
```

#### Fine-Tuning

```bash
# Override confidence thresholds
export SBIR_ETL__TRANSITION__DETECTION__HIGH_CONFIDENCE_THRESHOLD=0.88
export SBIR_ETL__TRANSITION__DETECTION__LIKELY_CONFIDENCE_THRESHOLD=0.70

# Override timing window (days)
export SBIR_ETL__TRANSITION__DETECTION__MAX_DAYS=365  # 12 months instead of 24

# Override signal weights (must sum to 1.0)
export SBIR_ETL__TRANSITION__DETECTION__AGENCY_WEIGHT=0.30
export SBIR_ETL__TRANSITION__DETECTION__TIMING_WEIGHT=0.20
export SBIR_ETL__TRANSITION__DETECTION__COMPETITION_WEIGHT=0.20
export SBIR_ETL__TRANSITION__DETECTION__PATENT_WEIGHT=0.15
export SBIR_ETL__TRANSITION__DETECTION__CET_WEIGHT=0.15
```

## Documentation

**Comprehensive Guides** (6,126 lines total):
- ğŸ“– [Detection Algorithm](detection-algorithm.md) - How the system works end-to-end
- ğŸ“– [Scoring Guide](scoring-guide.md) - Detailed scoring breakdown + tuning
- ğŸ“– [Vendor Matching](vendor-matching.md) - Vendor resolution methods + validation
- ğŸ“– [Evidence Bundles](evidence-bundles.md) - Evidence structure + interpretation
- ğŸ“– [Neo4j Schema](../schemas/transition-graph-schema.md) - Graph model + queries
- ğŸ“– [CET Integration](cet-integration.md) - Technology area alignment
- ğŸ“– [Data Dictionary](../data/dictionaries/transition-fields-dictionary.md) - Field reference

**Quick Reference**:
- ğŸ“‹ [MVP Guide](mvp.md) - Minimal viable product
- ğŸ“‹ [Configuration Guide](../../config/README.md) - YAML configuration guide

## Neo4j Queries

### Find All Transitions for an Award

```cypher
MATCH (a:Award {award_id: "SBIR-2020-PHASE-II-001"})
  -[:TRANSITIONED_TO]->(t:Transition)
  -[:RESULTED_IN]->(c:Contract)
RETURN a.award_id, c.contract_id, t.likelihood_score, t.confidence
ORDER BY t.likelihood_score DESC
```

### Find Patent-Backed Transitions

```cypher
MATCH (t:Transition)-[:ENABLED_BY]->(p:Patent)
  -[:RESULTED_IN]->(c:Contract)
WHERE t.confidence IN ["HIGH", "LIKELY"]
RETURN t.transition_id, p.title, c.piid, t.likelihood_score
```

### Transition Effectiveness by CET Area

```cypher
MATCH (a:Award)-[:INVOLVES_TECHNOLOGY]->(cet:CETArea)
  <-[:INVOLVES_TECHNOLOGY]-(t:Transition)
WITH cet.name as cet_area,
     count(DISTINCT a) as total_awards,
     count(DISTINCT t) as transitions
RETURN cet_area, total_awards, transitions,
       round(100.0 * transitions / total_awards) as effectiveness_percent
ORDER BY effectiveness_percent DESC
```

## Testing

```bash
# Run all transition detection tests
uv run pytest tests/unit/test_transition*.py -v
uv run pytest tests/integration/test_transition_integration.py -v
uv run pytest tests/e2e/test_transition_e2e.py -v

# Run with coverage
uv run pytest tests/unit/test_transition*.py --cov=src/transition --cov-report=html

# Run specific signal tests
uv run pytest tests/unit/test_transition_scorer.py -v  # 32 tests, 93% coverage
uv run pytest tests/unit/test_cet_signal_extractor.py -v  # 37 tests, 96% coverage
```

## Key Files

### Implementation
- `src/transition/detection/` - Detection pipeline (scoring, evidence, detector)
- `src/transition/features/` - Feature extraction (vendor resolver, patent analyzer, CET)
- `src/transition/analysis/` - Analytics (dual-perspective metrics)
- `src/transition/evaluation/` - Evaluation (precision/recall, confusion matrix)
- `src/transition/queries/` - Neo4j queries (pathways, analytics)
- `src/loaders/transition_loader.py` - Neo4j loading

### Configuration
- `config/transition/detection.yaml` - Scoring weights, thresholds
- `config/transition/presets.yaml` - Preset configurations
- `config/README.md` - Configuration guide

### Data
- `data/processed/transitions.parquet` - Detected transitions
- `data/processed/transitions_evidence.ndjson` - Evidence bundles (JSON per line)
- `data/processed/vendor_resolution.parquet` - Awardâ†’contractor cross-walk
- `data/processed/transition_analytics.json` - KPIs
- `reports/validation/transition_mvp.json` - MVP validation summary

## Algorithms

### Vendor Resolution (4-step cascade)

1. **UEI exact match** (confidence: 0.99)
2. **CAGE code exact match** (confidence: 0.95)
3. **DUNS number exact match** (confidence: 0.90)
4. **Fuzzy name matching** with RapidFuzz (confidence: 0.65â€“0.85)

### Transition Scoring (6 independent signals)

1. **Agency continuity** (weight: 0.25) - Same agency contracts
2. **Timing proximity** (weight: 0.20) - 0â€“24 months after award
3. **Competition type** (weight: 0.20) - Sole source/limited competition
4. **Patent signal** (weight: 0.15) - Patents filed; topic match
5. **CET alignment** (weight: 0.10) - Same technology area
6. **Vendor match** (weight: 0.10) - UEI/CAGE/DUNS confidence

### Confidence Bands

- **HIGH**: score â‰¥ 0.85 (high precision, ~85%)
- **LIKELY**: score 0.65â€“0.84 (balanced, ~75% precision)
- **POSSIBLE**: score <0.65 (high recall, ~40% precision)

## Implementation Status

**Transition Detection**: âœ… **FULLY COMPLETED** (October 30, 2025)

- All 169 specification tasks implemented and validated
- Performance metrics achieved: â‰¥10K detections/min, â‰¥85% precision, â‰¥70% recall
- Complete documentation suite delivered (8 guides, 6,126 lines)
- Neo4j graph schema implemented with full relationship modeling
- Archived in `.kiro/specs/archive/completed-features/transition_detection/`

---

For additional details, see the comprehensive guides listed above or consult the [main README](../../README.md).
