name: ETL Pipeline

on:
  schedule:
    # Weekly full pipeline on Monday at 10 AM UTC
    - cron: "0 10 * * 1"
  workflow_dispatch:
    inputs:
      job:
        description: "Job to run"
        required: true
        type: choice
        options:
          - sbir_ingestion
          - sbir_weekly_refresh  # includes Neo4j loading
          - usaspending_ingestion
          - uspto_pipeline
          - cet_pipeline
          - all
      environment:
        description: "Neo4j environment"
        required: true
        default: production
        type: choice
        options:
          - production
          - test
      skip_neo4j:
        description: "Skip Neo4j loading"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  contents: read
  packages: read

env:
  AWS_REGION: us-east-2
  S3_BUCKET: sbir-etl-production-data
  SBIR_ETL__PIPELINE__ENVIRONMENT: production
  IMAGE: ghcr.io/hollomancer/sbir-analytics:latest

jobs:
  sbir-pipeline:
    name: SBIR Pipeline
    if: github.event.inputs.job == 'sbir_ingestion' || github.event.inputs.job == 'sbir_weekly_refresh' || github.event.inputs.job == 'all' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    container:
      image: ghcr.io/hollomancer/sbir-analytics:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install AWS CLI
        run: pip install awscli && aws --version

      - name: Download data from S3
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        shell: bash
        run: |
          echo "ðŸ“¥ Downloading data from S3..."
          mkdir -p data/raw/sbir data/raw/sam_gov

          # Download latest SBIR awards (~350MB)
          LATEST_SBIR=$(aws s3 ls "s3://${S3_BUCKET}/raw/awards/" --recursive | sort | tail -1 | awk '{print $4}')
          if [ -n "$LATEST_SBIR" ]; then
            aws s3 cp "s3://${S3_BUCKET}/${LATEST_SBIR}" data/raw/sbir/award_data.csv
            echo "âœ… Downloaded SBIR awards"
          fi

          # Download SAM.gov entity data (~380MB)
          aws s3 cp "s3://${S3_BUCKET}/raw/sam_gov/sam_entity_records.parquet" data/raw/sam_gov/sam_entity_records.parquet && echo "âœ… Downloaded SAM.gov entities" || echo "âš ï¸ SAM.gov data not found"

          # Note: USAspending database (217GB) not downloaded - too large for GitHub runners
          # Use usaspending_ingestion job separately if needed

      - name: Run SBIR pipeline
        env:
          # Use test or production Neo4j based on environment input (default: production for scheduled runs)
          NEO4J_URI: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_URI || secrets.NEO4J_AURA_URI }}
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_PASSWORD || secrets.NEO4J_AURA_PASSWORD }}
          S3_BUCKET: ${{ env.S3_BUCKET }}
        shell: bash
        run: |
          echo "## ðŸŽ¯ SBIR Pipeline" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ github.event.inputs.environment || 'production' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Determine which job to run
          if [ "${{ github.event.inputs.job }}" = "sbir_ingestion" ]; then
            JOB_NAME="sbir_ingestion_job"
          else
            JOB_NAME="sbir_weekly_refresh_job"
          fi

          # Skip Neo4j if requested
          if [ "${{ github.event.inputs.skip_neo4j }}" = "true" ]; then
            JOB_NAME="sbir_ingestion_job"
          fi

          echo "**Job:** \`$JOB_NAME\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          dagster job execute \
            -m src.definitions \
            -j "$JOB_NAME" \
            2>&1 | tee sbir_output.txt

          EXIT_CODE=${PIPESTATUS[0]}

          # Strip ANSI codes and extract key metrics
          sed 's/\x1b\[[0-9;]*m//g' sbir_output.txt > sbir_clean.txt

          echo "### ðŸ“Š Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          # Extract final counts and summary lines
          grep -E "(Extracted [0-9]+ records|Validation complete|Loading Summary|Total rows processed|Successfully processed|Nodes Created|Relationships Created|Awards:|Companies:|Researchers:|RECIPIENT_OF|FOLLOWS|Quality check.*PASSED|neo4j_sbir_awards_load_check)" sbir_clean.txt | tail -20 >> $GITHUB_STEP_SUMMARY || echo "No summary metrics found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… SBIR pipeline completed successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ SBIR pipeline failed" >> $GITHUB_STEP_SUMMARY
            echo "### Error Details" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 sbir_clean.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit $EXIT_CODE
          fi

  usaspending-pipeline:
    name: USAspending Pipeline
    if: github.event.inputs.job == 'usaspending_ingestion' || github.event.inputs.job == 'all'
    needs: [sbir-pipeline]
    runs-on: ubuntu-latest
    timeout-minutes: 180
    container:
      image: ghcr.io/hollomancer/sbir-analytics:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Run USAspending pipeline
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        shell: bash
        run: |
          echo "## ðŸ’° USAspending Pipeline" >> $GITHUB_STEP_SUMMARY

          dagster job execute \
            -m src.definitions \
            -j usaspending_iterative_enrichment_job \
            2>&1 | tee usaspending_output.txt

          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… USAspending pipeline completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ USAspending pipeline failed" >> $GITHUB_STEP_SUMMARY
            tail -100 usaspending_output.txt >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  uspto-pipeline:
    name: USPTO Pipeline
    if: github.event.inputs.job == 'uspto_pipeline' || github.event.inputs.job == 'all'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    container:
      image: ghcr.io/hollomancer/sbir-analytics:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install AWS CLI
        run: pip install awscli && aws --version

      - name: Download USPTO data from S3
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "ðŸ“¥ Downloading USPTO data from S3..."
          mkdir -p data/raw/uspto

          # Download latest patent assignments to data/raw/uspto/ (asset expects files here)
          LATEST=$(aws s3 ls "s3://${S3_BUCKET}/raw/uspto/assignments/" | sort | tail -1 | awk '{print $2}')
          if [ -n "$LATEST" ]; then
            aws s3 cp "s3://${S3_BUCKET}/raw/uspto/assignments/${LATEST}" data/raw/uspto/ --recursive
            echo "âœ… Downloaded USPTO assignments from ${LATEST}"
            ls -la data/raw/uspto/
          fi

      - name: Run USPTO pipeline
        env:
          NEO4J_URI: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_URI || secrets.NEO4J_AURA_URI }}
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_PASSWORD || secrets.NEO4J_AURA_PASSWORD }}
          S3_BUCKET: ${{ env.S3_BUCKET }}
        shell: bash
        run: |
          echo "## ðŸ“œ USPTO Pipeline" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ github.event.inputs.environment || 'production' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Note: uspto_validation_job requires full Patent Assignment Dataset
          # (assignment, assignee, assignor tables). Using AI extraction for now.
          dagster job execute \
            -m src.definitions \
            -j uspto_ai_extraction_job \
            2>&1 | tee uspto_output.txt

          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… USPTO pipeline completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ USPTO pipeline failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 uspto_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  cet-pipeline:
    name: CET Classification Pipeline
    if: github.event.inputs.job == 'cet_pipeline' || github.event.inputs.job == 'all'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    container:
      image: ghcr.io/hollomancer/sbir-analytics:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Run CET pipeline
        env:
          NEO4J_URI: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_URI || secrets.NEO4J_AURA_URI }}
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_PASSWORD || secrets.NEO4J_AURA_PASSWORD }}
          S3_BUCKET: ${{ env.S3_BUCKET }}
        shell: bash
        run: |
          echo "## ðŸ·ï¸ CET Classification Pipeline" >> $GITHUB_STEP_SUMMARY

          dagster job execute \
            -m src.definitions \
            -j cet_full_pipeline_job \
            2>&1 | tee cet_output.txt

          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… CET pipeline completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ CET pipeline failed" >> $GITHUB_STEP_SUMMARY
            tail -100 cet_output.txt >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  summary:
    name: Pipeline Summary
    needs: [sbir-pipeline, usaspending-pipeline, uspto-pipeline, cet-pipeline]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Generate summary
        run: |
          echo "# ðŸ“Š ETL Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Pipeline | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| SBIR | ${{ needs.sbir-pipeline.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| USAspending | ${{ needs.usaspending-pipeline.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| USPTO | ${{ needs.uspto-pipeline.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| CET | ${{ needs.cet-pipeline.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
