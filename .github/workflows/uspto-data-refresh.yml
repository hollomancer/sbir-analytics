name: USPTO Patent Data Refresh

on:
  schedule:
    # Run monthly on the 1st at 9 AM UTC (USPTO typically updates monthly)
    - cron: "0 9 1 * *"
  workflow_dispatch:
    inputs:
      dataset:
        description: "Which USPTO dataset to download"
        required: true
        type: choice
        options:
          - all
          - patentsview
          - assignments
          - ai_patents
      force_refresh:
        description: "Force refresh even if data hash did not change"
        required: false
        default: false
        type: boolean
      format:
        description: "File format for assignments (csv, dta, parquet)"
        required: false
        default: "csv"
        type: choice
        options:
          - csv
          - dta
          - parquet
      patentsview_url:
        description: "PatentsView download URL (find at https://patentsview.org/downloads/data-downloads)"
        required: false
        type: string
      assignments_url:
        description: "USPTO Assignments download URL (find at https://www.uspto.gov/ip-policy/economic-research/research-datasets/patent-assignment-dataset)"
        required: false
        type: string
      ai_patents_url:
        description: "USPTO AI Patents download URL (find at https://www.uspto.gov/ip-policy/economic-research/research-datasets/artificial-intelligence-patent-dataset)"
        required: false
        type: string

permissions:
  id-token: write  # For OIDC authentication to AWS
  contents: read   # Only read access needed

concurrency:
  group: uspto-data-refresh
  cancel-in-progress: false

env:
  AWS_REGION: us-east-2
  S3_BUCKET: sbir-etl-production-data

jobs:
  download-uspto-data:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Determine datasets to download
        id: datasets
        run: |
          DATASET="${{ github.event.inputs.dataset || 'all' }}"
          if [ "$DATASET" = "all" ]; then
            echo "datasets=patentsview assignments ai_patents" >> $GITHUB_OUTPUT
          else
            echo "datasets=$DATASET" >> $GITHUB_OUTPUT
          fi

      - name: Download PatentsView data
        if: contains(steps.datasets.outputs.datasets, 'patentsview')
        id: patentsview
        run: |
          # PatentsView has default URLs built into the Lambda function
          # For manual runs, you can override with patentsview_url input
          # For scheduled runs, defaults are used (no override needed)
          # Default: downloads "patent" table from PatentsView S3
          SOURCE_URL="${{ github.event.inputs.patentsview_url }}"
          
          FUNCTION_NAME="sbir-analytics-download-uspto-patentsview"
          if [ -n "$SOURCE_URL" ]; then
            PAYLOAD=$(jq -n \
              --arg s3_bucket "${{ env.S3_BUCKET }}" \
              --arg dataset_type "patent" \
              --arg source_url "$SOURCE_URL" \
              --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
              '{
                s3_bucket: $s3_bucket,
                dataset_type: $dataset_type,
                source_url: $source_url,
                force_refresh: ($force_refresh == "true")
              }')
          else
            PAYLOAD=$(jq -n \
              --arg s3_bucket "${{ env.S3_BUCKET }}" \
              --arg dataset_type "patent" \
              --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
              '{
                s3_bucket: $s3_bucket,
                dataset_type: $dataset_type,
                force_refresh: ($force_refresh == "true")
              }')
          fi
          
          RESPONSE=$(aws lambda invoke \
            --function-name "$FUNCTION_NAME" \
            --payload "$(echo $PAYLOAD | jq -c .)" \
            --region "${{ env.AWS_REGION }}" \
            /tmp/patentsview-response.json)
          
          cat /tmp/patentsview-response.json | jq '.'
          
          STATUS=$(cat /tmp/patentsview-response.json | jq -r '.statusCode // 500')
          if [ "$STATUS" != "200" ]; then
            echo "❌ PatentsView download failed"
            exit 1
          fi
          
          echo "✅ PatentsView download successful"
          cat /tmp/patentsview-response.json | jq '.body' > /tmp/patentsview-result.json
          echo "result_file=/tmp/patentsview-result.json" >> $GITHUB_OUTPUT

      - name: Download USPTO Assignments
        if: contains(steps.datasets.outputs.datasets, 'assignments')
        id: assignments
        run: |
          # USPTO Assignments has default URLs built into the Lambda function (2023 CSV/DTA releases)
          # For manual runs, you can override with assignments_url input
          # For scheduled runs, defaults are used (no override needed)
          SOURCE_URL="${{ github.event.inputs.assignments_url }}"
          
          FUNCTION_NAME="sbir-analytics-download-uspto-assignments"
          FORMAT="${{ github.event.inputs.format || 'csv' }}"
          
          if [ -n "$SOURCE_URL" ]; then
            PAYLOAD=$(jq -n \
              --arg s3_bucket "${{ env.S3_BUCKET }}" \
              --arg source_url "$SOURCE_URL" \
              --arg format "$FORMAT" \
              --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
              '{
                s3_bucket: $s3_bucket,
                source_url: $source_url,
                format: $format,
                force_refresh: ($force_refresh == "true")
              }')
          else
            PAYLOAD=$(jq -n \
              --arg s3_bucket "${{ env.S3_BUCKET }}" \
              --arg format "$FORMAT" \
              --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
              '{
                s3_bucket: $s3_bucket,
                format: $format,
                force_refresh: ($force_refresh == "true")
              }')
          fi
          
          RESPONSE=$(aws lambda invoke \
            --function-name "$FUNCTION_NAME" \
            --payload "$(echo $PAYLOAD | jq -c .)" \
            --region "${{ env.AWS_REGION }}" \
            /tmp/assignments-response.json)
          
          cat /tmp/assignments-response.json | jq '.'
          
          STATUS=$(cat /tmp/assignments-response.json | jq -r '.statusCode // 500')
          if [ "$STATUS" != "200" ]; then
            echo "❌ USPTO Assignments download failed"
            exit 1
          fi
          
          echo "✅ USPTO Assignments download successful"
          cat /tmp/assignments-response.json | jq '.body' > /tmp/assignments-result.json
          echo "result_file=/tmp/assignments-result.json" >> $GITHUB_OUTPUT

      - name: Download USPTO AI Patents
        if: contains(steps.datasets.outputs.datasets, 'ai_patents')
        id: ai_patents
        run: |
          # USPTO AI Patents has default URL built into the Lambda function (2023 CSV release)
          # For manual runs, you can override with ai_patents_url input
          # For scheduled runs, defaults are used (no override needed)
          SOURCE_URL="${{ github.event.inputs.ai_patents_url }}"
          
          FUNCTION_NAME="sbir-analytics-download-uspto-ai-patents"
          if [ -n "$SOURCE_URL" ]; then
            PAYLOAD=$(jq -n \
              --arg s3_bucket "${{ env.S3_BUCKET }}" \
              --arg source_url "$SOURCE_URL" \
              --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
              '{
                s3_bucket: $s3_bucket,
                source_url: $source_url,
                force_refresh: ($force_refresh == "true")
              }')
          else
            PAYLOAD=$(jq -n \
              --arg s3_bucket "${{ env.S3_BUCKET }}" \
              --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
              '{
                s3_bucket: $s3_bucket,
                force_refresh: ($force_refresh == "true")
              }')
          fi
          
          RESPONSE=$(aws lambda invoke \
            --function-name "$FUNCTION_NAME" \
            --payload "$(echo $PAYLOAD | jq -c .)" \
            --region "${{ env.AWS_REGION }}" \
            /tmp/ai-patents-response.json)
          
          cat /tmp/ai-patents-response.json | jq '.'
          
          STATUS=$(cat /tmp/ai-patents-response.json | jq -r '.statusCode // 500')
          if [ "$STATUS" != "200" ]; then
            echo "❌ USPTO AI Patents download failed"
            exit 1
          fi
          
          echo "✅ USPTO AI Patents download successful"
          cat /tmp/ai-patents-response.json | jq '.body' > /tmp/ai-patents-result.json
          echo "result_file=/tmp/ai-patents-result.json" >> $GITHUB_OUTPUT

      - name: Display download summary
        if: always()
        run: |
          echo "## USPTO Data Download Summary"
          echo ""
          
          if [ -f "/tmp/patentsview-result.json" ]; then
            echo "### PatentsView"
            cat /tmp/patentsview-result.json | jq -r '
              "**S3 Key:** \(.s3_key)\n" +
              "**File Size:** \(.file_size | tonumber / 1024 / 1024 | floor) MB\n" +
              "**SHA256:** \(.sha256[0:16])...\n" +
              "**Downloaded:** \(.downloaded_at)\n"
            '
          fi
          
          if [ -f "/tmp/assignments-result.json" ]; then
            echo "### USPTO Patent Assignments"
            cat /tmp/assignments-result.json | jq -r '
              "**S3 Key:** \(.s3_key)\n" +
              "**Format:** \(.format)\n" +
              "**File Size:** \(.file_size | tonumber / 1024 / 1024 | floor) MB\n" +
              "**SHA256:** \(.sha256[0:16])...\n" +
              "**Downloaded:** \(.downloaded_at)\n"
            '
          fi
          
          if [ -f "/tmp/ai-patents-result.json" ]; then
            echo "### USPTO AI Patents"
            cat /tmp/ai-patents-result.json | jq -r '
              "**S3 Key:** \(.s3_key)\n" +
              "**File Size:** \(.file_size | tonumber / 1024 / 1024 | floor) MB\n" +
              "**SHA256:** \(.sha256[0:16])...\n" +
              "**Downloaded:** \(.downloaded_at)\n"
            '
          fi
          
          echo ""
          echo "**AWS Console:** https://console.aws.amazon.com/s3/buckets/${{ env.S3_BUCKET }}?region=${{ env.AWS_REGION }}&prefix=raw/uspto/"

      - name: Upload download results
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: uspto-download-results
          path: |
            /tmp/*-result.json
          if-no-files-found: warn
          retention-days: "7"

