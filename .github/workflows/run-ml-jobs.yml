name: Analysis Jobs On-Demand

on:
  # Run manually
  workflow_dispatch:
    inputs:
      job_name:
        description: 'Job to run'
        required: true
        type: choice
        options:
          - cet_full_pipeline
          - fiscal_returns_mvp
          - paecter_embeddings
          - all_analysis_jobs
      execution_mode:
        description: 'Where to run the job'
        required: true
        type: choice
        default: 'github-actions'
        options:
          - github-actions
          - aws-batch

  # Or weekly schedule (optional)
  schedule:
    - cron: '0 3 * * 1'  # Monday 3 AM UTC

permissions:
  contents: read
  id-token: write  # For AWS OIDC

env:
  AWS_REGION: us-east-2

jobs:
  run-analysis-job-github:
    name: Run Analysis Job on GitHub Actions
    if: ${{ github.event.inputs.execution_mode == 'github-actions' || github.event.inputs.execution_mode == '' }}
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours max (GitHub Actions limit is 6 hours)

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "true"

      - name: Install analysis dependencies
        run: |
          # Install heavy ML/fiscal extras that aren't in base dependencies
          uv pip install dagster dagster-cloud
          # Install R integration for fiscal analysis
          uv pip install "rpy2>=3.5.0,<4.0.0"
          # Install sentence-transformers for PaECTER embeddings
          uv pip install "sentence-transformers>=2.7.0,<3.0.0"

      - name: Verify analysis jobs are available
        env:
          DAGSTER_LOAD_HEAVY_ASSETS: "true"
        run: |
          echo "Verifying that analysis jobs loaded correctly..."
          uv run python -c "
          import os
          import sys

          # Ensure heavy assets are loaded
          os.environ['DAGSTER_LOAD_HEAVY_ASSETS'] = 'true'

          # Import definitions_ml
          try:
              from src.definitions_ml import auto_jobs
          except Exception as e:
              print(f'âŒ Failed to import definitions_ml: {e}')
              sys.exit(1)

          # Check that expected jobs exist
          available_jobs = list(auto_jobs.keys())
          print(f'Available jobs: {available_jobs}')

          required_jobs = [
              'cet_full_pipeline_job',
              'fiscal_returns_mvp_job',
              'paecter_job'
          ]

          missing_jobs = [job for job in required_jobs if job not in available_jobs]

          if missing_jobs:
              print(f'âŒ Missing required jobs: {missing_jobs}')
              print(f'   This likely means heavy dependencies are not installed correctly.')
              sys.exit(1)

          print('âœ… All analysis jobs loaded successfully:')
          for job_name in required_jobs:
              job = auto_jobs[job_name]
              # Check if it's a placeholder (empty selection)
              if hasattr(job, 'asset_selection') and len(list(job.asset_selection.resolve([]))) == 0:
                  print(f'   âš ï¸  {job_name}: PLACEHOLDER (no assets)')
              else:
                  print(f'   âœ… {job_name}: OK')
          "

      - name: Run Analysis Job
        env:
          DAGSTER_LOAD_HEAVY_ASSETS: "true"
          DAGSTER_HOME: /tmp/dagster_home
        run: |
          # Initialize Dagster
          mkdir -p $DAGSTER_HOME

          # Determine which job to run
          JOB_NAME="${{ github.event.inputs.job_name || 'all_analysis_jobs' }}"

          case $JOB_NAME in
            cet_full_pipeline)
              echo "Running CET full pipeline..."
              dagster job execute -m src.definitions_ml -j cet_full_pipeline_job
              ;;
            fiscal_returns_mvp)
              echo "Running fiscal returns analysis..."
              dagster job execute -m src.definitions_ml -j fiscal_returns_mvp_job
              ;;
            paecter_embeddings)
              echo "Running PaECTER embeddings..."
              dagster job execute -m src.definitions_ml -j paecter_job
              ;;
            all_analysis_jobs)
              echo "Running all analysis jobs..."
              dagster job execute -m src.definitions_ml -j cet_full_pipeline_job
              dagster job execute -m src.definitions_ml -j fiscal_returns_mvp_job
              dagster job execute -m src.definitions_ml -j paecter_job
              ;;
            *)
              echo "Unknown job: $JOB_NAME"
              exit 1
              ;;
          esac

      - name: Upload job results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: analysis-job-results
          path: |
            /tmp/dagster_home
            reports/
          retention-days: 7

  run-analysis-job-batch:
    name: Run Analysis Job on AWS Batch
    if: ${{ github.event.inputs.execution_mode == 'aws-batch' }}
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Just for orchestration, actual job runs on Batch

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Submit job to AWS Batch
        id: batch-submit
        run: |
          # Determine which job to run
          JOB_NAME="${{ github.event.inputs.job_name || 'all_analysis_jobs' }}"

          # Map job name to job definition
          case $JOB_NAME in
            cet_full_pipeline)
              JOB_DEFINITION="sbir-analytics-analysis-cet-pipeline"
              DAGSTER_JOB="cet_full_pipeline_job"
              ;;
            fiscal_returns_mvp)
              JOB_DEFINITION="sbir-analytics-analysis-fiscal-returns"
              DAGSTER_JOB="fiscal_returns_mvp_job"
              ;;
            paecter_embeddings)
              JOB_DEFINITION="sbir-analytics-analysis-paecter-embeddings"
              DAGSTER_JOB="paecter_job"
              ;;
            all_analysis_jobs)
              # For "all_analysis_jobs", submit each job separately
              echo "Submitting all analysis jobs to AWS Batch..."

              # Submit CET job
              CET_JOB_ID=$(aws batch submit-job \
                --job-name "ml-cet-${{ github.run_id }}" \
                --job-queue sbir-analytics-analysis-job-queue \
                --job-definition sbir-analytics-analysis-cet-pipeline \
                --query 'jobId' \
                --output text)
              echo "Submitted CET job: $CET_JOB_ID"
              echo "cet_job_id=$CET_JOB_ID" >> $GITHUB_OUTPUT

              # Submit Fiscal job
              FISCAL_JOB_ID=$(aws batch submit-job \
                --job-name "ml-fiscal-${{ github.run_id }}" \
                --job-queue sbir-analytics-analysis-job-queue \
                --job-definition sbir-analytics-analysis-fiscal-returns \
                --query 'jobId' \
                --output text)
              echo "Submitted Fiscal job: $FISCAL_JOB_ID"
              echo "fiscal_job_id=$FISCAL_JOB_ID" >> $GITHUB_OUTPUT

              # Submit PaECTER job
              PAECTER_JOB_ID=$(aws batch submit-job \
                --job-name "ml-paecter-${{ github.run_id }}" \
                --job-queue sbir-analytics-analysis-job-queue \
                --job-definition sbir-analytics-analysis-paecter-embeddings \
                --query 'jobId' \
                --output text)
              echo "Submitted PaECTER job: $PAECTER_JOB_ID"
              echo "paecter_job_id=$PAECTER_JOB_ID" >> $GITHUB_OUTPUT

              echo "All jobs submitted successfully"
              echo "job_ids=$CET_JOB_ID,$FISCAL_JOB_ID,$PAECTER_JOB_ID" >> $GITHUB_OUTPUT
              exit 0
              ;;
            *)
              echo "Unknown job: $JOB_NAME"
              exit 1
              ;;
          esac

          # Submit single job
          echo "Submitting $JOB_NAME to AWS Batch..."
          JOB_ID=$(aws batch submit-job \
            --job-name "ml-${JOB_NAME}-${{ github.run_id }}" \
            --job-queue sbir-analytics-analysis-job-queue \
            --job-definition "$JOB_DEFINITION" \
            --query 'jobId' \
            --output text)

          echo "Submitted AWS Batch job: $JOB_ID"
          echo "job_id=$JOB_ID" >> $GITHUB_OUTPUT

      - name: Wait for job completion
        run: |
          JOB_IDS="${{ steps.batch-submit.outputs.job_ids || steps.batch-submit.outputs.job_id }}"

          if [ -z "$JOB_IDS" ]; then
            echo "No job IDs found"
            exit 1
          fi

          # Split comma-separated job IDs
          IFS=',' read -ra JOBS <<< "$JOB_IDS"

          echo "Waiting for ${#JOBS[@]} job(s) to complete..."

          FAILED_JOBS=()
          for JOB_ID in "${JOBS[@]}"; do
            echo "Monitoring job: $JOB_ID"

            # Wait for job to complete (max 6 hours)
            MAX_WAIT=21600  # 6 hours in seconds
            ELAPSED=0
            INTERVAL=30

            while [ $ELAPSED -lt $MAX_WAIT ]; do
              STATUS=$(aws batch describe-jobs --jobs "$JOB_ID" \
                --query 'jobs[0].status' --output text)

              echo "[$(date '+%Y-%m-%d %H:%M:%S')] Job $JOB_ID status: $STATUS"

              if [ "$STATUS" = "SUCCEEDED" ]; then
                echo "âœ… Job $JOB_ID completed successfully"
                break
              elif [ "$STATUS" = "FAILED" ]; then
                echo "âŒ Job $JOB_ID failed"
                FAILED_JOBS+=("$JOB_ID")
                break
              fi

              sleep $INTERVAL
              ELAPSED=$((ELAPSED + INTERVAL))
            done

            if [ $ELAPSED -ge $MAX_WAIT ]; then
              echo "â±ï¸ Job $JOB_ID timed out after 6 hours"
              FAILED_JOBS+=("$JOB_ID")
            fi
          done

          # Check if any jobs failed
          if [ ${#FAILED_JOBS[@]} -gt 0 ]; then
            echo "The following jobs failed: ${FAILED_JOBS[*]}"
            exit 1
          fi

          echo "All jobs completed successfully! ðŸŽ‰"

      - name: Get job logs
        if: always()
        run: |
          JOB_IDS="${{ steps.batch-submit.outputs.job_ids || steps.batch-submit.outputs.job_id }}"

          if [ -z "$JOB_IDS" ]; then
            echo "No job IDs to fetch logs for"
            exit 0
          fi

          IFS=',' read -ra JOBS <<< "$JOB_IDS"

          for JOB_ID in "${JOBS[@]}"; do
            echo "=== Logs for job: $JOB_ID ==="

            # Get log stream name from job details
            LOG_STREAM=$(aws batch describe-jobs --jobs "$JOB_ID" \
              --query 'jobs[0].container.logStreamName' --output text)

            if [ "$LOG_STREAM" != "None" ] && [ -n "$LOG_STREAM" ]; then
              aws logs tail "/aws/batch/sbir-analytics-ml" \
                --log-stream-names "$LOG_STREAM" \
                --format short \
                --since 6h || echo "Failed to fetch logs for $JOB_ID"
            else
              echo "No log stream found for job $JOB_ID"
            fi

            echo ""
          done

      - name: Output job summary
        if: always()
        run: |
          echo "### ðŸš€ AWS Batch Job Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Job Name:** ${{ github.event.inputs.job_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Mode:** AWS Batch" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          JOB_IDS="${{ steps.batch-submit.outputs.job_ids || steps.batch-submit.outputs.job_id }}"

          if [ -n "$JOB_IDS" ]; then
            echo "**Job IDs:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$JOB_IDS" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "View logs in [AWS Batch Console](https://console.aws.amazon.com/batch/home?region=${{ env.AWS_REGION }}#jobs/queue/sbir-analytics-analysis-job-queue)" >> $GITHUB_STEP_SUMMARY
          fi
