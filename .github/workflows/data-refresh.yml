name: Data Refresh

on:
  schedule:
    # SBIR Awards: Weekly on Monday at 9 AM UTC
    - cron: "0 9 * * 1"
    # USAspending: Monthly on 6th at 2 AM UTC
    - cron: "0 2 6 * *"
    # USPTO: Monthly on 1st at 9 AM UTC
    - cron: "0 9 1 * *"
  workflow_dispatch:
    inputs:
      source:
        description: "Data source to refresh"
        required: true
        type: choice
        options:
          - sbir
          - usaspending
          - uspto
          - all
      force_refresh:
        description: "Force refresh even if no changes detected"
        required: false
        default: false
        type: boolean
      trigger_dagster:
        description: "Trigger Dagster ETL after data refresh"
        required: false
        default: true
        type: boolean

permissions:
  id-token: write
  contents: read

concurrency:
  group: data-refresh-${{ github.event.inputs.source || 'scheduled' }}
  cancel-in-progress: false

env:
  AWS_REGION: us-east-2
  S3_BUCKET: sbir-etl-production-data

jobs:
  determine-source:
    name: Determine Refresh Source
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      sbir: ${{ steps.determine.outputs.sbir }}
      usaspending: ${{ steps.determine.outputs.usaspending }}
      uspto: ${{ steps.determine.outputs.uspto }}
    steps:
      - name: Determine which sources to refresh
        id: determine
        run: |
          # Manual trigger
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            SOURCE="${{ github.event.inputs.source }}"
            if [ "$SOURCE" = "all" ]; then
              echo "sbir=true" >> $GITHUB_OUTPUT
              echo "usaspending=true" >> $GITHUB_OUTPUT
              echo "uspto=true" >> $GITHUB_OUTPUT
            else
              echo "sbir=$([[ $SOURCE == 'sbir' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
              echo "usaspending=$([[ $SOURCE == 'usaspending' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
              echo "uspto=$([[ $SOURCE == 'uspto' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
            fi
          # Scheduled trigger - determine by cron schedule
          else
            CRON_SCHEDULE="${{ github.event.schedule }}"
            case "$CRON_SCHEDULE" in
              "0 9 * * 1")  # Weekly Monday
                echo "sbir=true" >> $GITHUB_OUTPUT
                echo "usaspending=false" >> $GITHUB_OUTPUT
                echo "uspto=false" >> $GITHUB_OUTPUT
                ;;
              "0 2 6 * *")  # Monthly 6th
                echo "sbir=false" >> $GITHUB_OUTPUT
                echo "usaspending=true" >> $GITHUB_OUTPUT
                echo "uspto=false" >> $GITHUB_OUTPUT
                ;;
              "0 9 1 * *")  # Monthly 1st
                echo "sbir=false" >> $GITHUB_OUTPUT
                echo "usaspending=false" >> $GITHUB_OUTPUT
                echo "uspto=true" >> $GITHUB_OUTPUT
                ;;
            esac
          fi

      - name: Generate workflow summary
        run: |
          echo "## ðŸ“Š Data Refresh Plan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph LR" >> $GITHUB_STEP_SUMMARY
          echo "  A[Determine Source] --> B{Which Source?}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.determine.outputs.sbir }}" = "true" ]; then
            echo "  B -->|SBIR| C[Refresh SBIR Awards]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.determine.outputs.usaspending }}" = "true" ]; then
            echo "  B -->|USAspending| D[Refresh USAspending DB]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.determine.outputs.uspto }}" = "true" ]; then
            echo "  B -->|USPTO| E[Refresh USPTO Patents]" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY

  refresh-sbir:
    name: Refresh SBIR Awards
    needs: determine-source
    if: needs.determine-source.outputs.sbir == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Check existing SBIR files
        run: |
          echo "## ðŸŽ¯ SBIR Awards Refresh Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“‚ Current SBIR Files in S3" >> $GITHUB_STEP_SUMMARY
          aws s3 ls "s3://${{ env.S3_BUCKET }}/raw/awards/" --recursive --human-readable | tail -10 >> $GITHUB_STEP_SUMMARY || echo "No files found"
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: Download SBIR awards
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "### ðŸ“¥ Downloading SBIR Awards" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Install minimal dependencies
          uv pip install boto3 requests

          # Download and upload
          uv run python scripts/data/download_sbir.py 2>&1 | tee sbir_download.log

          echo "<details><summary>Download Log</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat sbir_download.log >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Check downloaded files
        if: always()
        run: |
          echo "### ðŸ“¦ SBIR Files After Refresh" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          aws s3 ls "s3://${{ env.S3_BUCKET }}/raw/awards/" --recursive --human-readable | tail -10 >> $GITHUB_STEP_SUMMARY || echo "No files found"

  refresh-usaspending:
    name: Refresh USAspending Database
    needs: determine-source
    if: needs.determine-source.outputs.usaspending == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours for 217GB download with resume capability
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 14400  # 4 hours (IAM role max)

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Install async dependencies
        run: |
          uv pip install aiohttp aioboto3

      - name: Check if new USAspending file available
        id: check-new-file
        run: |
          echo "## ðŸ” USAspending File Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check for new file (exit code 0 = new file, 1 = not new)
          # Capture stderr separately to avoid corrupting JSON
          if uv run python scripts/usaspending/check_new_file.py \
            --s3-bucket ${{ env.S3_BUCKET }} \
            --database-type full \
            --json > check_result.json 2>check_stderr.txt; then
            IS_NEW="true"
            echo "is_new=true" >> $GITHUB_OUTPUT
            echo "### âœ… New/Updated File Detected - Will Download" >> $GITHUB_STEP_SUMMARY
          else
            EXIT_CODE=$?
            IS_NEW="false"
            echo "is_new=false" >> $GITHUB_OUTPUT
            echo "### â„¹ï¸ No New File - Skipping Download (exit code: $EXIT_CODE)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show any stderr output
          if [ -s check_stderr.txt ]; then
            echo "<details><summary>Script output</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat check_stderr.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Show check details if valid JSON
          if [ -f check_result.json ] && jq -e . check_result.json > /dev/null 2>&1; then
            SOURCE_URL=$(jq -r '.source_url // "N/A"' check_result.json)
            AVAILABLE=$(jq -r '.available // false' check_result.json)
            CONTENT_LENGTH=$(jq -r '.content_length // 0' check_result.json)
            S3_CONTENT_LENGTH=$(jq -r '.s3_content_length // 0' check_result.json)
            SIZE_MISMATCH=$(jq -r '.size_mismatch // false' check_result.json)

            SOURCE_SIZE_GB=$(echo "scale=2; $CONTENT_LENGTH / 1073741824" | bc 2>/dev/null || echo "0")
            S3_SIZE_GB=$(echo "scale=2; $S3_CONTENT_LENGTH / 1073741824" | bc 2>/dev/null || echo "0")

            echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Source URL | \`${SOURCE_URL}\` |" >> $GITHUB_STEP_SUMMARY
            echo "| Source Available | ${AVAILABLE} |" >> $GITHUB_STEP_SUMMARY
            echo "| Source Size | ${SOURCE_SIZE_GB} GB |" >> $GITHUB_STEP_SUMMARY
            echo "| S3 Size | ${S3_SIZE_GB} GB |" >> $GITHUB_STEP_SUMMARY
            echo "| Size Mismatch | ${SIZE_MISMATCH} |" >> $GITHUB_STEP_SUMMARY
            echo "| Will Download | ${IS_NEW} |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            if [ "$SIZE_MISMATCH" = "true" ]; then
              echo "âš ï¸ **Size mismatch detected** - S3 file appears incomplete. Will re-download." >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Download and process USAspending database (parallel)
        id: usaspending-download
        # Download if new file available OR force_refresh is enabled
        if: steps.check-new-file.outputs.is_new == 'true' || github.event.inputs.force_refresh == 'true'
        timeout-minutes: 330  # 5.5 hours for full download with resume
        run: |
          set -e
          echo "Starting USAspending download with resume support..."
          echo "Timeout: 330 minutes (5.5 hours)"
          echo ""

          # Concurrency of 3 to prevent OOM on GitHub runner
          # Script uses consistent S3 key based on source file date for resume
          uv run python scripts/data/download_usaspending_parallel.py \
            --s3-bucket ${{ env.S3_BUCKET }} \
            --database-type full \
            --max-concurrent 3 2>&1 | tee usaspending_output.txt

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Download script exited with code: $EXIT_CODE"
          if [ $EXIT_CODE -ne 0 ]; then
            echo "ERROR: Download failed with exit code $EXIT_CODE"
            exit $EXIT_CODE
          fi

      - name: Report USAspending status
        if: always()
        run: |
          echo "## ðŸ’¾ USAspending Database Download Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if download was skipped or ran
          if [ "${{ steps.usaspending-download.outcome }}" = "skipped" ]; then
            echo "### â­ï¸ Status: SKIPPED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show why it was skipped
            if [ -f check_result.json ]; then
              echo "**Reason:** File in S3 matches source (same size and date)" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Check Results:**" >> $GITHUB_STEP_SUMMARY
              echo '```json' >> $GITHUB_STEP_SUMMARY
              cat check_result.json >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          elif [ "${{ steps.usaspending-download.outcome }}" = "success" ]; then
            echo "### âœ… Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.usaspending-download.outcome }}" = "failure" ]; then
            echo "### âŒ Status: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Note:** Progress is saved. Re-run workflow to resume download." >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show download progress if output exists
          if [ -f usaspending_output.txt ]; then
            echo "### ðŸ“Š Download Progress" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep -E "File size:|Total chunks:|Already uploaded:|Remaining:|DOWNLOAD COMPLETE|INTERRUPTED|Parts completed:" usaspending_output.txt | head -20 >> $GITHUB_STEP_SUMMARY || echo "No progress info"
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show last few parts
            echo "<details><summary>Last 20 parts uploaded</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            grep "âœ… Part" usaspending_output.txt | tail -20 >> $GITHUB_STEP_SUMMARY || echo "No parts uploaded"
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Show current S3 files
          echo "### ðŸ“¦ USAspending Files in S3" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| File | Size | Last Modified |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|---------------|" >> $GITHUB_STEP_SUMMARY

          aws s3api list-objects-v2 \
            --bucket "${{ env.S3_BUCKET }}" \
            --prefix "raw/usaspending/" \
            --query 'Contents[*].[Key,Size,LastModified]' \
            --output text | while read key size modified; do
              if [ -n "$key" ]; then
                size_gb=$(echo "scale=2; $size / 1073741824" | bc)
                echo "| \`${key}\` | ${size_gb} GB | ${modified} |" >> $GITHUB_STEP_SUMMARY
              fi
            done

          echo "" >> $GITHUB_STEP_SUMMARY

          # Full output log in collapsible section
          if [ -f usaspending_output.txt ]; then
            echo "<details><summary>ðŸ“‹ Full Output Log - Click to expand</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat usaspending_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
          fi

  refresh-uspto:
    name: Refresh USPTO Patents
    needs: determine-source
    if: needs.determine-source.outputs.uspto == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Download USPTO data
        id: uspto-download
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸ“¥ USPTO Patent Data Download" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Install minimal dependencies
          uv pip install boto3 requests

          SUCCESS=true

          # PatentsView (run in parallel)
          echo "### 1ï¸âƒ£ PatentsView Data" >> $GITHUB_STEP_SUMMARY
          uv run python scripts/data/download_uspto.py --dataset patentsview --table patent 2>&1 | tee patentsview.log &
          PID_PV=$!

          # Assignments
          echo "### 2ï¸âƒ£ Patent Assignments" >> $GITHUB_STEP_SUMMARY
          uv run python scripts/data/download_uspto.py --dataset assignments 2>&1 | tee assignments.log &
          PID_AS=$!

          # AI Patents
          echo "### 3ï¸âƒ£ AI Patents" >> $GITHUB_STEP_SUMMARY
          uv run python scripts/data/download_uspto.py --dataset ai_patents 2>&1 | tee ai_patents.log &
          PID_AI=$!

          # Wait for all downloads
          wait $PID_PV || SUCCESS=false
          wait $PID_AS || SUCCESS=false
          wait $PID_AI || SUCCESS=false

          # Report results
          for log in patentsview.log assignments.log ai_patents.log; do
            echo "<details><summary>$log</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat $log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
          done

          if [ "$SUCCESS" = "false" ]; then
            echo "::error::One or more USPTO downloads failed"
            exit 1
          fi

      - name: Check S3 uploads
        if: always()
        run: |
          echo "## ðŸ“¦ USPTO S3 Upload Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for prefix in "raw/uspto/patentsview" "raw/uspto/assignments" "raw/uspto/ai_patents"; do
            echo "### $prefix" >> $GITHUB_STEP_SUMMARY
            aws s3 ls "s3://${{ env.S3_BUCKET }}/$prefix/" --recursive --human-readable | tail -5 >> $GITHUB_STEP_SUMMARY || echo "No files found"
            echo "" >> $GITHUB_STEP_SUMMARY
          done

  summary:
    name: Refresh Summary
    needs: [determine-source, refresh-sbir, refresh-usaspending, refresh-uspto]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Configure AWS credentials
        uses: actions/checkout@v6

      - name: Setup AWS
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Generate summary
        run: |
          echo "# ðŸ“Š Data Refresh Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "**Requested Source:** ${{ github.event.inputs.source }}" >> $GITHUB_STEP_SUMMARY
            echo "**Force Refresh:** ${{ github.event.inputs.force_refresh }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall status
          OVERALL_SUCCESS=true
          TOTAL=0
          SUCCESS_COUNT=0
          FAILED_COUNT=0
          SKIPPED_COUNT=0

          echo "## ðŸ“‹ Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Data Source | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------|--------|--------|" >> $GITHUB_STEP_SUMMARY

          # SBIR
          if [ "${{ needs.determine-source.outputs.sbir }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            JOB_STATUS="${{ needs.refresh-sbir.result }}"

            # Check if job succeeded (Step Functions started)
            if [ "$JOB_STATUS" = "success" ]; then
              # Job succeeded, but check execution status from the detailed report
              # The execution status is shown in the SBIR job's own summary
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Step Functions execution completed - check detailed report above"
            elif [ "$JOB_STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Failed to start or complete execution"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸŽ¯ SBIR Awards | $EMOJI $JOB_STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸŽ¯ SBIR Awards | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          # USAspending
          if [ "${{ needs.determine-source.outputs.usaspending }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            STATUS="${{ needs.refresh-usaspending.result }}"
            if [ "$STATUS" = "success" ]; then
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Database downloaded and uploaded"
            elif [ "$STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Download or upload failed"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸ’° USAspending DB | $EMOJI $STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ’° USAspending DB | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          # USPTO
          if [ "${{ needs.determine-source.outputs.uspto }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            STATUS="${{ needs.refresh-uspto.result }}"
            if [ "$STATUS" = "success" ]; then
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Patent data downloaded via Lambda"
            elif [ "$STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Lambda invocation failed"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸ“œ USPTO Patents | $EMOJI $STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ“œ USPTO Patents | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # File details for downloaded sources
          echo "## ðŸ“¦ Downloaded Files" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # USAspending files
          if [ "${{ needs.determine-source.outputs.usaspending }}" = "true" ] && [ "${{ needs.refresh-usaspending.result }}" = "success" ]; then
            echo "### ðŸ’° USAspending Database" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/raw/usaspending/database/ --recursive --human-readable --summarize | tail -20
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # USPTO files
          if [ "${{ needs.determine-source.outputs.uspto }}" = "true" ] && [ "${{ needs.refresh-uspto.result }}" = "success" ]; then
            echo "### ðŸ“œ USPTO Patent Data" >> $GITHUB_STEP_SUMMARY

            echo "**PatentsView:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/patentsview/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "**Assignments:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/assignments/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "**AI Patents:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/ai-patents/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Statistics
          echo "## ðŸ“ˆ Statistics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total jobs:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Successful:** $SUCCESS_COUNT âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** $FAILED_COUNT âŒ" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped:** $SKIPPED_COUNT â­ï¸" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Final verdict
          if [ "$OVERALL_SUCCESS" = "true" ] && [ "$TOTAL" -gt 0 ]; then
            echo "## âœ… Overall Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All data refresh jobs completed successfully!" >> $GITHUB_STEP_SUMMARY
          elif [ "$TOTAL" -eq 0 ]; then
            echo "## âš ï¸ Overall Status: NO JOBS RUN" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "No data sources were selected for refresh." >> $GITHUB_STEP_SUMMARY
          else
            echo "## âŒ Overall Status: FAILURES DETECTED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ One or more data refresh jobs failed. Please review the detailed reports above and check the job logs." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow completed at:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**S3 Bucket:** \`${{ env.S3_BUCKET }}\`" >> $GITHUB_STEP_SUMMARY

  run-pipeline:
    name: Run ETL Pipeline
    needs: [determine-source, refresh-sbir, refresh-usaspending, refresh-uspto, summary]
    if: |
      always() &&
      (github.event.inputs.trigger_dagster != 'false') &&
      (needs.refresh-sbir.result == 'success' || needs.refresh-usaspending.result == 'success' || needs.refresh-uspto.result == 'success')
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: uv sync

      - name: Run SBIR pipeline
        if: needs.refresh-sbir.result == 'success'
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USER: ${{ secrets.NEO4J_USER }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          SBIR_ETL__PIPELINE__ENVIRONMENT: production
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸŽ¯ SBIR Pipeline" >> $GITHUB_STEP_SUMMARY

          uv run dagster job execute \
            -m src.definitions \
            -j sbir_weekly_refresh_job \
            2>&1 | tee sbir_output.txt

          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "âœ… SBIR pipeline completed (extract â†’ validate â†’ Neo4j)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ SBIR pipeline failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 sbir_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Run USAspending enrichment
        if: needs.refresh-usaspending.result == 'success'
        env:
          SBIR_ETL__PIPELINE__ENVIRONMENT: production
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸ’° USAspending Enrichment" >> $GITHUB_STEP_SUMMARY

          uv run dagster job execute \
            -m src.definitions \
            -j usaspending_ingestion_job \
            2>&1 | tee usaspending_output.txt

          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "âœ… USAspending enrichment completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ USAspending enrichment failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 usaspending_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            # Don't fail the whole workflow for enrichment
          fi

      - name: Run USPTO pipeline
        if: needs.refresh-uspto.result == 'success'
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USER: ${{ secrets.NEO4J_USER }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          SBIR_ETL__PIPELINE__ENVIRONMENT: production
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸ“œ USPTO Pipeline" >> $GITHUB_STEP_SUMMARY

          uv run dagster job execute \
            -m src.definitions \
            -j uspto_full_pipeline_job \
            2>&1 | tee uspto_output.txt

          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "âœ… USPTO pipeline completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ USPTO pipeline failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 uspto_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
