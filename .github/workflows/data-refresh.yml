name: Data Refresh

on:
  schedule:
    # SBIR Awards: Weekly on Monday at 9 AM UTC
    - cron: "0 9 * * 1"
    # USAspending: Monthly on 6th at 2 AM UTC
    - cron: "0 2 6 * *"
    # USPTO: Monthly on 1st at 9 AM UTC
    - cron: "0 9 1 * *"
  workflow_dispatch:
    inputs:
      source:
        description: "Data source to refresh"
        required: true
        type: choice
        options:
          - sbir
          - usaspending
          - uspto
          - all
      force_refresh:
        description: "Force refresh even if no changes detected"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  contents: read

concurrency:
  group: data-refresh-${{ github.event.inputs.source || 'scheduled' }}
  cancel-in-progress: false

env:
  AWS_REGION: us-east-2
  S3_BUCKET: sbir-etl-production-data

jobs:
  determine-source:
    name: Determine Refresh Source
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      sbir: ${{ steps.determine.outputs.sbir }}
      usaspending: ${{ steps.determine.outputs.usaspending }}
      uspto: ${{ steps.determine.outputs.uspto }}
    steps:
      - name: Determine which sources to refresh
        id: determine
        run: |
          # Manual trigger
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            SOURCE="${{ github.event.inputs.source }}"
            if [ "$SOURCE" = "all" ]; then
              echo "sbir=true" >> $GITHUB_OUTPUT
              echo "usaspending=true" >> $GITHUB_OUTPUT
              echo "uspto=true" >> $GITHUB_OUTPUT
            else
              echo "sbir=$([[ $SOURCE == 'sbir' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
              echo "usaspending=$([[ $SOURCE == 'usaspending' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
              echo "uspto=$([[ $SOURCE == 'uspto' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
            fi
          # Scheduled trigger - determine by cron schedule
          else
            CRON_SCHEDULE="${{ github.event.schedule }}"
            case "$CRON_SCHEDULE" in
              "0 9 * * 1")  # Weekly Monday
                echo "sbir=true" >> $GITHUB_OUTPUT
                echo "usaspending=false" >> $GITHUB_OUTPUT
                echo "uspto=false" >> $GITHUB_OUTPUT
                ;;
              "0 2 6 * *")  # Monthly 6th
                echo "sbir=false" >> $GITHUB_OUTPUT
                echo "usaspending=true" >> $GITHUB_OUTPUT
                echo "uspto=false" >> $GITHUB_OUTPUT
                ;;
              "0 9 1 * *")  # Monthly 1st
                echo "sbir=false" >> $GITHUB_OUTPUT
                echo "usaspending=false" >> $GITHUB_OUTPUT
                echo "uspto=true" >> $GITHUB_OUTPUT
                ;;
            esac
          fi

      - name: Generate workflow summary
        run: |
          echo "## ðŸ“Š Data Refresh Plan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph LR" >> $GITHUB_STEP_SUMMARY
          echo "  A[Determine Source] --> B{Which Source?}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.determine.outputs.sbir }}" = "true" ]; then
            echo "  B -->|SBIR| C[Refresh SBIR Awards]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.determine.outputs.usaspending }}" = "true" ]; then
            echo "  B -->|USAspending| D[Refresh USAspending DB]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.determine.outputs.uspto }}" = "true" ]; then
            echo "  B -->|USPTO| E[Refresh USPTO Patents]" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY

  refresh-sbir:
    name: Refresh SBIR Awards
    needs: determine-source
    if: needs.determine-source.outputs.sbir == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check existing SBIR files
        id: check-existing
        run: |
          # Use shared reporting script for header
          ./scripts/ci/report-download-status.sh \
            --title "SBIR Awards Refresh Report" \
            --emoji "ðŸŽ¯" \
            --section-level 2

          # Use shared S3 listing script
          ./scripts/ci/report-s3-files.sh \
            --title "ðŸ“‚ Current S3 Files" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            --s3-prefix "raw/sbir/" \
            --tail 10

      - name: Start SBIR refresh Step Functions
        id: sbir-stepfunctions
        run: |
          # Source helper functions
          source ./scripts/ci/report-helpers.sh

          INPUT_PAYLOAD=$(jq -n \
            --arg force_refresh "${{ github.event.inputs.force_refresh || 'false' }}" \
            --arg s3_bucket "${{ env.S3_BUCKET }}" \
            '{
              force_refresh: ($force_refresh == "true"),
              s3_bucket: $s3_bucket
            }')

          # Display configuration using helper
          report_config "$INPUT_PAYLOAD"

          EXECUTION=$(aws stepfunctions start-execution \
            --state-machine-arn "${{ secrets.STEP_FUNCTIONS_STATE_MACHINE_ARN }}" \
            --input "$INPUT_PAYLOAD" \
            --region ${{ env.AWS_REGION }})

          EXECUTION_ARN=$(echo $EXECUTION | jq -r '.executionArn')
          EXECUTION_NAME=$(echo $EXECUTION_ARN | rev | cut -d: -f1 | rev)
          START_DATE=$(echo $EXECUTION | jq -r '.startDate')

          echo "execution_arn=$EXECUTION_ARN" >> $GITHUB_OUTPUT

          # Display execution details using helper
          report_execution_details "$EXECUTION_NAME" "$START_DATE" "SBIR ETL Pipeline" "${{ env.AWS_REGION }}"
          echo "ðŸ”— **Monitor execution:** [AWS Console](https://console.aws.amazon.com/states/home?region=${{ env.AWS_REGION }}#/executions/details/$EXECUTION_ARN)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: Wait for Step Functions completion
        id: wait-completion
        run: |
          EXECUTION_ARN="${{ steps.sbir-stepfunctions.outputs.execution_arn }}"
          echo "â³ Waiting for Step Functions execution to complete..."

          MAX_WAIT=300  # 5 minutes
          ELAPSED=0
          INTERVAL=10

          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS=$(aws stepfunctions describe-execution \
              --execution-arn "$EXECUTION_ARN" \
              --region ${{ env.AWS_REGION }} \
              --query 'status' \
              --output text)

            echo "Status: $STATUS (${ELAPSED}s elapsed)"

            if [ "$STATUS" = "SUCCEEDED" ]; then
              echo "âœ… Execution completed successfully"
              echo "status=success" >> $GITHUB_OUTPUT
              break
            elif [ "$STATUS" = "FAILED" ] || [ "$STATUS" = "TIMED_OUT" ] || [ "$STATUS" = "ABORTED" ]; then
              echo "âŒ Execution failed with status: $STATUS"
              echo "status=failed" >> $GITHUB_OUTPUT
              echo "::error::Step Functions execution failed with status: $STATUS"
              exit 1
            fi

            sleep $INTERVAL
            ELAPSED=$((ELAPSED + INTERVAL))
          done

          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "â° Timeout waiting for execution (still running)"
            echo "status=running" >> $GITHUB_OUTPUT
          fi

      - name: Get execution results
        if: always()
        run: |
          # Source helper functions
          source ./scripts/ci/report-helpers.sh

          EXECUTION_ARN="${{ steps.sbir-stepfunctions.outputs.execution_arn }}"

          report_header 3 "ðŸ“Š" "Execution Status"

          # Get execution details
          EXECUTION_DETAILS=$(aws stepfunctions describe-execution \
            --execution-arn "$EXECUTION_ARN" \
            --region ${{ env.AWS_REGION }})

          STATUS=$(echo "$EXECUTION_DETAILS" | jq -r '.status')
          STOP_DATE=$(echo "$EXECUTION_DETAILS" | jq -r '.stopDate // "Still running"')

          # Display status using helper
          if [ "$STATUS" = "SUCCEEDED" ]; then
            report_status_badge "success" "Completed at $STOP_DATE"
          elif [ "$STATUS" = "RUNNING" ]; then
            report_status_badge "running" "Check console for progress"
          else
            report_status_badge "failed" "$STATUS at $STOP_DATE"
          fi

          # Get execution history for errors
          if [ "$STATUS" = "FAILED" ]; then
            ERROR_DETAILS=$(aws stepfunctions get-execution-history \
              --execution-arn "$EXECUTION_ARN" \
              --region ${{ env.AWS_REGION }} \
              --max-results 10 \
              --reverse-order \
              | jq '.events[] | select(.type | contains("Failed"))' || echo "No error details available")
            report_error "$ERROR_DETAILS"
          fi

          # Check CloudWatch logs
          LOG_GROUP="/aws/lambda/sbir-etl-download"
          LOGS=$(aws logs tail "$LOG_GROUP" --since 5m --format short --region ${{ env.AWS_REGION }} 2>&1 || echo "No recent logs found")
          report_logs "$LOGS" "Recent Logs" 20

      - name: Check downloaded files
        if: always()
        run: |
          # Source helper functions
          source ./scripts/ci/report-helpers.sh

          # Use S3 listing script
          ./scripts/ci/report-s3-files.sh \
            --title "ðŸ“¦ Downloaded Files" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            --s3-prefix "raw/sbir/" \
            --tail 20 \
            --summarize

          # Compare before/after
          CURRENT_COUNT=$(aws s3 ls s3://${{ env.S3_BUCKET }}/raw/sbir/ --recursive | wc -l)
          report_file_changes "$CURRENT_COUNT"

          report_info "The Step Functions workflow handles SBIR data download. Check the AWS Console link above for detailed execution logs."

  refresh-usaspending:
    name: Refresh USAspending Database
    needs: determine-source
    if: needs.determine-source.outputs.usaspending == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours for 217GB download with resume capability
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-duration-seconds: 14400  # 4 hours (IAM role max)

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Install async dependencies
        run: |
          uv pip install aiohttp aioboto3

      - name: Check if new USAspending file available
        id: check-new-file
        run: |
          # Source helper functions
          source ./scripts/ci/report-helpers.sh

          # Use shared reporting script for header
          ./scripts/ci/report-download-status.sh \
            --title "USAspending File Availability Check" \
            --emoji "ðŸ”" \
            --section-level 2

          # Check for new file (exit code 0 = new file, 1 = not new)
          if uv run python scripts/usaspending/check_new_file.py \
            --s3-bucket ${{ env.S3_BUCKET }} \
            --database-type full \
            --json > check_result.json; then

            # File is new
            IS_NEW="true"
            echo "is_new=true" >> $GITHUB_OUTPUT
            report_header 3 "âœ…" "New File Available"
          else
            # File is not new or not available (exit code 1)
            IS_NEW="false"
            echo "is_new=false" >> $GITHUB_OUTPUT
            report_header 3 "â„¹ï¸" "No New File - Using Existing Data"
          fi

          # Display check results using shared script
          ./scripts/ci/report-download-status.sh \
            --json-file check_result.json \
            --json-label "Check Results" \
            --section-level 4

          # Extract and display key information
          if [ -f check_result.json ]; then
            SOURCE_URL=$(jq -r '.source_url // "N/A"' check_result.json)
            AVAILABLE=$(jq -r '.available // false' check_result.json)
            LAST_MODIFIED=$(jq -r '.last_modified // "N/A"' check_result.json)
            S3_LAST_MODIFIED=$(jq -r '.s3_last_modified // "N/A"' check_result.json)

            echo "**Source URL:** $SOURCE_URL" >> $GITHUB_STEP_SUMMARY
            echo "**Available:** $AVAILABLE" >> $GITHUB_STEP_SUMMARY
            echo "**Is New:** $IS_NEW" >> $GITHUB_STEP_SUMMARY
            if [ "$LAST_MODIFIED" != "N/A" ]; then
              echo "**Source Last Modified:** $LAST_MODIFIED" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "$S3_LAST_MODIFIED" != "N/A" ]; then
              echo "**S3 Last Modified:** $S3_LAST_MODIFIED" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Download and process USAspending database (parallel)
        id: usaspending-download
        # Download if new file available OR force_refresh is enabled
        if: steps.check-new-file.outputs.is_new == 'true' || github.event.inputs.force_refresh == 'true'
        timeout-minutes: 330  # 5.5 hours for full download with resume
        run: |
          set -e
          echo "Starting USAspending download with resume support..."
          echo "Timeout: 330 minutes (5.5 hours)"
          echo ""

          # Concurrency of 3 to prevent OOM on GitHub runner
          # Script uses consistent S3 key based on source file date for resume
          uv run python scripts/data/download_usaspending_parallel.py \
            --s3-bucket ${{ env.S3_BUCKET }} \
            --database-type full \
            --max-concurrent 3 2>&1 | tee usaspending_output.txt

          EXIT_CODE=${PIPESTATUS[0]}
          echo "Download script exited with code: $EXIT_CODE"
          if [ $EXIT_CODE -ne 0 ]; then
            echo "ERROR: Download failed with exit code $EXIT_CODE"
            exit $EXIT_CODE
          fi

      - name: Report USAspending status
        if: always()
        run: |
          # Source helper functions
          source ./scripts/ci/report-helpers.sh

          # Use shared reporting script for header
          ./scripts/ci/report-download-status.sh \
            --title "USAspending Database Download Report" \
            --emoji "ðŸ’¾" \
            --section-level 2

          # Check if download was skipped or ran
          if [ "${{ steps.usaspending-download.outcome }}" = "skipped" ]; then
            report_header 3 "â­ï¸" "Status: SKIPPED - No New Data Available"
            echo "The USAspending database has not changed since the last download." >> $GITHUB_STEP_SUMMARY
            echo "Existing data in S3 is up-to-date." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.usaspending-download.outcome }}" = "success" ]; then
            report_status_badge "success" "New Data Downloaded"
          elif [ "${{ steps.usaspending-download.outcome }}" = "failure" ]; then
            report_status_badge "failed" "Download failed"
          fi

          # Extract and display key metrics from output
          if [ -f usaspending_output.txt ]; then
            METRICS=$(grep -E "File size:|Total chunks:|Already uploaded:|Remaining:|Concurrency:|Part [0-9]+/|DOWNLOAD COMPLETE|INTERRUPTED|Total size:|Total parts:|Total time:|S3:" usaspending_output.txt | tail -30 || echo "No metrics found")
            report_logs "$METRICS" "ðŸ“Š Download Metrics" 30

            # Show final summary section if it exists
            if grep -q "DOWNLOAD COMPLETE" usaspending_output.txt; then
              SUMMARY=$(awk '/^=+$/{if(p)exit;p=1}p' usaspending_output.txt | tail -15)
              report_logs "$SUMMARY" "âœ¨ Completion Summary" 15
            fi

            # Show interrupted message if present (progress saved for resume)
            if grep -q "INTERRUPTED" usaspending_output.txt; then
              INTERRUPTED_BLOCK=$(awk '/INTERRUPTED/,/^=+$/' usaspending_output.txt)
              report_logs "$INTERRUPTED_BLOCK" "âš ï¸ Download Interrupted - Will Resume Next Run" 15
            fi

            # Show error details if failed
            if grep -q "ERROR DURING DOWNLOAD" usaspending_output.txt; then
              ERROR_BLOCK=$(awk '/ERROR DURING DOWNLOAD/,/^=+$/' usaspending_output.txt)
              report_error "$ERROR_BLOCK"
            fi

            # Show recent progress
            RECENT_PROGRESS=$(tail -50 usaspending_output.txt)
            report_logs "$RECENT_PROGRESS" "ðŸ“ˆ Recent Progress" 50
          else
            report_warning "No output file found"
          fi

          # S3 Status using shared script
          ./scripts/ci/report-s3-files.sh \
            --title "ðŸ“¦ S3 Status" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            --s3-prefix "raw/usaspending/database/" \
            --tail 5 \
            --message "Latest files in s3://${{ env.S3_BUCKET }}/raw/usaspending/database/:"

          # Full output log in collapsible section
          if [ -f usaspending_output.txt ]; then
            FULL_OUTPUT=$(cat usaspending_output.txt)
            report_collapsible "ðŸ“‹ Full Output Log - Click to expand" "$FULL_OUTPUT"
          fi

  refresh-uspto:
    name: Refresh USPTO Patents
    needs: determine-source
    if: needs.determine-source.outputs.uspto == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Trigger USPTO data downloads via Lambda
        id: uspto-lambda
        run: |
          # Source helper functions
          source ./scripts/ci/report-helpers.sh

          # Use shared reporting script for header
          ./scripts/ci/report-download-status.sh \
            --title "USPTO Patent Data Download Report" \
            --emoji "ðŸ“¥" \
            --section-level 2

          LAMBDA_SUCCESS=true

          # PatentsView
          report_header 3 "1ï¸âƒ£" "PatentsView Data"
          if aws lambda invoke \
            --function-name sbir-analytics-download-uspto \
            --payload '{"dataset": "patentsview", "table_name": "patent"}' \
            response_patentsview.json; then
            ./scripts/ci/report-download-status.sh \
              --status "success" \
              --json-file response_patentsview.json \
              --section-level 4
          else
            ./scripts/ci/report-download-status.sh \
              --status "failed" \
              --message "Failed to invoke Lambda" \
              --section-level 4
            LAMBDA_SUCCESS=false
          fi

          # Assignments
          report_header 3 "2ï¸âƒ£" "Patent Assignments"
          if aws lambda invoke \
            --function-name sbir-analytics-download-uspto \
            --payload '{"dataset": "assignments", "format": "csv"}' \
            response_assignments.json; then
            ./scripts/ci/report-download-status.sh \
              --status "success" \
              --json-file response_assignments.json \
              --section-level 4
          else
            ./scripts/ci/report-download-status.sh \
              --status "failed" \
              --message "Failed to invoke Lambda" \
              --section-level 4
            LAMBDA_SUCCESS=false
          fi

          # AI Patents
          report_header 3 "3ï¸âƒ£" "AI Patents"
          if aws lambda invoke \
            --function-name sbir-analytics-download-uspto \
            --payload '{"dataset": "ai_patents"}' \
            response_ai_patents.json; then
            ./scripts/ci/report-download-status.sh \
              --status "success" \
              --json-file response_ai_patents.json \
              --section-level 4
          else
            ./scripts/ci/report-download-status.sh \
              --status "failed" \
              --message "Failed to invoke Lambda" \
              --section-level 4
            LAMBDA_SUCCESS=false
          fi

          if [ "$LAMBDA_SUCCESS" = "false" ]; then
            echo "::warning::One or more USPTO downloads failed to execute"
            exit 1
          fi

      - name: Check S3 uploads
        if: always()
        run: |
          # Use shared reporting script for header
          ./scripts/ci/report-download-status.sh \
            --title "USPTO S3 Upload Status" \
            --emoji "ðŸ“¦" \
            --section-level 2

          # Use shared S3 listing scripts for each dataset
          ./scripts/ci/report-s3-files.sh \
            --title "**PatentsView:**" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            --s3-prefix "raw/uspto/patentsview/" \
            --tail 5 \
            --section-level 3

          ./scripts/ci/report-s3-files.sh \
            --title "**Assignments:**" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            --s3-prefix "raw/uspto/assignments/" \
            --tail 5 \
            --section-level 3

          ./scripts/ci/report-s3-files.sh \
            --title "**AI Patents:**" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            --s3-prefix "raw/uspto/ai_patents/" \
            --tail 5 \
            --section-level 3

          # Summary from Lambda responses using table helper
          ./scripts/ci/format-summary-table.sh \
            --title "ðŸ“Š Download Summary" \
            --headers "Dataset,Status,File Size,S3 Key" \
            --section-level 3 \
            "PatentsView|$(jq -r '.body.status // "unknown"' response_patentsview.json 2>/dev/null || echo "N/A")|$(jq -r '.body.file_size // 0' response_patentsview.json 2>/dev/null | awk '{printf "%.2f MB", $1/1048576}')|$(jq -r '.body.s3_key // "N/A"' response_patentsview.json 2>/dev/null || echo "N/A")" \
            "Assignments|$(jq -r '.body.status // "unknown"' response_assignments.json 2>/dev/null || echo "N/A")|$(jq -r '.body.file_size // 0' response_assignments.json 2>/dev/null | awk '{printf "%.2f MB", $1/1048576}')|$(jq -r '.body.s3_key // "N/A"' response_assignments.json 2>/dev/null || echo "N/A")" \
            "AI Patents|$(jq -r '.body.status // "unknown"' response_ai_patents.json 2>/dev/null || echo "N/A")|$(jq -r '.body.file_size // 0' response_ai_patents.json 2>/dev/null | awk '{printf "%.2f MB", $1/1048576}')|$(jq -r '.body.s3_key // "N/A"' response_ai_patents.json 2>/dev/null || echo "N/A")"

  summary:
    name: Refresh Summary
    needs: [determine-source, refresh-sbir, refresh-usaspending, refresh-uspto]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Configure AWS credentials
        uses: actions/checkout@v6

      - name: Setup AWS
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Generate summary
        run: |
          echo "# ðŸ“Š Data Refresh Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "**Requested Source:** ${{ github.event.inputs.source }}" >> $GITHUB_STEP_SUMMARY
            echo "**Force Refresh:** ${{ github.event.inputs.force_refresh }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall status
          OVERALL_SUCCESS=true
          TOTAL=0
          SUCCESS_COUNT=0
          FAILED_COUNT=0
          SKIPPED_COUNT=0

          echo "## ðŸ“‹ Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Data Source | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------|--------|--------|" >> $GITHUB_STEP_SUMMARY

          # SBIR
          if [ "${{ needs.determine-source.outputs.sbir }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            JOB_STATUS="${{ needs.refresh-sbir.result }}"

            # Check if job succeeded (Step Functions started)
            if [ "$JOB_STATUS" = "success" ]; then
              # Job succeeded, but check execution status from the detailed report
              # The execution status is shown in the SBIR job's own summary
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Step Functions execution completed - check detailed report above"
            elif [ "$JOB_STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Failed to start or complete execution"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸŽ¯ SBIR Awards | $EMOJI $JOB_STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸŽ¯ SBIR Awards | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          # USAspending
          if [ "${{ needs.determine-source.outputs.usaspending }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            STATUS="${{ needs.refresh-usaspending.result }}"
            if [ "$STATUS" = "success" ]; then
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Database downloaded and uploaded"
            elif [ "$STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Download or upload failed"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸ’° USAspending DB | $EMOJI $STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ’° USAspending DB | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          # USPTO
          if [ "${{ needs.determine-source.outputs.uspto }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            STATUS="${{ needs.refresh-uspto.result }}"
            if [ "$STATUS" = "success" ]; then
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Patent data downloaded via Lambda"
            elif [ "$STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Lambda invocation failed"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸ“œ USPTO Patents | $EMOJI $STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ“œ USPTO Patents | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # File details for downloaded sources
          echo "## ðŸ“¦ Downloaded Files" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # USAspending files
          if [ "${{ needs.determine-source.outputs.usaspending }}" = "true" ] && [ "${{ needs.refresh-usaspending.result }}" = "success" ]; then
            echo "### ðŸ’° USAspending Database" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/raw/usaspending/database/ --recursive --human-readable --summarize | tail -20
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # USPTO files
          if [ "${{ needs.determine-source.outputs.uspto }}" = "true" ] && [ "${{ needs.refresh-uspto.result }}" = "success" ]; then
            echo "### ðŸ“œ USPTO Patent Data" >> $GITHUB_STEP_SUMMARY

            echo "**PatentsView:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/patentsview/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "**Assignments:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/assignments/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "**AI Patents:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/ai-patents/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Statistics
          echo "## ðŸ“ˆ Statistics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total jobs:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Successful:** $SUCCESS_COUNT âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** $FAILED_COUNT âŒ" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped:** $SKIPPED_COUNT â­ï¸" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Final verdict
          if [ "$OVERALL_SUCCESS" = "true" ] && [ "$TOTAL" -gt 0 ]; then
            echo "## âœ… Overall Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All data refresh jobs completed successfully!" >> $GITHUB_STEP_SUMMARY
          elif [ "$TOTAL" -eq 0 ]; then
            echo "## âš ï¸ Overall Status: NO JOBS RUN" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "No data sources were selected for refresh." >> $GITHUB_STEP_SUMMARY
          else
            echo "## âŒ Overall Status: FAILURES DETECTED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ One or more data refresh jobs failed. Please review the detailed reports above and check the job logs." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow completed at:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**S3 Bucket:** \`${{ env.S3_BUCKET }}\`" >> $GITHUB_STEP_SUMMARY
