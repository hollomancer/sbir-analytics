name: Data Refresh

on:
  schedule:
    # SBIR Awards: Weekly on Monday at 9 AM UTC
    - cron: "0 9 * * 1"
    # USAspending: Monthly on 6th at 2 AM UTC
    - cron: "0 2 6 * *"
    # USPTO: Monthly on 1st at 9 AM UTC
    - cron: "0 9 1 * *"
  workflow_dispatch:
    inputs:
      source:
        description: "Data source to refresh"
        required: true
        type: choice
        options:
          - sbir
          - usaspending
          - uspto
          - all
      environment:
        description: "Neo4j environment"
        required: true
        default: production
        type: choice
        options:
          - production
          - test
      force_refresh:
        description: "Force refresh even if no changes detected"
        required: false
        default: false
        type: boolean
      trigger_dagster:
        description: "Trigger Dagster ETL after data refresh"
        required: false
        default: true
        type: boolean

permissions:
  id-token: write
  contents: read

concurrency:
  group: data-refresh-${{ github.event.inputs.source || 'scheduled' }}
  cancel-in-progress: false

env:
  AWS_REGION: us-east-2
  S3_BUCKET: sbir-etl-production-data

jobs:
  determine-source:
    name: Determine Refresh Source
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      sbir: ${{ steps.determine.outputs.sbir }}
      usaspending: ${{ steps.determine.outputs.usaspending }}
      uspto: ${{ steps.determine.outputs.uspto }}
    steps:
      - name: Determine which sources to refresh
        id: determine
        run: |
          # Manual trigger
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            SOURCE="${{ github.event.inputs.source }}"
            if [ "$SOURCE" = "all" ]; then
              echo "sbir=true" >> $GITHUB_OUTPUT
              echo "usaspending=true" >> $GITHUB_OUTPUT
              echo "uspto=true" >> $GITHUB_OUTPUT
            else
              echo "sbir=$([[ $SOURCE == 'sbir' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
              echo "usaspending=$([[ $SOURCE == 'usaspending' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
              echo "uspto=$([[ $SOURCE == 'uspto' ]] && echo true || echo false)" >> $GITHUB_OUTPUT
            fi
          # Scheduled trigger - determine by cron schedule
          else
            CRON_SCHEDULE="${{ github.event.schedule }}"
            case "$CRON_SCHEDULE" in
              "0 9 * * 1")  # Weekly Monday
                echo "sbir=true" >> $GITHUB_OUTPUT
                echo "usaspending=false" >> $GITHUB_OUTPUT
                echo "uspto=false" >> $GITHUB_OUTPUT
                ;;
              "0 2 6 * *")  # Monthly 6th
                echo "sbir=false" >> $GITHUB_OUTPUT
                echo "usaspending=true" >> $GITHUB_OUTPUT
                echo "uspto=false" >> $GITHUB_OUTPUT
                ;;
              "0 9 1 * *")  # Monthly 1st
                echo "sbir=false" >> $GITHUB_OUTPUT
                echo "usaspending=false" >> $GITHUB_OUTPUT
                echo "uspto=true" >> $GITHUB_OUTPUT
                ;;
            esac
          fi

      - name: Generate workflow summary
        run: |
          echo "## ðŸ“Š Data Refresh Plan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph LR" >> $GITHUB_STEP_SUMMARY
          echo "  A[Determine Source] --> B{Which Source?}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.determine.outputs.sbir }}" = "true" ]; then
            echo "  B -->|SBIR| C[Refresh SBIR Awards]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.determine.outputs.usaspending }}" = "true" ]; then
            echo "  B -->|USAspending| D[Extract Recipients]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.determine.outputs.uspto }}" = "true" ]; then
            echo "  B -->|USPTO| E[Refresh USPTO Patents]" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY

  refresh-sbir:
    name: Refresh SBIR Awards
    needs: determine-source
    if: needs.determine-source.outputs.sbir == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Check existing SBIR files
        run: |
          echo "## ðŸŽ¯ SBIR Awards Refresh Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“‚ Current SBIR Files in S3" >> $GITHUB_STEP_SUMMARY
          aws s3 ls "s3://${{ env.S3_BUCKET }}/raw/awards/" --recursive --human-readable | tail -10 >> $GITHUB_STEP_SUMMARY || echo "No files found"
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: Download SBIR awards
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "### ðŸ“¥ Downloading SBIR Awards" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Install minimal dependencies
          uv pip install boto3 requests

          # Download and upload
          uv run python scripts/data/download_sbir.py 2>&1 | tee sbir_download.log

          echo "<details><summary>Download Log</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat sbir_download.log >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Check downloaded files
        if: always()
        run: |
          echo "### ðŸ“¦ SBIR Files After Refresh" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          aws s3 ls "s3://${{ env.S3_BUCKET }}/raw/awards/" --recursive --human-readable | tail -10 >> $GITHUB_STEP_SUMMARY || echo "No files found"

  refresh-usaspending:
    name: Refresh USAspending Recipients
    needs: determine-source
    if: needs.determine-source.outputs.usaspending == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Much faster - only downloads ~1.2GB
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Install dependencies
        run: |
          uv pip install boto3 requests pandas pyarrow

      - name: Check for new USAspending dump
        id: check-new
        run: |
          echo "## ðŸ” Checking for New USAspending Data" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Find latest dump URL from USAspending
          LATEST_URL=$(uv run python -c "
          import requests
          from datetime import datetime, timedelta

          base_url = 'https://files.usaspending.gov/database_download'
          for days_ago in range(0, 30):
              date = datetime.now() - timedelta(days=days_ago)
              filename = f'usaspending-db_{date.strftime(\"%Y%m%d\")}.zip'
              url = f'{base_url}/{filename}'
              try:
                  resp = requests.head(url, timeout=5)
                  if resp.status_code == 200:
                      print(url)
                      break
              except:
                  continue
          ")

          if [ -z "$LATEST_URL" ]; then
            echo "âŒ Could not find USAspending dump" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          # Extract date from URL
          DUMP_DATE=$(echo "$LATEST_URL" | grep -oE '[0-9]{8}')
          echo "latest_url=$LATEST_URL" >> $GITHUB_OUTPUT
          echo "dump_date=$DUMP_DATE" >> $GITHUB_OUTPUT

          echo "**Latest dump:** \`$LATEST_URL\`" >> $GITHUB_STEP_SUMMARY
          echo "**Dump date:** $DUMP_DATE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if we already have this version
          EXISTING=$(aws s3 ls "s3://${{ env.S3_BUCKET }}/raw/usaspending/recipient_lookup/" --recursive | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}' | sort -r | head -1 || echo "")

          if [ -n "$EXISTING" ]; then
            EXISTING_YYYYMMDD=$(echo "$EXISTING" | tr -d '-')
            echo "**Existing version:** $EXISTING" >> $GITHUB_STEP_SUMMARY

            if [ "$EXISTING_YYYYMMDD" -ge "$DUMP_DATE" ] && [ "${{ github.event.inputs.force_refresh }}" != "true" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "âœ… Already up to date - skipping extraction" >> $GITHUB_STEP_SUMMARY
              echo "is_new=false" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ†• New version available - will extract" >> $GITHUB_STEP_SUMMARY
          echo "is_new=true" >> $GITHUB_OUTPUT

      - name: Extract recipient_lookup via streaming
        if: steps.check-new.outputs.is_new == 'true' || github.event.inputs.force_refresh == 'true'
        run: |
          echo "## ðŸ“¥ Extracting recipient_lookup" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Using streaming extraction - downloads only ~1.2GB instead of 217GB" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          uv run python scripts/data/stream_extract_recipient.py \
            --url "${{ steps.check-new.outputs.latest_url }}" \
            --s3-bucket "${{ env.S3_BUCKET }}" \
            2>&1 | tee extract_output.txt

          EXIT_CODE=${PIPESTATUS[0]}

          echo "<details><summary>Extraction Log</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat extract_output.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

          if [ $EXIT_CODE -ne 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âŒ Extraction failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

          # Extract record count from output
          RECORDS=$(grep -oE 'Extracted [0-9,]+ recipients' extract_output.txt | grep -oE '[0-9,]+' || echo "unknown")
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Extracted **$RECORDS** recipient records" >> $GITHUB_STEP_SUMMARY

      - name: Report status
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“¦ USAspending Files in S3" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          aws s3 ls "s3://${{ env.S3_BUCKET }}/raw/usaspending/recipient_lookup/" --recursive --human-readable | tail -10 >> $GITHUB_STEP_SUMMARY || echo "No files found"

  refresh-uspto:
    name: Refresh USPTO Patents
    needs: determine-source
    if: needs.determine-source.outputs.uspto == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"

      - name: Download USPTO data
        id: uspto-download
        env:
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸ“¥ USPTO Patent Data Download" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Install minimal dependencies
          uv pip install boto3 requests

          SUCCESS=true

          # PatentsView (run in parallel)
          echo "### 1ï¸âƒ£ PatentsView Data" >> $GITHUB_STEP_SUMMARY
          uv run python scripts/data/download_uspto.py --dataset patentsview --table patent 2>&1 | tee patentsview.log &
          PID_PV=$!

          # Assignments
          echo "### 2ï¸âƒ£ Patent Assignments" >> $GITHUB_STEP_SUMMARY
          uv run python scripts/data/download_uspto.py --dataset assignments 2>&1 | tee assignments.log &
          PID_AS=$!

          # AI Patents
          echo "### 3ï¸âƒ£ AI Patents" >> $GITHUB_STEP_SUMMARY
          uv run python scripts/data/download_uspto.py --dataset ai_patents 2>&1 | tee ai_patents.log &
          PID_AI=$!

          # Wait for all downloads
          wait $PID_PV || SUCCESS=false
          wait $PID_AS || SUCCESS=false
          wait $PID_AI || SUCCESS=false

          # Report results
          for log in patentsview.log assignments.log ai_patents.log; do
            echo "<details><summary>$log</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat $log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
          done

          if [ "$SUCCESS" = "false" ]; then
            echo "::error::One or more USPTO downloads failed"
            exit 1
          fi

      - name: Check S3 uploads
        if: always()
        run: |
          echo "## ðŸ“¦ USPTO S3 Upload Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for prefix in "raw/uspto/patentsview" "raw/uspto/assignments" "raw/uspto/ai_patents"; do
            echo "### $prefix" >> $GITHUB_STEP_SUMMARY
            aws s3 ls "s3://${{ env.S3_BUCKET }}/$prefix/" --recursive --human-readable | tail -5 >> $GITHUB_STEP_SUMMARY || echo "No files found"
            echo "" >> $GITHUB_STEP_SUMMARY
          done

  summary:
    name: Refresh Summary
    needs: [determine-source, refresh-sbir, refresh-usaspending, refresh-uspto]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Configure AWS credentials
        uses: actions/checkout@v6

      - name: Setup AWS
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Generate summary
        run: |
          echo "# ðŸ“Š Data Refresh Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "**Requested Source:** ${{ github.event.inputs.source }}" >> $GITHUB_STEP_SUMMARY
            echo "**Force Refresh:** ${{ github.event.inputs.force_refresh }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall status
          OVERALL_SUCCESS=true
          TOTAL=0
          SUCCESS_COUNT=0
          FAILED_COUNT=0
          SKIPPED_COUNT=0

          echo "## ðŸ“‹ Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Data Source | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------|--------|--------|" >> $GITHUB_STEP_SUMMARY

          # SBIR
          if [ "${{ needs.determine-source.outputs.sbir }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            JOB_STATUS="${{ needs.refresh-sbir.result }}"

            # Check if job succeeded (Step Functions started)
            if [ "$JOB_STATUS" = "success" ]; then
              # Job succeeded, but check execution status from the detailed report
              # The execution status is shown in the SBIR job's own summary
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Step Functions execution completed - check detailed report above"
            elif [ "$JOB_STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Failed to start or complete execution"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸŽ¯ SBIR Awards | $EMOJI $JOB_STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸŽ¯ SBIR Awards | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          # USAspending
          if [ "${{ needs.determine-source.outputs.usaspending }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            STATUS="${{ needs.refresh-usaspending.result }}"
            if [ "$STATUS" = "success" ]; then
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Recipients extracted via streaming"
            elif [ "$STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Extraction failed"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸ’° USAspending | $EMOJI $STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ’° USAspending | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          # USPTO
          if [ "${{ needs.determine-source.outputs.uspto }}" = "true" ]; then
            TOTAL=$((TOTAL + 1))
            STATUS="${{ needs.refresh-uspto.result }}"
            if [ "$STATUS" = "success" ]; then
              EMOJI="âœ…"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              RESULT="Patent data downloaded via Lambda"
            elif [ "$STATUS" = "failure" ]; then
              EMOJI="âŒ"
              FAILED_COUNT=$((FAILED_COUNT + 1))
              RESULT="Lambda invocation failed"
              OVERALL_SUCCESS=false
            else
              EMOJI="â­ï¸"
              SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
              RESULT="Skipped or cancelled"
            fi
            echo "| ðŸ“œ USPTO Patents | $EMOJI $STATUS | $RESULT |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ðŸ“œ USPTO Patents | â­ï¸ skipped | Not scheduled for this run |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # File details for downloaded sources
          echo "## ðŸ“¦ Downloaded Files" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # USAspending files
          if [ "${{ needs.determine-source.outputs.usaspending }}" = "true" ] && [ "${{ needs.refresh-usaspending.result }}" = "success" ]; then
            echo "### ðŸ’° USAspending Recipients" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/raw/usaspending/recipient_lookup/ --recursive --human-readable --summarize | tail -20
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # USPTO files
          if [ "${{ needs.determine-source.outputs.uspto }}" = "true" ] && [ "${{ needs.refresh-uspto.result }}" = "success" ]; then
            echo "### ðŸ“œ USPTO Patent Data" >> $GITHUB_STEP_SUMMARY

            echo "**PatentsView:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/patentsview/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "**Assignments:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/assignments/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "**AI Patents:**" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            aws s3 ls s3://${{ env.S3_BUCKET }}/uspto/ai-patents/ --recursive --human-readable | tail -5
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Statistics
          echo "## ðŸ“ˆ Statistics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Total jobs:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Successful:** $SUCCESS_COUNT âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** $FAILED_COUNT âŒ" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped:** $SKIPPED_COUNT â­ï¸" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Final verdict
          if [ "$OVERALL_SUCCESS" = "true" ] && [ "$TOTAL" -gt 0 ]; then
            echo "## âœ… Overall Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All data refresh jobs completed successfully!" >> $GITHUB_STEP_SUMMARY
          elif [ "$TOTAL" -eq 0 ]; then
            echo "## âš ï¸ Overall Status: NO JOBS RUN" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "No data sources were selected for refresh." >> $GITHUB_STEP_SUMMARY
          else
            echo "## âŒ Overall Status: FAILURES DETECTED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ One or more data refresh jobs failed. Please review the detailed reports above and check the job logs." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow completed at:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**S3 Bucket:** \`${{ env.S3_BUCKET }}\`" >> $GITHUB_STEP_SUMMARY

  run-pipeline:
    name: Run ETL Pipeline
    needs: [determine-source, refresh-sbir, refresh-usaspending, refresh-uspto, summary]
    if: |
      always() &&
      (github.event.inputs.trigger_dagster != 'false') &&
      (needs.refresh-sbir.result == 'success' || needs.refresh-usaspending.result == 'success' || needs.refresh-uspto.result == 'success')
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: uv sync

      - name: Run SBIR pipeline
        if: needs.refresh-sbir.result == 'success'
        env:
          # Use test or production Neo4j based on environment input (default: production for scheduled runs)
          NEO4J_URI: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_URI || secrets.NEO4J_AURA_URI }}
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_PASSWORD || secrets.NEO4J_AURA_PASSWORD }}
          SBIR_ETL__PIPELINE__ENVIRONMENT: production
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸŽ¯ SBIR Pipeline" >> $GITHUB_STEP_SUMMARY

          uv run dagster job execute \
            -m src.definitions \
            -j sbir_weekly_refresh_job \
            2>&1 | tee sbir_output.txt

          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "âœ… SBIR pipeline completed (extract â†’ validate â†’ Neo4j)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ SBIR pipeline failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 sbir_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Run USAspending enrichment
        if: needs.refresh-usaspending.result == 'success'
        env:
          SBIR_ETL__PIPELINE__ENVIRONMENT: production
          S3_BUCKET: ${{ env.S3_BUCKET }}
        shell: bash
        run: |
          echo "## ðŸ’° USAspending Enrichment" >> $GITHUB_STEP_SUMMARY

          uv run dagster job execute \
            -m src.definitions \
            -j usaspending_iterative_enrichment_job \
            2>&1 | tee usaspending_output.txt

          EXIT_CODE=${PIPESTATUS[0]}
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… USAspending enrichment completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ USAspending enrichment failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 usaspending_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Run USPTO pipeline
        if: needs.refresh-uspto.result == 'success'
        env:
          NEO4J_URI: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_URI || secrets.NEO4J_AURA_URI }}
          NEO4J_USER: neo4j
          NEO4J_PASSWORD: ${{ github.event.inputs.environment == 'test' && secrets.NEO4J_AURA_TEST_PASSWORD || secrets.NEO4J_AURA_PASSWORD }}
          SBIR_ETL__PIPELINE__ENVIRONMENT: production
          S3_BUCKET: ${{ env.S3_BUCKET }}
        run: |
          echo "## ðŸ“œ USPTO Pipeline" >> $GITHUB_STEP_SUMMARY

          uv run dagster job execute \
            -m src.definitions \
            -j uspto_validation_job \
            2>&1 | tee uspto_output.txt

          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "âœ… USPTO pipeline completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ USPTO pipeline failed" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 uspto_output.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
