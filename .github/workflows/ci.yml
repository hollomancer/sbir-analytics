name: CI

# Permissions: Minimal required permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write  # For Codecov and performance checks

on:
  workflow_call:  # Allow other workflows to call this CI workflow
  pull_request:
    branches-ignore: []
    # Run on any PR to any branch
  push:
    branches: [main, develop]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Shared configuration values
  PYTHON_VERSION: "3.11"  # Python version for all jobs
  NEO4J_IMAGE: "neo4j:5"  # Neo4j Docker image version
  NEO4J_USERNAME: "neo4j"  # Default Neo4j username for test containers
  NEO4J_PASSWORD: "password"  # pragma: allowlist secret
  DEFAULT_TIMEOUT: 30  # Default job timeout in minutes
  # Performance test configuration
  PERFORMANCE_SAMPLE_SIZE: "500"  # Sample size for performance regression tests
  PERFORMANCE_BASELINE_CACHE_KEY: "benchmark-baseline-main"  # Cache key for performance baseline
  # Test configuration
  AURA_SAMPLE_LIMIT: "1000"  # Sample limit for Aura Free (stays within 100K node limit)
  DOCKER_SAMPLE_LIMIT: "10000"  # Sample limit for Docker Neo4j (no limit)
  # Artifact retention
  DEFAULT_RETENTION_DAYS: "7"  # Default artifact retention in days
  CET_ARTIFACT_RETENTION_DAYS: "14"  # CET artifacts retained longer for analysis

jobs:
  detect-changes:
    name: Detect File Changes
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Run change detection on all events to enable path-based filtering
    permissions:
      contents: read
      pull-requests: read
    outputs:
      performance: ${{ steps.detect.outputs.performance }}
      cet: ${{ steps.detect.outputs.cet }}
      transition: ${{ steps.detect.outputs.transition }}
      docker: ${{ steps.detect.outputs.docker }}
      workflows: ${{ steps.detect.outputs.workflows }}
      infrastructure: ${{ steps.detect.outputs.infrastructure }}
      docs-only: ${{ steps.detect.outputs.docs-only }}
    steps:
      - uses: actions/checkout@v6
      - name: Detect changes
        id: detect
        uses: ./.github/actions/detect-changes
        with:
          filters: |
            performance:
              - 'src/enrichers/**'
              - 'src/assets/**'
              - 'src/utils/performance_monitor.py'
              - 'src/extractors/**'
              - 'scripts/performance/benchmark_enrichment.py'
              - 'scripts/performance/detect_performance_regression.py'
              - 'config/base.yaml'
            cet:
              - 'src/ml/config/**'
              - 'src/assets/jobs/cet_*.py'
              - 'tests/unit/ml/**'
              - 'tests/integration/ml/**'
              - 'tests/e2e/ml/**'
              - 'config/cet/**'
              - 'pyproject.toml'
              - 'uv.lock'
            transition:
              - 'src/assets/transition/**'
              - 'src/assets/jobs/transition_job.py'
              - 'tests/e2e/transition/**'
              - 'tests/integration/transition/**'
              - 'docs/transition/**'
            docker:
              - 'Dockerfile*'
              - 'docker-compose*.yml'
              - '.dockerignore'
              - 'docker/**'
            workflows:
              - '.github/workflows/**'
              - '.github/actions/**'
            infrastructure:
              - 'infrastructure/cdk/**'
            docs-only:
              - '**/*.md'
              - 'docs/**'
              - '!**/*.py'
              - '!**/*.yaml'
              - '!**/*.yml'
              - '!**/*.json'
              - '!**/*.toml'
              - '!**/*.lock'
              - '!Makefile'
              - '!.github/**'

      - name: Generate workflow visualization
        run: |
          echo "## ðŸ”„ CI Workflow Execution Plan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph TD" >> $GITHUB_STEP_SUMMARY
          echo "  A[Detect Changes] --> B[Code Quality]" >> $GITHUB_STEP_SUMMARY
          echo "  A --> D[Verify Setup]" >> $GITHUB_STEP_SUMMARY
          echo "  B --> E[Tests]" >> $GITHUB_STEP_SUMMARY
          echo "  D --> E" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.detect.outputs.cet }}" = "true" ]; then
            echo "  E --> F[CET Tests]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.transition }}" = "true" ]; then
            echo "  E --> G[Transition MVP]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.performance }}" = "true" ]; then
            echo "  A --> H[Performance Check]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.docker }}" = "true" ]; then
            echo "  A --> I[Container Build]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.workflows }}" = "true" ]; then
            echo "  A --> J[Workflow Lint]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.infrastructure }}" = "true" ]; then
            echo "  A --> K[CDK Validate]" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY

  code-quality:
    name: Code Quality (Lint + Type Check)
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: needs.detect-changes.outputs.docs-only != 'true'
    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"

      - name: Run Ruff lint
        run: uv run python -m ruff check src tests

      - name: Run Ruff format check
        run: uv run python -m ruff format --check src tests

      - name: Run MyPy type checker
        run: uv run python -m mypy src

  verify-setup-script:
    name: Verify Setup Script
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: needs.detect-changes.outputs.docs-only != 'true'
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Run setup script
        env:
          RPY2_CFFI_MODE: ABI  # Build rpy2 in ABI mode without requiring R
        run: |
          chmod +x scripts/setup_dev.sh
          ./scripts/setup_dev.sh

      - name: Verify venv activation
        run: |
          source .venv/bin/activate
          python --version
          python -c "import pydantic; print('Pydantic installed')"

  workflow-lint:
    name: Validate GitHub Actions Workflows
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Always run - fast job, no need to filter by changes
    if: needs.detect-changes.outputs.docs-only != 'true'
    steps:
      - uses: actions/checkout@v6

      - name: Validate workflow YAML syntax
        run: |
          echo "## ðŸ” Workflow Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          ERRORS=0
          for f in .github/workflows/*.yml; do
            if python3 -c "import yaml; yaml.safe_load(open('$f'))" 2>/dev/null; then
              echo "âœ… $f" >> $GITHUB_STEP_SUMMARY
            else
              echo "âŒ $f - Invalid YAML" >> $GITHUB_STEP_SUMMARY
              ERRORS=$((ERRORS + 1))
            fi
          done

          if [ $ERRORS -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**$ERRORS workflow(s) have invalid YAML**" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Check for common workflow issues
        run: |
          # Check for deprecated actions
          if grep -r "actions/checkout@v3\|actions/setup-python@v4" .github/workflows/; then
            echo "::warning::Found deprecated action versions. Consider upgrading."
          fi

          # Check for hardcoded secrets (basic check)
          if grep -rE "(password|secret|token)\s*[:=]\s*['\"][^$]" .github/workflows/ --include="*.yml" | grep -v "pragma: allowlist"; then
            echo "::error::Potential hardcoded secrets found in workflows"
            exit 1
          fi

  infrastructure-validate:
    name: Validate CDK Infrastructure
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    # Always run - catches CDK issues early, not expensive
    if: needs.detect-changes.outputs.docs-only != 'true'
    steps:
      - uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Setup Node.js (for CDK CLI)
        uses: actions/setup-node@v6
        with:
          node-version: "20"

      - name: Install CDK dependencies
        working-directory: infrastructure/cdk
        run: |
          python -m venv .venv
          source .venv/bin/activate
          pip install -r requirements.txt
          npm install -g aws-cdk

      - name: Prepare Lambda functions
        run: |
          # Copy common module to Lambda directories (required for CDK synth)
          for dir in scripts/lambda/*/; do
            if [ -d "$dir" ] && [ "$(basename $dir)" != "common" ]; then
              cp -r scripts/lambda/common/* "$dir" 2>/dev/null || true
            fi
          done

      - name: CDK Synth (validate templates)
        working-directory: infrastructure/cdk
        env:
          CDK_DEFAULT_ACCOUNT: "123456789012"
          CDK_DEFAULT_REGION: "us-east-2"
        run: |
          source .venv/bin/activate
          echo "## ðŸ—ï¸ CDK Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Run CDK synth with python directly (bypass cdk.json's uv command)
          if cdk synth --app "python app.py" --quiet 2>&1; then
            echo "âœ… CDK synth successful" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ CDK synth failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  test:
    name: Tests (${{ matrix.suite.name }})
    needs: [detect-changes, code-quality, verify-setup-script]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: needs.detect-changes.outputs.docs-only != 'true'

    env:
      ENVIRONMENT: test
      SBIR_ETL__EXTRACTION__SAMPLE_LIMIT: "10000"

    strategy:
      fail-fast: false
      matrix:
        suite:
          - name: unit-fast
            path: tests/unit/
            args: '-m "not slow and not integration"'
            needs_neo4j: false
          - name: unit-slow
            path: tests/unit/
            args: '-m "slow and not integration"'
            needs_neo4j: false
          - name: integration
            path: tests/integration/
            args: '-m "integration and not slow"'
            needs_neo4j: true

    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "true"
          install-pyreadstat: "true"

      - name: Start Neo4j service
        if: matrix.suite.needs_neo4j
        uses: ./.github/actions/start-neo4j
        with:
          container-name: test-neo4j
          neo4j-image: ${{ env.NEO4J_IMAGE }}
          neo4j-user: ${{ env.NEO4J_USERNAME }}
          neo4j-password: ${{ env.NEO4J_PASSWORD }}

      - name: Run ${{ matrix.suite.name }} tests
        env:
          NEO4J_URI: "bolt://localhost:7687"
          NEO4J_USER: ${{ env.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}
        run: |
          uv run pytest ${{ matrix.suite.path }} ${{ matrix.suite.args }} \
            --cov=src --cov-report=xml --cov-report=term \
            --junitxml=test-results-${{ matrix.suite.name }}.xml -v

      - name: Stop Neo4j service
        if: always() && matrix.suite.needs_neo4j
        uses: ./.github/actions/stop-neo4j
        with:
          container-name: test-neo4j

      - name: Upload coverage
        if: always()
        uses: codecov/codecov-action@v5
        with:
          files: ./coverage.xml
          flags: ${{ matrix.suite.name }}
          name: ${{ matrix.suite.name }}-coverage

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-${{ matrix.suite.name }}
          path: test-results-${{ matrix.suite.name }}.xml
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

  test-report:
    name: Test Report Summary
    needs: [test]
    if: always() && github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    permissions:
      pull-requests: write
    steps:
      - uses: actions/checkout@v6

      - name: Download all test results
        uses: actions/download-artifact@v6
        with:
          pattern: test-results-*
          path: test-results/

      - name: Aggregate test results
        run: |
          cat > /tmp/test_summary.md << 'EOF'
          ## ðŸ§ª Test Results Summary

          EOF

          TOTAL_PASSED=0
          TOTAL_FAILED=0
          TOTAL_SKIPPED=0

          for suite in unit-fast unit-slow integration; do
            XML_FILE="test-results/test-results-${suite}/test-results-${suite}.xml"
            if [ -f "$XML_FILE" ]; then
              # Parse JUnit XML for test counts
              TESTS=$(grep -oP 'tests="\K[0-9]+' "$XML_FILE" | head -1 || echo "0")
              FAILURES=$(grep -oP 'failures="\K[0-9]+' "$XML_FILE" | head -1 || echo "0")
              ERRORS=$(grep -oP 'errors="\K[0-9]+' "$XML_FILE" | head -1 || echo "0")
              SKIPPED=$(grep -oP 'skipped="\K[0-9]+' "$XML_FILE" | head -1 || echo "0")

              FAILED=$((FAILURES + ERRORS))
              PASSED=$((TESTS - FAILED - SKIPPED))

              TOTAL_PASSED=$((TOTAL_PASSED + PASSED))
              TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
              TOTAL_SKIPPED=$((TOTAL_SKIPPED + SKIPPED))

              if [ "$FAILED" -gt 0 ]; then
                echo "### âŒ ${suite} Failures" >> /tmp/test_summary.md
                echo "" >> /tmp/test_summary.md
                # Extract failure messages from JUnit XML
                grep -oP '<failure[^>]*message="\K[^"]+' "$XML_FILE" | head -5 | while read msg; do
                  echo "- $msg" >> /tmp/test_summary.md
                done
                echo "" >> /tmp/test_summary.md
              fi
            fi
          done

          # Add summary at top
          sed -i "3i **Total:** $TOTAL_PASSED âœ… passed, $TOTAL_FAILED âŒ failed, $TOTAL_SKIPPED â­ï¸ skipped\n" /tmp/test_summary.md

          if [ "$TOTAL_FAILED" -eq 0 ]; then
            sed -i "3i ### âœ… All Tests Passed!\n" /tmp/test_summary.md
          fi

      - name: Comment PR
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('/tmp/test_summary.md', 'utf8');

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸ§ª Test Results Summary')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: summary
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: summary
              });
            }

  container-build-test:
    name: Container Build and Test
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 20
    # Optimized: Only required on push to main/develop, optional on PRs with docker changes
    # Skip on PRs without docker changes or docs-only to save ~20 min
    if: |
      always() &&
      needs.detect-changes.outputs.docs-only != 'true' &&
      (github.event_name == 'push' ||
       (github.event_name == 'pull_request' && needs.detect-changes.outputs.docker == 'true') ||
       github.event_name == 'workflow_dispatch')
    permissions:
      contents: read
      packages: write  # For pushing cache to GHCR
    env:
      REGISTRY: ghcr.io
      IMAGE_NAME: ${{ github.repository }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx
        with:
          setup-qemu: "true"

      - name: Log in to GitHub Container Registry
        if: github.event_name == 'push'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Prepare .env for CI
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build image and load into Docker daemon
        id: build
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile
          push: false
          load: true
          tags: |
            sbir-analytics:ci-${{ github.sha }}
          build-args: |
            WITH_R=false
            BUILDKIT_INLINE_CACHE=1
          # Unified cache scope shared across workflows
          cache-from: |
            type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
            type=gha,scope=sbir-python-deps
          cache-to: |
            type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max
            type=gha,mode=max,scope=sbir-python-deps

      - name: Tag and push to registry
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          docker tag sbir-analytics:ci-${{ github.sha }} ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          docker tag sbir-analytics:ci-${{ github.sha }} ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}

      - name: Show built image
        run: docker images --format 'table {{.Repository}}:{{.Tag}}\t{{.ID}}\t{{.Size}}' | rg "sbir-analytics" || true

      - name: Smoke test entrypoint
        run: |
          set -eux
          echo "Running smoke test: validate image entrypoint responds to 'dagster --version'"
          # Validate the built image can execute the dagster CLI. The image tag matches the one we built above.
          docker run --rm "sbir-analytics:ci-${{ github.sha }}" dagster --version

      - name: Start test compose and run pytest
        env:
          # Provide the GITHUB_SHA used in the test compose overlay variable
          GITHUB_SHA: ${{ github.sha }}
        run: |
          set -eux
          # Use profile-based compose to bring up ephemeral neo4j + app (app configured to run pytest)
          docker compose --profile ci up --abort-on-container-exit --build neo4j app
        timeout-minutes: 30

      - name: Tear down test compose (cleanup)
        if: always()
        run: |
          set -eux
          docker compose --profile ci down --remove-orphans --volumes || true

      - name: Upload logs on failure
        if: failure()
        uses: ./.github/actions/upload-artifacts
        with:
          name: ci-compose-logs
          path: |
            logs
            reports || true
          if-no-files-found: ignore

  performance-check:
    name: Performance Regression Check
    needs: [detect-changes]
    # Condition: Run only when performance-sensitive files change (or workflow_dispatch), skip docs-only
    if: |
      needs.detect-changes.outputs.docs-only != 'true' &&
      needs.detect-changes.outputs.performance == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 20
    continue-on-error: true  # Don't block PRs on performance checks
    permissions:
      id-token: write        # For OIDC authentication to AWS
      contents: read         # For checking out code
      pull-requests: write   # For posting PR comments
      checks: write          # For creating check runs
    env:
      AWS_REGION: us-east-2
      S3_BUCKET: sbir-etl-production-data
      SBIR_ANALYTICS_S3_BUCKET: sbir-etl-production-data
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Configure AWS credentials
        id: aws-credentials
        continue-on-error: true  # Optional: Falls back to local data if AWS not configured
        if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Cache baseline benchmark
        uses: actions/cache@v4
        with:
          path: reports/benchmarks/baseline.json
          key: benchmark-baseline-${{ github.ref_name }}-${{ hashFiles('scripts/performance/**/*.py') }}
          restore-keys: |
            benchmark-baseline-${{ github.ref_name }}-
            benchmark-baseline-main-
            benchmark-baseline-

      - name: Check if baseline exists
        id: baseline_check
        run: |
          if [ -f "reports/benchmarks/baseline.json" ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "âœ“ Baseline found"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "âš  No baseline found - this is first run or cache miss"
          fi

      - name: Check if AWS credentials are available
        id: check_aws
        run: |
          # Check if AWS credentials step succeeded AND credentials actually work
          if [ "${{ steps.aws-credentials.outcome }}" == "success" ]; then
            # Verify credentials are actually usable
            if aws sts get-caller-identity >/dev/null 2>&1; then
              echo "aws_available=true" >> $GITHUB_OUTPUT
              echo "âœ“ AWS credentials configured and verified"
            else
              echo "aws_available=false" >> $GITHUB_OUTPUT
              echo "âš ï¸  AWS credentials step succeeded but credentials not usable"
            fi
          else
            echo "aws_available=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  AWS credentials step did not succeed (outcome: ${{ steps.aws-credentials.outcome }})"
          fi

      - name: Prepare test data for benchmark
        if: steps.check_aws.outputs.aws_available == 'true'
        run: |
          # Use test fixture if S3 data not available
          mkdir -p data/raw/sbir
          if ! aws s3 ls s3://${{ env.S3_BUCKET }}/data/raw/sbir/award_data.csv >/dev/null 2>&1; then
            echo "âš ï¸  S3 data not available, using test fixture"
            cp tests/fixtures/sbir_sample.csv data/raw/sbir/award_data.csv
          fi

      - name: Run Performance Regression Detection
        id: regression
        if: steps.check_aws.outputs.aws_available == 'true'
        continue-on-error: true
        run: |
          mkdir -p reports/benchmarks
          uv run python scripts/performance/detect_performance_regression.py \
            --sample-size ${{ env.PERFORMANCE_SAMPLE_SIZE }} \
            --output-json /tmp/regression.json \
            --output-markdown /tmp/report.md \
            --output-html /tmp/report.html \
            --output-github-comment /tmp/pr_comment.md \
            --fail-on-regression

      - name: Generate summary for first run
        if: steps.baseline_check.outputs.baseline_exists == 'false'
        run: |
          echo "## ðŸ“Š Performance Benchmark - Baseline Established" >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          echo "This is the first performance run. Baseline has been established." >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          if [ -f "/tmp/regression.json" ]; then
            echo "**Current Metrics:**" >> /tmp/pr_comment.md
            echo "" >> /tmp/pr_comment.md
            jq '.current_metrics | to_entries | .[] | "- **\(.key):** \(.value)"' /tmp/regression.json >> /tmp/pr_comment.md
          fi

      - name: Create baseline if needed
        if: steps.baseline_check.outputs.baseline_exists == 'false' && success()
        run: |
          # Ensure test data is available
          mkdir -p data/raw/sbir
          if [ ! -f "data/raw/sbir/award_data.csv" ]; then
            echo "Using test fixture for baseline"
            cp tests/fixtures/sbir_sample.csv data/raw/sbir/award_data.csv
          fi
          export PYTHONPATH="${PYTHONPATH}:${GITHUB_WORKSPACE}"
          uv run python scripts/performance/benchmark_enrichment.py \
            --sample-size ${{ env.PERFORMANCE_SAMPLE_SIZE }} \
            --save-as-baseline

      - name: Upload reports
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: performance-reports
          path: |
            /tmp/regression.json
            /tmp/report.md
            /tmp/report.html
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Parse regression results
        id: parse_results
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity // "UNKNOWN"' /tmp/regression.json)
            DURATION_DELTA=$(jq -r '.duration_delta_percent // 0' /tmp/regression.json)
            MEMORY_DELTA=$(jq -r '.memory_delta_percent // 0' /tmp/regression.json)
            MATCH_RATE=$(jq -r '.match_rate // 0' /tmp/regression.json)
            echo "severity=$SEVERITY" >> $GITHUB_OUTPUT
            echo "duration_delta=$DURATION_DELTA" >> $GITHUB_OUTPUT
            echo "memory_delta=$MEMORY_DELTA" >> $GITHUB_OUTPUT
            echo "match_rate=$MATCH_RATE" >> $GITHUB_OUTPUT
          fi

      - name: Set check status - Success
        if: steps.regression.outcome == 'success' && github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'success',
              output: {
                title: 'âœ“ Performance Check Passed',
                summary: 'No performance regressions detected',
                text: 'All performance metrics are within acceptable thresholds'
              }
            });

      - name: Set check status - Failure
        if: steps.regression.outcome == 'failure' && github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'failure',
              output: {
                title: 'âŒ Performance Check Failed',
                summary: 'Performance regression detected',
                text: 'Review the PR comment and artifacts for detailed regression analysis'
              }
            });

      - name: Comment on PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            let comment_body = '';

            // Try to read generated comment
            if (fs.existsSync('/tmp/pr_comment.md')) {
              comment_body = fs.readFileSync('/tmp/pr_comment.md', 'utf8');
            } else {
              comment_body = '## âš ï¸ Performance Check\n\nNo report generated. Please check the workflow logs.';
            }

            // Add alert summary if regression detected
            if (fs.existsSync('/tmp/regression.json')) {
              const results = JSON.parse(fs.readFileSync('/tmp/regression.json', 'utf8'));
              if (results.severity === 'FAILURE' || results.severity === 'WARNING') {
                comment_body += '\n\n## ðŸš¨ Performance Alerts\n\n';
                if (results.issues && results.issues.length > 0) {
                  results.issues.forEach(issue => {
                    comment_body += `- **${issue.alert_type}:** ${issue.message}\n`;
                  });
                }
              }
            }

            // Add link to artifacts
            comment_body += '\n\n---\n\n';
            comment_body += '**ðŸ“Ž Artifacts:** [View Performance Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment_body
            });

      - name: Fail if FAILURE severity detected
        if: steps.regression.outcome == 'failure'
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity' /tmp/regression.json)
            if [ "$SEVERITY" = "FAILURE" ]; then
              echo "âŒ Performance regression FAILURE detected"
              echo ""
              echo "## Alert Summary"
              jq '.issues' /tmp/regression.json
              echo ""
              echo "Regression thresholds exceeded. Please review and address before merging."
              exit 1
            fi
          fi

      - name: Print regression report
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            echo "## Regression Detection Summary"
            echo ""
            jq '.' /tmp/regression.json
          fi

  cet-tests:
    name: CET Smoke Test
    needs: [detect-changes]
    if: |
      needs.detect-changes.outputs.docs-only != 'true' &&
      github.event_name == 'pull_request' &&
      needs.detect-changes.outputs.cet == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      neo4j:
        image: neo4j:5
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/password
        options: >-
          --health-cmd="bash -c 'cypher-shell -u neo4j -p password \"RETURN 1\" || exit 1'"
          --health-interval=10s
          --health-timeout=10s
          --health-retries=12
          --health-start-period=30s

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          neo4j-image: ${{ env.NEO4J_IMAGE }}
          neo4j-username: ${{ env.NEO4J_USERNAME }}
          neo4j-password: ${{ env.NEO4J_PASSWORD }}
          default-timeout: ${{ env.DEFAULT_TIMEOUT }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "false"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Prepare CET configs and test data
        run: |
          set -eux
          mkdir -p config/cet data/processed

          # Generate taxonomy for smoke test
          cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
          YAML

          # Generate classification config for smoke test
          cat > config/cet/classification.yaml << 'YAML'
          model_version: "vtest"
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 64}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          tfidf:
            min_df: 1
            max_df: 1.0
            ngram_range: [1, 2]
            keyword_boost_factor: 2.0
          feature_selection: {enabled: false}
          calibration: {method: "sigmoid", cv: 2}
          YAML

          # Generate test data for smoke test
          cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"smoke_award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          NDJSON

      - name: Setup Neo4j environment variables
        uses: ./.github/actions/setup-neo4j-service
        with:
          username: ${{ env.NEO4J_USERNAME }}
          password: ${{ env.NEO4J_PASSWORD }}
          auth-mode: password
          uri: ${{ env.NEO4J_URI }}

      - name: Run CET smoke test
        run: |
          set -eux
          uv run python scripts/ci/cet_smoke_test.py

      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" \) -print || true

      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-smoke-artifacts
          retention-days: ${{ env.CET_ARTIFACT_RETENTION_DAYS }}
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/cet_*.*
            data/processed/*.checks.json
            data/loaded/neo4j/*.checks.json
          if-no-files-found: warn

      - name: Success note
        if: success()
        run: echo "CET smoke test completed successfully"

  cet-dev-e2e:
    name: CET Dev E2E (Docker Compose)
    needs: test
    # Only run on push to main/develop, skip on PRs
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'push'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      # Allow override via repository secrets; fall back to defaults for dev runs
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: ${{ secrets.NEO4J_USER || 'neo4j' }}
      NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD || 'password' }}
      IMAGE_NAME: sbir-analytics
      IMAGE_TAG: dev-${{ github.sha }}
      STARTUP_TIMEOUT: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx

      - name: Prepare .env for dev compose
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Build runtime image
        run: |
          set -eux
          docker build --target runtime \
            --cache-from type=local,src=/tmp/.buildx-cache \
            --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
            -t "${IMAGE_NAME}:${IMAGE_TAG}" .

          # Move cache to avoid growing indefinitely
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

      - name: Start development compose stack (dev profile)
        run: |
          set -eux
          docker compose --env-file .env --profile dev up -d --build

      - name: Wait for Neo4j bolt port
        run: |
          set -eux
          sudo apt-get update -y
          sudo apt-get install -y --no-install-recommends netcat-openbsd
          TIMEOUT=${STARTUP_TIMEOUT}
          i=0
          until nc -z localhost 7687 || [ $i -ge $TIMEOUT ]; do
            i=$((i+5))
            echo "Waiting for Neo4j on 7687..."
            sleep 5
          done
          if ! nc -z localhost 7687; then
            echo "Neo4j did not become available on bolt port 7687"
            docker compose --env-file .env --profile dev ps || true
            docker logs $(docker ps --filter "name=sbir-neo4j" --format '{{.Names}}' | head -n 1) || true
            exit 1
          fi
          echo "Neo4j appears reachable on 7687"

      - name: Run CET full pipeline via Makefile helper
        env:
          # Ensure composer commands inside Make target use same .env
          DOCKER_BUILDKIT: 1
        run: |
          set -eux

  e2e-aura:
    name: E2E Tests (Neo4j Aura)
    needs: test
    # Only run on push to main/develop - secrets checked at step level
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'push'
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      NEO4J_URI: ${{ secrets.NEO4J_AURA_URI }}
      NEO4J_USER: ${{ secrets.NEO4J_AURA_USER }}
      NEO4J_PASSWORD: ${{ secrets.NEO4J_AURA_PASSWORD }}
      SBIR_ETL__PIPELINE__SAMPLE_SIZE: "1000"  # Aura Free limit

    steps:
      - name: Check for Aura credentials
        id: check-creds
        run: |
          if [ -z "${{ secrets.NEO4J_AURA_URI }}" ] || [ -z "${{ secrets.NEO4J_AURA_PASSWORD }}" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "âš ï¸ Neo4j Aura credentials not configured, skipping E2E tests"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - uses: actions/checkout@v6
        if: steps.check-creds.outputs.skip != 'true'

      - name: Setup Python and UV
        if: steps.check-creds.outputs.skip != 'true'
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "false"
          cache-venv: "true"

      - name: Clear Neo4j Aura database
        if: steps.check-creds.outputs.skip != 'true'
        run: |
          echo "ðŸ§¹ Clearing Neo4j Aura database before test"
          uv run python -c "
          from neo4j import GraphDatabase
          import os
          driver = GraphDatabase.driver(
              os.environ['NEO4J_URI'],
              auth=(os.environ['NEO4J_USER'], os.environ['NEO4J_PASSWORD'])
          )
          with driver.session() as session:
              # Delete all nodes and relationships
              session.run('MATCH (n) DETACH DELETE n')
              result = session.run('MATCH (n) RETURN count(n) as count')
              count = result.single()['count']
              print(f'âœ… Database cleared. Node count: {count}')
          driver.close()
          "

      - name: Run E2E tests with Aura
        if: steps.check-creds.outputs.skip != 'true'
        run: |
          echo "ðŸ§ª Running E2E tests against Neo4j Aura"
          uv run pytest tests/e2e/ \
            -v \
            --tb=short \
            --maxfail=3 \
            -m "not slow"

      - name: Verify Neo4j data loaded
        if: steps.check-creds.outputs.skip != 'true' && success()
        run: |
          echo "âœ… Verifying data in Neo4j Aura"
          uv run python -c "
          from neo4j import GraphDatabase
          import os
          driver = GraphDatabase.driver(
              os.environ['NEO4J_URI'],
              auth=(os.environ['NEO4J_USER'], os.environ['NEO4J_PASSWORD'])
          )
          with driver.session() as session:
              # Count nodes and relationships
              nodes = session.run('MATCH (n) RETURN count(n) as count').single()['count']
              rels = session.run('MATCH ()-[r]->() RETURN count(r) as count').single()['count']
              print(f'ðŸ“Š Nodes: {nodes:,}')
              print(f'ðŸ“Š Relationships: {rels:,}')

              # Check we're within Aura free limits
              if nodes > 200000:
                  print(f'âš ï¸  Warning: Node count ({nodes:,}) exceeds Aura free limit (200k)')
              if rels > 400000:
                  print(f'âš ï¸  Warning: Relationship count ({rels:,}) exceeds Aura free limit (400k)')
          driver.close()
          "

      - name: Cleanup on failure
        if: failure()
        run: |
          echo "ðŸ§¹ Cleaning up Neo4j Aura after failure"
          uv run python -c "
          from neo4j import GraphDatabase
          import os
          driver = GraphDatabase.driver(
              os.environ['NEO4J_URI'],
              auth=(os.environ['NEO4J_USER'], os.environ['NEO4J_PASSWORD'])
          )
          with driver.session() as session:
              session.run('MATCH (n) DETACH DELETE n')
              print('âœ… Database cleared')
          driver.close()
          " || echo "âš ï¸  Cleanup failed, manual intervention may be needed"
          # Use the Make helper which runs the job inside the app container via docker compose
          make cet-pipeline-dev

      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          echo "Artifacts produced under data/ and reports/:"
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" -o -name "*.ndjson" \) -print || true
          find reports -maxdepth 4 -type f \( -name "*.json" -o -name "*.md" -o -name "*.html" \) -print || true

      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-dev-e2e-artifacts
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/*.checks.json
            data/processed/*.ndjson
            reports/analytics/**
            reports/alerts/**
            reports/benchmarks/**
          retention-days: ${{ env.CET_ARTIFACT_RETENTION_DAYS }}
          if-no-files-found: warn

      - name: Tear down dev compose stack (always)
        if: always()
        run: |
          set -eux
          docker compose --env-file .env --profile dev down --remove-orphans || true
          echo "Compose stack torn down"

      - name: Success note
        if: success()
        run: echo "CET Dev E2E completed successfully"

      - name: Failure diagnostics
        if: failure()
        run: |
          echo "Job failed â€” gathering diagnostic information"
          docker compose --env-file .env --profile dev ps || true
          echo "Recent container logs (if available):"
          for name in $(docker ps --format '{{.Names}}'); do
            echo "==== logs: $name ===="
            docker logs --tail 200 "$name" || true
          done

  transition-mvp:
    name: Run Transition MVP (shim) and gate on validation summary
    needs: [detect-changes]
    # Run on PR only if transition files changed, always run on push to main/develop, skip docs-only
    if: |
      needs.detect-changes.outputs.docs-only != 'true' &&
      (github.event_name == 'push' || needs.detect-changes.outputs.transition == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read

    concurrency:
      group: transition-mvp-${{ github.ref }}
      cancel-in-progress: true

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Run Transition MVP (shim)
        run: |
          make transition-mvp-run

      - name: Export precision audit sample (CSV)
        run: |
          uv run python scripts/transition/transition_precision_audit.py --export-csv reports/validation/vendor_resolution_audit_sample.csv

      - name: Upload precision audit sample
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-precision-audit-sample
          path: reports/validation/vendor_resolution_audit_sample.csv
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Gate on validation summary
        id: gate
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_validation.py

      - name: Gate on evidence completeness
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_evidence.py

      - name: Gate on analytics checks
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_analytics.py

      - name: Upload validation summary
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-validation
          path: |
            reports/validation/transition_mvp.json
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Upload processed artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-artifacts
          path: |
            data/processed/vendor_resolution.*
            data/processed/transitions.*
            data/processed/transitions_evidence.*
            data/processed/transition_analytics.*
            data/processed/contracts_sample.*
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

  ml-smoke-test:
    name: ML Smoke Test
    needs: [detect-changes]
    if: |
      needs.detect-changes.outputs.docs-only != 'true' &&
      (needs.detect-changes.outputs.cet == 'true' || github.event_name == 'workflow_dispatch')
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v6

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "false"
          cache-venv: "true"

      - name: Run ML smoke test
        env:
          SBIR_ETL__CET__SAMPLE_SIZE: 50
        run: |
          uv run python scripts/ci/cet_smoke_test.py

      - name: Validate model loading
        run: |
          uv run python -c "
          from src.ml.cet_classifier import CETClassifier
          clf = CETClassifier()
          print('âœ… Model loads successfully')
          "

  test-s3-integration:
    name: S3 Integration Tests
    needs: [detect-changes, test]
    if: |
      needs.detect-changes.outputs.docs-only != 'true' &&
      !contains(github.event.pull_request.labels.*.name, 'skip-cloud-tests')
    runs-on: ubuntu-latest
    timeout-minutes: 5
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v6

      - name: Configure AWS credentials
        continue-on-error: true
        id: aws-creds
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_TEST_ROLE_ARN }}
          aws-region: us-east-2

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"

      - name: Run S3 integration tests
        if: steps.aws-creds.outcome == 'success'
        env:
          TEST_S3_BUCKET: sbir-analytics-test
        run: |
          uv run pytest tests/integration/test_s3_operations.py -v -m s3

      - name: Skip S3 tests (no credentials)
        if: steps.aws-creds.outcome != 'success'
        run: |
          echo "âš ï¸ AWS credentials not available, skipping S3 integration tests"
          echo "This is expected for external PRs"

  ci-summary:
    name: CI Summary
    needs: [detect-changes, code-quality, verify-setup-script, test, container-build-test, performance-check, cet-tests, cet-dev-e2e, e2e-aura, transition-mvp, ml-smoke-test, test-s3-integration]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Generate CI summary
        run: |
          echo "## ðŸ“Š CI Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY

          # Core jobs
          echo "| ðŸ” Detect Changes | ${{ needs.detect-changes.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.detect-changes.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ§¹ Code Quality | ${{ needs.code-quality.result == 'success' && 'âœ…' || needs.code-quality.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.code-quality.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| âœ… Verify Setup | ${{ needs.verify-setup-script.result == 'success' && 'âœ…' || needs.verify-setup-script.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.verify-setup-script.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ§ª Tests (3 suites) | ${{ needs.test.result == 'success' && 'âœ…' || needs.test.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY

          # Conditional jobs
          if [ "${{ needs.container-build-test.result }}" != "skipped" ]; then
            echo "| ðŸ³ Container Build | ${{ needs.container-build-test.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.container-build-test.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance-check.result }}" != "skipped" ]; then
            echo "| âš¡ Performance Check | ${{ needs.performance-check.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.performance-check.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.ml-smoke-test.result }}" != "skipped" ]; then
            echo "| ðŸ¤– ML Smoke Test | ${{ needs.ml-smoke-test.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.ml-smoke-test.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.test-s3-integration.result }}" != "skipped" ]; then
            echo "| â˜ï¸ S3 Integration | ${{ needs.test-s3-integration.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.test-s3-integration.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.cet-tests.result }}" != "skipped" ]; then
            echo "| ðŸ¤– CET Tests | ${{ needs.cet-tests.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.cet-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.cet-dev-e2e.result }}" != "skipped" ]; then
            echo "| ðŸ”¬ CET E2E (Docker) | ${{ needs.cet-dev-e2e.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.cet-dev-e2e.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.e2e-aura.result }}" != "skipped" ]; then
            echo "| â˜ï¸ E2E (Aura) | ${{ needs.e2e-aura.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.e2e-aura.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.transition-mvp.result }}" != "skipped" ]; then
            echo "| ðŸ”„ Transition MVP | ${{ needs.transition-mvp.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.transition-mvp.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Execution Flow" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph LR" >> $GITHUB_STEP_SUMMARY
          echo "  A[Detect Changes] --> B[Code Quality]" >> $GITHUB_STEP_SUMMARY
          echo "  A --> D[Verify Setup]" >> $GITHUB_STEP_SUMMARY
          echo "  B --> E[Tests]" >> $GITHUB_STEP_SUMMARY
          echo "  D --> E" >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY

      - name: Check for failures
        if: |
          needs.code-quality.result == 'failure' ||
          needs.test.result == 'failure' ||
          needs.container-build-test.result == 'failure' ||
          needs.performance-check.result == 'failure' ||
          needs.ml-smoke-test.result == 'failure' ||
          needs.test-s3-integration.result == 'failure' ||
          needs.cet-tests.result == 'failure' ||
          needs.cet-dev-e2e.result == 'failure' ||
          needs.e2e-aura.result == 'failure' ||
          needs.transition-mvp.result == 'failure'
        run: |
          echo "::error::One or more CI jobs failed"
          exit 1
