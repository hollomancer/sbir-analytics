name: CI

# Permissions: Minimal required permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write  # For Codecov and performance checks

on:
  pull_request:
    branches-ignore: []
    # Run on any PR to any branch
  push:
    branches: [main, develop]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Shared configuration values
  PYTHON_VERSION: "3.11"  # Python version for all jobs
  NEO4J_IMAGE: "neo4j:5"  # Neo4j Docker image version
  NEO4J_USERNAME: "neo4j"  # Default Neo4j username for test containers
  NEO4J_PASSWORD: "password"  # pragma: allowlist secret
  DEFAULT_TIMEOUT: 30  # Default job timeout in minutes
  # Performance test configuration
  PERFORMANCE_SAMPLE_SIZE: "500"  # Sample size for performance regression tests
  PERFORMANCE_BASELINE_CACHE_KEY: "benchmark-baseline-main"  # Cache key for performance baseline
  # Test configuration
  AURA_SAMPLE_LIMIT: "1000"  # Sample limit for Aura Free (stays within 100K node limit)
  DOCKER_SAMPLE_LIMIT: "10000"  # Sample limit for Docker Neo4j (no limit)
  # Artifact retention
  DEFAULT_RETENTION_DAYS: "7"  # Default artifact retention in days
  CET_ARTIFACT_RETENTION_DAYS: "14"  # CET artifacts retained longer for analysis

jobs:
  detect-changes:
    name: Detect File Changes
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Run change detection on all events to enable path-based filtering
    permissions:
      contents: read
      pull-requests: read
    outputs:
      performance: ${{ steps.detect.outputs.performance }}
      cet: ${{ steps.detect.outputs.cet }}
      transition: ${{ steps.detect.outputs.transition }}
      docker: ${{ steps.detect.outputs.docker }}
      docs-only: ${{ steps.detect.outputs.docs-only }}
    steps:
      - uses: actions/checkout@v6
      - name: Detect changes
        id: detect
        uses: ./.github/actions/detect-changes
        with:
          filters: |
            performance:
              - 'src/enrichers/**'
              - 'src/assets/**'
              - 'src/utils/performance_monitor.py'
              - 'src/extractors/**'
              - 'scripts/performance/benchmark_enrichment.py'
              - 'scripts/performance/detect_performance_regression.py'
              - 'config/base.yaml'
            cet:
              - 'src/ml/config/**'
              - 'src/assets/jobs/cet_*.py'
              - 'tests/unit/ml/**'
              - 'tests/integration/ml/**'
              - 'tests/e2e/ml/**'
              - 'config/cet/**'
              - 'pyproject.toml'
              - 'uv.lock'
            transition:
              - 'src/assets/transition/**'
              - 'src/assets/jobs/transition_job.py'
              - 'tests/e2e/transition/**'
              - 'tests/integration/transition/**'
              - 'docs/transition/**'
            docker:
              - 'Dockerfile'
              - 'docker-compose.yml'
              - '.dockerignore'
              - 'docker/**'
            docs-only:
              - '**/*.md'
              - 'docs/**'
              - '!**/*.py'
              - '!**/*.yaml'
              - '!**/*.yml'
              - '!**/*.json'
              - '!**/*.toml'
              - '!**/*.lock'
              - '!Makefile'
              - '!.github/**'

      - name: Generate workflow visualization
        run: |
          echo "## ðŸ”„ CI Workflow Execution Plan" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph TD" >> $GITHUB_STEP_SUMMARY
          echo "  A[Detect Changes] --> B[Lint]" >> $GITHUB_STEP_SUMMARY
          echo "  A --> C[Type Check]" >> $GITHUB_STEP_SUMMARY
          echo "  A --> D[Verify Setup]" >> $GITHUB_STEP_SUMMARY
          echo "  B --> E[Unit Tests]" >> $GITHUB_STEP_SUMMARY
          echo "  C --> E" >> $GITHUB_STEP_SUMMARY
          echo "  D --> E" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.detect.outputs.cet }}" = "true" ]; then
            echo "  E --> F[CET Tests]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.transition }}" = "true" ]; then
            echo "  E --> G[Transition MVP]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.performance }}" = "true" ]; then
            echo "  A --> H[Performance Check]" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ steps.detect.outputs.docker }}" = "true" ]; then
            echo "  A --> I[Container Build]" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY

  lint:
    name: Lint (Ruff)
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only != 'true')
    steps:
      - name: Setup environment
        uses: ./.github/actions/setup-environment
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run Ruff lint
        run: uv run python -m ruff check src tests

      - name: Run Ruff format check
        run: uv run python -m ruff format --check src tests

  type-check:
    name: Type Check (MyPy)
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only != 'true')
    steps:
      - name: Setup environment
        uses: ./.github/actions/setup-environment
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run MyPy type checker
        run: uv run python -m mypy src

  verify-setup-script:
    name: Verify Setup Script
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    # Skip on docs-only changes to save CI time
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only != 'true')
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Run setup script
        env:
          RPY2_CFFI_MODE: ABI  # Build rpy2 in ABI mode without requiring R
        run: |
          chmod +x scripts/setup_dev.sh
          ./scripts/setup_dev.sh

      - name: Verify venv activation
        run: |
          source .venv/bin/activate
          python --version
          python -c "import pydantic; print('Pydantic installed')"

  test:
    name: Unit Tests (Shard ${{ matrix.shard }}/4)
    needs: [detect-changes, lint, type-check]
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Reduced from 30 for faster failure
    # Skip test job on PRs if only docs changed (but always run on push to main/develop)
    # On push events, we could skip detect-changes, but we need it for docs-only filtering
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only != 'true')

    # Neo4j service removed - fast tests use mocks, integration tests start Neo4j conditionally

    env:
      # Environment variables are set by setup-test-environment action
      # Additional job-specific variables
      ENVIRONMENT: test
      SBIR_ETL__EXTRACTION__SAMPLE_LIMIT: "10000"

    strategy:
      fail-fast: false  # Run all shards even if one fails
      matrix:
        python-version: ["3.11"]
        shard: [1, 2, 3, 4]

    steps:
      - uses: actions/checkout@v6

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment
        with:
          python-version: ${{ matrix.python-version }}
          neo4j-image: ${{ env.NEO4J_IMAGE }}
          neo4j-username: ${{ env.NEO4J_USERNAME }}
          neo4j-password: ${{ env.NEO4J_PASSWORD }}
          default-timeout: ${{ env.DEFAULT_TIMEOUT }}

      - name: Display test environment
        run: echo "ðŸš€ Running fast tests shard ${{ matrix.shard }}/4 without Neo4j service (mocked)"

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "true"
          install-pyreadstat: "true"

      - name: Run taxonomy completeness checks
        # Only run on shard 1 to avoid duplication
        if: |
          matrix.shard == 1 &&
          (github.event_name == 'push' ||
           (github.event_name == 'pull_request' && needs.detect-changes.outputs.cet == 'true'))
        run: |
          echo "Running CET taxonomy checks (write checks JSON; evaluation in next step)..."
          # Run checks but don't let this step fail the job immediately; we'll evaluate and comment in the next step.
          uv run python -m src.ml.config.taxonomy_checks --output data/processed/cet_taxonomy_checks.json || true

      - name: Evaluate taxonomy checks
        # Only evaluate when checks were run
        if: |
          matrix.shard == 1 &&
          (github.event_name == 'push' ||
           (github.event_name == 'pull_request' && needs.detect-changes.outputs.cet == 'true'))
        run: uv run python scripts/ci/evaluate_taxonomy_checks.py

      - name: Verify pandas, duckdb, and pyreadstat installed
        if: matrix.shard == 1
        run: uv run python scripts/ci/verify_dependencies.py

      - name: Run fast tests (sharded)
        id: run_tests
        continue-on-error: true
        run: |
          # Use pytest-xdist with test sharding for parallel execution across jobs
          # Each shard runs 1/4 of the tests with 2 workers for optimal performance
          uv run pytest -m fast \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --shard-id=${{ matrix.shard }} \
            --num-shards=4 \
            --json-report \
            --json-report-file=test-results-shard-${{ matrix.shard }}.json \
            -n 2

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-shard-${{ matrix.shard }}
          path: test-results-shard-${{ matrix.shard }}.json
          retention-days: 7

      - name: Check test results
        if: always()
        run: |
          if [ "${{ steps.run_tests.outcome }}" != "success" ]; then
            echo "::error::Tests failed in shard ${{ matrix.shard }}"
            exit 1
          fi

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          file: ./coverage.xml
          flags: unittests-shard-${{ matrix.shard }}
          name: codecov-shard-${{ matrix.shard }}
          fail_ci_if_error: false

  test-report:
    name: Test Report Summary
    needs: [test]
    if: always() && github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    permissions:
      pull-requests: write
    steps:
      - uses: actions/checkout@v6

      - name: Download all test results
        uses: actions/download-artifact@v6
        with:
          pattern: test-results-shard-*
          path: test-results/

      - name: Aggregate test results
        run: |
          cat > /tmp/test_summary.md << 'EOF'
          ## ðŸ§ª Test Results Summary

          EOF

          TOTAL_PASSED=0
          TOTAL_FAILED=0
          TOTAL_SKIPPED=0
          FAILED_TESTS=""

          for shard in 1 2 3 4; do
            if [ -f "test-results/test-results-shard-${shard}/test-results-shard-${shard}.json" ]; then
              RESULT_FILE="test-results/test-results-shard-${shard}/test-results-shard-${shard}.json"

              PASSED=$(jq -r '.summary.passed // 0' "$RESULT_FILE")
              FAILED=$(jq -r '.summary.failed // 0' "$RESULT_FILE")
              SKIPPED=$(jq -r '.summary.skipped // 0' "$RESULT_FILE")

              TOTAL_PASSED=$((TOTAL_PASSED + PASSED))
              TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
              TOTAL_SKIPPED=$((TOTAL_SKIPPED + SKIPPED))

              if [ "$FAILED" -gt 0 ]; then
                echo "### âŒ Shard $shard Failures" >> /tmp/test_summary.md
                echo "" >> /tmp/test_summary.md
                jq -r '.tests[] | select(.outcome == "failed") | "- **\(.nodeid)**\n  ```\n  \(.call.longrepr // "No details")\n  ```"' "$RESULT_FILE" >> /tmp/test_summary.md
                echo "" >> /tmp/test_summary.md
              fi
            fi
          done

          # Add summary at top
          sed -i "3i **Total:** $TOTAL_PASSED âœ… passed, $TOTAL_FAILED âŒ failed, $TOTAL_SKIPPED â­ï¸ skipped\n" /tmp/test_summary.md

          if [ "$TOTAL_FAILED" -eq 0 ]; then
            sed -i "3i ### âœ… All Tests Passed!\n" /tmp/test_summary.md
          fi

      - name: Comment PR
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('/tmp/test_summary.md', 'utf8');

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸ§ª Test Results Summary')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: summary
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: summary
              });
            }

  container-build-test:
    name: Container Build and Test
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Reduced from 30 for faster failure
    # Optimized: Only required on push to main/develop, optional on PRs with docker changes
    # Skip on PRs without docker changes to save ~20 min
    if: |
      always() &&
      (github.event_name == 'push' ||
       (github.event_name == 'pull_request' && needs.detect-changes.outputs.docker == 'true') ||
       github.event_name == 'workflow_dispatch')
    permissions:
      contents: read
      packages: write  # For pushing cache to GHCR
    env:
      REGISTRY: ghcr.io
      IMAGE_NAME: ${{ github.repository }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx
        with:
          setup-qemu: "true"

      - name: Log in to GitHub Container Registry
        if: github.event_name == 'push'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Prepare .env for CI
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build image and load into Docker daemon
        id: build
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile
          push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          load: true
          tags: |
            sbir-analytics:ci-${{ github.sha }}
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          build-args: |
            BUILD_WITH_R=false
          cache-from: |
            type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
            type=gha,scope=ci-no-r
            type=gha,scope=main
          cache-to: type=gha,mode=max,scope=ci-no-r

      - name: Show built image
        run: docker images --format 'table {{.Repository}}:{{.Tag}}\t{{.ID}}\t{{.Size}}' | rg "sbir-analytics" || true

      - name: Smoke test entrypoint
        run: |
          set -eux
          echo "Running smoke test: validate image entrypoint responds to 'dagster --version'"
          # Validate the built image can execute the dagster CLI. The image tag matches the one we built above.
          docker run --rm "sbir-analytics:ci-${{ github.sha }}" dagster --version

      - name: Start test compose and run pytest
        env:
          # Provide the GITHUB_SHA used in the test compose overlay variable
          GITHUB_SHA: ${{ github.sha }}
        run: |
          set -eux
          # Use profile-based compose to bring up ephemeral neo4j + app (app configured to run pytest)
          docker compose --profile ci up --abort-on-container-exit --build neo4j app
        timeout-minutes: 30

      - name: Tear down test compose (cleanup)
        if: always()
        run: |
          set -eux
          docker compose --profile ci down --remove-orphans --volumes || true

      - name: Upload logs on failure
        if: failure()
        uses: ./.github/actions/upload-artifacts
        with:
          name: ci-compose-logs
          path: |
            logs
            reports || true
          if-no-files-found: ignore

  performance-check:
    name: Performance Regression Check
    needs: [detect-changes]
    # Condition: Run only when performance-sensitive files change (or workflow_dispatch)
    if: needs.detect-changes.outputs.performance == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Reduced from 30 for faster failure
    continue-on-error: true  # Don't block PRs on performance checks
    permissions:
      id-token: write        # For OIDC authentication to AWS
      contents: read         # For checking out code
      pull-requests: write   # For posting PR comments
      checks: write          # For creating check runs
    env:
      AWS_REGION: us-east-2
      S3_BUCKET: sbir-etl-production-data
      SBIR_ANALYTICS_S3_BUCKET: sbir-etl-production-data
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Configure AWS credentials
        id: aws-credentials
        continue-on-error: true  # Optional: Falls back to local data if AWS not configured
        if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
        uses: ./.github/actions/setup-aws-credentials
        with:
          role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Cache baseline benchmark
        uses: actions/cache@v4
        with:
          path: reports/benchmarks/baseline.json
          key: benchmark-baseline-${{ github.ref_name }}-${{ hashFiles('scripts/performance/**/*.py') }}
          restore-keys: |
            benchmark-baseline-${{ github.ref_name }}-
            benchmark-baseline-main-
            benchmark-baseline-

      - name: Check if baseline exists
        id: baseline_check
        run: |
          if [ -f "reports/benchmarks/baseline.json" ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "âœ“ Baseline found"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "âš  No baseline found - this is first run or cache miss"
          fi

      - name: Check if AWS credentials are available
        id: check_aws
        run: |
          # Check if AWS credentials step succeeded AND credentials actually work
          if [ "${{ steps.aws-credentials.outcome }}" == "success" ]; then
            # Verify credentials are actually usable
            if aws sts get-caller-identity >/dev/null 2>&1; then
              echo "aws_available=true" >> $GITHUB_OUTPUT
              echo "âœ“ AWS credentials configured and verified"
            else
              echo "aws_available=false" >> $GITHUB_OUTPUT
              echo "âš ï¸  AWS credentials step succeeded but credentials not usable"
            fi
          else
            echo "aws_available=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  AWS credentials step did not succeed (outcome: ${{ steps.aws-credentials.outcome }})"
          fi

      - name: Prepare test data for benchmark
        if: steps.check_aws.outputs.aws_available == 'true'
        run: |
          # Use test fixture if S3 data not available
          mkdir -p data/raw/sbir
          if ! aws s3 ls s3://${{ env.S3_BUCKET }}/data/raw/sbir/award_data.csv >/dev/null 2>&1; then
            echo "âš ï¸  S3 data not available, using test fixture"
            cp tests/fixtures/sbir_sample.csv data/raw/sbir/award_data.csv
          fi

      - name: Run Performance Regression Detection
        id: regression
        if: steps.check_aws.outputs.aws_available == 'true'
        continue-on-error: true
        run: |
          mkdir -p reports/benchmarks
          uv run python scripts/performance/detect_performance_regression.py \
            --sample-size ${{ env.PERFORMANCE_SAMPLE_SIZE }} \
            --output-json /tmp/regression.json \
            --output-markdown /tmp/report.md \
            --output-html /tmp/report.html \
            --output-github-comment /tmp/pr_comment.md \
            --fail-on-regression

      - name: Generate summary for first run
        if: steps.baseline_check.outputs.baseline_exists == 'false'
        run: |
          echo "## ðŸ“Š Performance Benchmark - Baseline Established" >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          echo "This is the first performance run. Baseline has been established." >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          if [ -f "/tmp/regression.json" ]; then
            echo "**Current Metrics:**" >> /tmp/pr_comment.md
            echo "" >> /tmp/pr_comment.md
            jq '.current_metrics | to_entries | .[] | "- **\(.key):** \(.value)"' /tmp/regression.json >> /tmp/pr_comment.md
          fi

      - name: Create baseline if needed
        if: steps.baseline_check.outputs.baseline_exists == 'false' && success()
        run: |
          # Ensure test data is available
          mkdir -p data/raw/sbir
          if [ ! -f "data/raw/sbir/award_data.csv" ]; then
            echo "Using test fixture for baseline"
            cp tests/fixtures/sbir_sample.csv data/raw/sbir/award_data.csv
          fi
          export PYTHONPATH="${PYTHONPATH}:${GITHUB_WORKSPACE}"
          uv run python scripts/performance/benchmark_enrichment.py \
            --sample-size ${{ env.PERFORMANCE_SAMPLE_SIZE }} \
            --save-as-baseline

      - name: Upload reports
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: performance-reports
          path: |
            /tmp/regression.json
            /tmp/report.md
            /tmp/report.html
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Parse regression results
        id: parse_results
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity // "UNKNOWN"' /tmp/regression.json)
            DURATION_DELTA=$(jq -r '.duration_delta_percent // 0' /tmp/regression.json)
            MEMORY_DELTA=$(jq -r '.memory_delta_percent // 0' /tmp/regression.json)
            MATCH_RATE=$(jq -r '.match_rate // 0' /tmp/regression.json)
            echo "severity=$SEVERITY" >> $GITHUB_OUTPUT
            echo "duration_delta=$DURATION_DELTA" >> $GITHUB_OUTPUT
            echo "memory_delta=$MEMORY_DELTA" >> $GITHUB_OUTPUT
            echo "match_rate=$MATCH_RATE" >> $GITHUB_OUTPUT
          fi

      - name: Set check status - Success
        if: steps.regression.outcome == 'success' && github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'success',
              output: {
                title: 'âœ“ Performance Check Passed',
                summary: 'No performance regressions detected',
                text: 'All performance metrics are within acceptable thresholds'
              }
            });

      - name: Set check status - Failure
        if: steps.regression.outcome == 'failure' && github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'failure',
              output: {
                title: 'âŒ Performance Check Failed',
                summary: 'Performance regression detected',
                text: 'Review the PR comment and artifacts for detailed regression analysis'
              }
            });

      - name: Comment on PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            let comment_body = '';

            // Try to read generated comment
            if (fs.existsSync('/tmp/pr_comment.md')) {
              comment_body = fs.readFileSync('/tmp/pr_comment.md', 'utf8');
            } else {
              comment_body = '## âš ï¸ Performance Check\n\nNo report generated. Please check the workflow logs.';
            }

            // Add alert summary if regression detected
            if (fs.existsSync('/tmp/regression.json')) {
              const results = JSON.parse(fs.readFileSync('/tmp/regression.json', 'utf8'));
              if (results.severity === 'FAILURE' || results.severity === 'WARNING') {
                comment_body += '\n\n## ðŸš¨ Performance Alerts\n\n';
                if (results.issues && results.issues.length > 0) {
                  results.issues.forEach(issue => {
                    comment_body += `- **${issue.alert_type}:** ${issue.message}\n`;
                  });
                }
              }
            }

            // Add link to artifacts
            comment_body += '\n\n---\n\n';
            comment_body += '**ðŸ“Ž Artifacts:** [View Performance Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment_body
            });

      - name: Fail if FAILURE severity detected
        if: steps.regression.outcome == 'failure'
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity' /tmp/regression.json)
            if [ "$SEVERITY" = "FAILURE" ]; then
              echo "âŒ Performance regression FAILURE detected"
              echo ""
              echo "## Alert Summary"
              jq '.issues' /tmp/regression.json
              echo ""
              echo "Regression thresholds exceeded. Please review and address before merging."
              exit 1
            fi
          fi

      - name: Print regression report
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            echo "## Regression Detection Summary"
            echo ""
            jq '.' /tmp/regression.json
          fi

  cet-tests:
    name: CET Tests (Smoke Only on PR)
    needs: [detect-changes]
    # Optimized: Only run smoke tests on PR, full pipeline on push (via cet-dev-e2e)
    # This saves ~15 min on PRs by skipping pipeline test
    if: |
      github.event_name == 'pull_request' &&
      needs.detect-changes.outputs.cet == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Reduced from 30 since only running smoke

    strategy:
      matrix:
        test-type: [smoke]  # Only smoke tests on PR, pipeline runs on push via cet-dev-e2e

    services:
      neo4j:
        image: neo4j:5
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/password
        options: >-
          --health-cmd="bash -c 'cypher-shell -u neo4j -p password \"RETURN 1\" || exit 1'"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=12

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          neo4j-image: ${{ env.NEO4J_IMAGE }}
          neo4j-username: ${{ env.NEO4J_USERNAME }}
          neo4j-password: ${{ env.NEO4J_PASSWORD }}
          default-timeout: ${{ env.DEFAULT_TIMEOUT }}

      - name: Wait for Neo4j
        uses: ./.github/actions/wait-for-neo4j
        with:
          method: "tcp"
          port: "7687"
          timeout: "120"

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "false"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Prepare CET configs and test data
        run: |
          set -eux
          mkdir -p config/cet data/processed

          # Generate taxonomy (pipeline needs 2 areas, smoke needs 1)
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
            - cet_id: quantum_information_science
              name: Quantum Information Science
              definition: Quantum computing and algorithms
              keywords: ["quantum", "qubit", "entanglement"]
              parent_cet_id: null
          YAML
          else
            cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
          YAML
          fi

          # Generate classification config
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > config/cet/classification.yaml << 'YAML'
          model_version: vtest
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 256}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          YAML
          else
            cat > config/cet/classification.yaml << 'YAML'
          model_version: "vtest"
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 64}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          tfidf:
            min_df: 1
            max_df: 1.0
            ngram_range: [1, 2]
            keyword_boost_factor: 2.0
          feature_selection: {enabled: false}
          calibration: {method: "sigmoid", cv: 2}
          YAML
          fi

          # Generate test data
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          {"award_id":"award_002","title":"Quantum sensor","abstract":"Research on qubit coherence and entanglement for sensors.","keywords":["quantum","qubit"]}
          NDJSON
          else
            cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"smoke_award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          NDJSON
          fi

      - name: Setup Neo4j environment variables
        uses: ./.github/actions/setup-neo4j-service
        with:
          username: ${{ env.NEO4J_USERNAME }}
          password: ${{ env.NEO4J_PASSWORD }}
          auth-mode: password
          uri: ${{ env.NEO4J_URI }}

      - name: Run CET pipeline test
        if: matrix.test-type == 'pipeline'
        env:
          SBIR_ETL__CET__NEO4J_OUTPUT_DIR: data/loaded/neo4j
        run: |
          set -eux
          uv run dagster job execute -m src.definitions -j cet_full_pipeline_job

      - name: Run CET smoke test
        if: matrix.test-type == 'smoke'
        run: |
          set -eux
          uv run python scripts/ci/cet_smoke_test.py

      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" \) -print || true

      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-${{ matrix.test-type }}-artifacts
          retention-days: ${{ env.CET_ARTIFACT_RETENTION_DAYS }}
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/cet_*.*
            data/processed/*.checks.json
            data/loaded/neo4j/*.checks.json
          if-no-files-found: warn

      - name: Success note
        if: success()
        run: echo "CET ${{ matrix.test-type }} test completed successfully"

  cet-dev-e2e:
    name: CET Dev E2E (Docker Compose)
    needs: test
    # Only run on push to main/develop, skip on PRs
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'push'
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Reduced from 45 for faster failure
    env:
      # Allow override via repository secrets; fall back to defaults for dev runs
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: ${{ secrets.NEO4J_USER || 'neo4j' }}
      NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD || 'password' }}
      IMAGE_NAME: sbir-analytics
      IMAGE_TAG: dev-${{ github.sha }}
      STARTUP_TIMEOUT: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx

      - name: Prepare .env for dev compose
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build runtime image
        run: |
          set -eux
          docker build --target runtime \
            --cache-from ghcr.io/${{ github.repository }}:latest \
            -t "${IMAGE_NAME}:${IMAGE_TAG}" .

      - name: Start development compose stack (dev profile)
        run: |
          set -eux
          docker compose --env-file .env --profile dev up -d --build

      - name: Wait for Neo4j bolt port
        run: |
          set -eux
          sudo apt-get update -y
          sudo apt-get install -y --no-install-recommends netcat-openbsd
          TIMEOUT=${STARTUP_TIMEOUT}
          i=0
          until nc -z localhost 7687 || [ $i -ge $TIMEOUT ]; do
            i=$((i+5))
            echo "Waiting for Neo4j on 7687..."
            sleep 5
          done
          if ! nc -z localhost 7687; then
            echo "Neo4j did not become available on bolt port 7687"
            docker compose --env-file .env --profile dev ps || true
            docker logs $(docker ps --filter "name=sbir-neo4j" --format '{{.Names}}' | head -n 1) || true
            exit 1
          fi
          echo "Neo4j appears reachable on 7687"

      - name: Run CET full pipeline via Makefile helper
        env:
          # Ensure composer commands inside Make target use same .env
          DOCKER_BUILDKIT: 1
        run: |
          set -eux
          # Use the Make helper which runs the job inside the app container via docker compose
          make cet-pipeline-dev

      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          echo "Artifacts produced under data/ and reports/:"
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" -o -name "*.ndjson" \) -print || true
          find reports -maxdepth 4 -type f \( -name "*.json" -o -name "*.md" -o -name "*.html" \) -print || true

      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-dev-e2e-artifacts
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/*.checks.json
            data/processed/*.ndjson
            reports/analytics/**
            reports/alerts/**
            reports/benchmarks/**
          retention-days: ${{ env.CET_ARTIFACT_RETENTION_DAYS }}
          if-no-files-found: warn

      - name: Tear down dev compose stack (always)
        if: always()
        run: |
          set -eux
          docker compose --env-file .env --profile dev down --remove-orphans || true
          echo "Compose stack torn down"

      - name: Success note
        if: success()
        run: echo "CET Dev E2E completed successfully"

      - name: Failure diagnostics
        if: failure()
        run: |
          echo "Job failed â€” gathering diagnostic information"
          docker compose --env-file .env --profile dev ps || true
          echo "Recent container logs (if available):"
          for name in $(docker ps --format '{{.Names}}'); do
            echo "==== logs: $name ===="
            docker logs --tail 200 "$name" || true
          done

  transition-mvp:
    name: Run Transition MVP (shim) and gate on validation summary
    needs: [detect-changes]
    # Run on PR only if transition files changed, always run on push to main/develop
    if: |
      github.event_name == 'push' || needs.detect-changes.outputs.transition == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Reduced from 20 for faster failure
    permissions:
      contents: read

    concurrency:
      group: transition-mvp-${{ github.ref }}
      cancel-in-progress: true

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Run Transition MVP (shim)
        run: |
          make transition-mvp-run

      - name: Export precision audit sample (CSV)
        run: |
          uv run python scripts/transition/transition_precision_audit.py --export-csv reports/validation/vendor_resolution_audit_sample.csv

      - name: Upload precision audit sample
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-precision-audit-sample
          path: reports/validation/vendor_resolution_audit_sample.csv
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Gate on validation summary
        id: gate
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_validation.py

      - name: Gate on evidence completeness
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_evidence.py

      - name: Gate on analytics checks
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_analytics.py

      - name: Upload validation summary
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-validation
          path: |
            reports/validation/transition_mvp.json
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Upload processed artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-artifacts
          path: |
            data/processed/vendor_resolution.*
            data/processed/transitions.*
            data/processed/transitions_evidence.*
            data/processed/transition_analytics.*
            data/processed/contracts_sample.*
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

  ci-summary:
    name: CI Summary
    needs: [detect-changes, lint, type-check, verify-setup-script, test, container-build-test, performance-check, cet-tests, cet-dev-e2e, transition-mvp]
    if: always()
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Generate CI summary
        run: |
          echo "## ðŸ“Š CI Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY

          # Core jobs
          echo "| ðŸ” Detect Changes | ${{ needs.detect-changes.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.detect-changes.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ§¹ Lint | ${{ needs.lint.result == 'success' && 'âœ…' || needs.lint.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ”Ž Type Check | ${{ needs.type-check.result == 'success' && 'âœ…' || needs.type-check.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.type-check.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| âœ… Verify Setup | ${{ needs.verify-setup-script.result == 'success' && 'âœ…' || needs.verify-setup-script.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.verify-setup-script.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ§ª Unit Tests (4 shards) | ${{ needs.test.result == 'success' && 'âœ…' || needs.test.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY

          # Conditional jobs
          if [ "${{ needs.container-build-test.result }}" != "skipped" ]; then
            echo "| ðŸ³ Container Build | ${{ needs.container-build-test.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.container-build-test.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance-check.result }}" != "skipped" ]; then
            echo "| âš¡ Performance Check | ${{ needs.performance-check.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.performance-check.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.cet-tests.result }}" != "skipped" ]; then
            echo "| ðŸ¤– CET Tests | ${{ needs.cet-tests.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.cet-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.cet-dev-e2e.result }}" != "skipped" ]; then
            echo "| ðŸ”¬ CET E2E | ${{ needs.cet-dev-e2e.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.cet-dev-e2e.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.transition-mvp.result }}" != "skipped" ]; then
            echo "| ðŸ”„ Transition MVP | ${{ needs.transition-mvp.result == 'success' && 'âœ…' || 'âŒ' }} ${{ needs.transition-mvp.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Execution Flow" >> $GITHUB_STEP_SUMMARY
          echo "```mermaid" >> $GITHUB_STEP_SUMMARY
          echo "graph LR" >> $GITHUB_STEP_SUMMARY
          echo "  A[Detect Changes] --> B[Lint]" >> $GITHUB_STEP_SUMMARY
          echo "  A --> C[Type Check]" >> $GITHUB_STEP_SUMMARY
          echo "  A --> D[Verify Setup]" >> $GITHUB_STEP_SUMMARY
          echo "  B --> E[Tests]" >> $GITHUB_STEP_SUMMARY
          echo "  C --> E" >> $GITHUB_STEP_SUMMARY
          echo "  D --> E" >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY

      - name: Check for failures
        if: |
          needs.lint.result == 'failure' ||
          needs.type-check.result == 'failure' ||
          needs.test.result == 'failure' ||
          needs.container-build-test.result == 'failure' ||
          needs.performance-check.result == 'failure' ||
          needs.cet-tests.result == 'failure' ||
          needs.cet-dev-e2e.result == 'failure' ||
          needs.transition-mvp.result == 'failure'
        run: |
          echo "::error::One or more CI jobs failed"
          exit 1
