name: CI

on:
  pull_request:
    branches-ignore: []
    # Run on any PR to any branch
  push:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      use_docker:
        description: 'Use Docker Neo4j instead of Aura Free'
        required: false
        type: boolean
        default: false

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Shared configuration values
  PYTHON_VERSION: "3.11"
  NEO4J_IMAGE: "neo4j:5"
  NEO4J_USERNAME: "neo4j"
  NEO4J_PASSWORD: "password"
  DEFAULT_TIMEOUT: 30

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Run change detection on all events to enable path-based filtering
    permissions:
      contents: read
      pull-requests: read
    outputs:
      performance: ${{ steps.filter.outputs.performance }}
      cet: ${{ steps.filter.outputs.cet }}
      transition: ${{ steps.filter.outputs.transition }}
      docker: ${{ steps.filter.outputs.docker }}
      docs-only: ${{ steps.filter.outputs.docs-only }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            performance:
              - 'src/enrichers/**'
              - 'src/assets/**'
              - 'src/utils/performance_monitor.py'
              - 'src/extractors/**'
              - 'scripts/performance/benchmark_enrichment.py'
              - 'scripts/performance/detect_performance_regression.py'
              - 'config/base.yaml'
            cet:
              - 'src/**'
              - 'tests/**'
              - 'pyproject.toml'
              - 'uv.lock'
            transition:
              - 'src/assets/transition_assets.py'
              - 'tests/integration/test_transition_mvp_chain.py'
              - 'tests/unit/transition/test_transition_signals_and_boosts.py'
              - 'docs/transition/**'
              - 'Makefile'
              - '.github/workflows/**'
            docker:
              - 'Dockerfile'
              - 'docker-compose.yml'
              - '.dockerignore'
              - 'docker/**'
            docs-only:
              - '**/*.md'
              - 'docs/**'
              - '!**/*.py'
              - '!**/*.yaml'
              - '!**/*.yml'
              - '!**/*.json'
              - '!**/*.toml'
              - '!**/*.lock'
              - '!Makefile'
              - '!.github/**'

  test:
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Skip test job on PRs if only docs changed (but always run on push to main/develop)
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only != 'true')

    # Only use Neo4j service container if explicitly requested via workflow_dispatch
    services:
      neo4j:
        image: ${{ (github.event.inputs.use_docker == 'true' && 'neo4j:5') || '' }}
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: none
          NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
        options: >-
          ${{ (github.event.inputs.use_docker == 'true' && '--health-cmd "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1" --health-interval 10s --health-timeout 5s --health-retries 10 --health-start-period 40s') || '' }}

    env:
      # Use Neo4j Aura Free by default, Docker as fallback
      # Aura Free test credentials from GitHub secrets (configure in repo settings)
      NEO4J_URI: ${{ (github.event.inputs.use_docker == 'true' && 'bolt://localhost:7687') || secrets.NEO4J_AURA_TEST_URI || 'neo4j+s://test.databases.neo4j.io' }}
      NEO4J_USERNAME: ${{ (github.event.inputs.use_docker == 'true' && 'neo4j') || secrets.NEO4J_AURA_TEST_USERNAME || 'neo4j' }}
      NEO4J_PASSWORD: ${{ (github.event.inputs.use_docker == 'true' && '') || secrets.NEO4J_AURA_TEST_PASSWORD || '' }}
      SBIR_ETL__NEO4J__URI: ${{ (github.event.inputs.use_docker == 'true' && 'bolt://localhost:7687') || secrets.NEO4J_AURA_TEST_URI || 'neo4j+s://test.databases.neo4j.io' }}
      SBIR_ETL__NEO4J__USERNAME: ${{ (github.event.inputs.use_docker == 'true' && 'neo4j') || secrets.NEO4J_AURA_TEST_USERNAME || 'neo4j' }}
      SBIR_ETL__NEO4J__PASSWORD: ${{ (github.event.inputs.use_docker == 'true' && '') || secrets.NEO4J_AURA_TEST_PASSWORD || '' }}
      ENVIRONMENT: ${{ (github.event.inputs.use_docker == 'true' && 'dev') || 'test-aura' }}
      # Limit sample size for Aura Free (stays within 100K node limit)
      SBIR_ETL__EXTRACTION__SAMPLE_LIMIT: ${{ (github.event.inputs.use_docker == 'true' && '10000') || '1000' }}

    strategy:
      matrix:
        python-version: [${{ env.PYTHON_VERSION }}]

    steps:
      - uses: actions/checkout@v4

      - name: Display test environment
        run: |
          if [ "${{ github.event.inputs.use_docker }}" = "true" ]; then
            echo "üê≥ Using Docker Neo4j (local service container)"
          else
            echo "‚òÅÔ∏è  Using Neo4j Aura Free (cloud)"
            echo "URI: ${{ env.NEO4J_URI }}"
          fi

      - name: Wait for Neo4j (Docker only)
        if: github.event.inputs.use_docker == 'true'
        run: |
          echo "Waiting for Neo4j Docker service to be ready..."
          until curl -s -f -o /dev/null "http://localhost:7474"; do
            echo "Neo4j not ready, sleeping..."
            sleep 5
          done
          echo "Neo4j is ready!"

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ matrix.python-version }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "true"
          install-pyreadstat: "true"

      - name: Run taxonomy completeness checks
        # Only run taxonomy checks when CET-related files change or on push to main/develop
        if: |
          github.event_name == 'push' ||
          (github.event_name == 'pull_request' && needs.detect-changes.outputs.cet == 'true')
        run: |
          echo "Running CET taxonomy checks (write checks JSON; evaluation in next step)..."
          # Run checks but don't let this step fail the job immediately; we'll evaluate and comment in the next step.
          uv run python -m src.ml.config.taxonomy_checks --output data/processed/cet_taxonomy_checks.json || true

      - name: Evaluate taxonomy checks
        # Only evaluate when checks were run
        if: |
          github.event_name == 'push' ||
          (github.event_name == 'pull_request' && needs.detect-changes.outputs.cet == 'true')
        run: uv run python scripts/ci/evaluate_taxonomy_checks.py

      - name: Verify pandas, duckdb, and pyreadstat installed
        run: |
          uv run python - <<'PY'
          import pandas, sys
          try:
              import duckdb
              import pyreadstat
              print("pandas", pandas.__version__)
              print("duckdb", duckdb.__version__)
              print("pyreadstat", pyreadstat.__version__)
          except Exception as e:
              # Print diagnostics to help CI debugging and exit non-zero so failures are visible
              print("Dependency verification failed:", e)
              raise
          PY

      - name: Verify Neo4j connection (Aura Free)
        if: github.event.inputs.use_docker != 'true'
        continue-on-error: true
        run: |
          echo "Testing connection to Neo4j Aura Free..."
          uv run python - <<'PY'
          import os
          import sys
          try:
              from neo4j import GraphDatabase
              uri = os.environ.get("NEO4J_URI", "")
              user = os.environ.get("NEO4J_USERNAME", "neo4j")
              password = os.environ.get("NEO4J_PASSWORD", "")

              if not uri or not password:
                  print("‚ö†Ô∏è  Warning: Neo4j Aura test credentials not configured in GitHub secrets")
                  print("   Please configure NEO4J_AURA_TEST_URI, NEO4J_AURA_TEST_USERNAME, NEO4J_AURA_TEST_PASSWORD")
                  print("   See docs/CI_AURA_SETUP.md for setup instructions")
                  sys.exit(0)  # Don't fail the build, just warn

              print(f"Connecting to: {uri}")
              driver = GraphDatabase.driver(uri, auth=(user, password))
              with driver.session() as session:
                  result = session.run("RETURN 1 as test")
                  value = result.single()["test"]
                  assert value == 1
              driver.close()
              print("‚úÖ Neo4j Aura Free connection successful!")
          except Exception as e:
              print(f"‚ùå Neo4j Aura Free connection failed: {e}")
              print("   Tip: Check if your Aura instance is paused at console.neo4j.io")
              raise
          PY

      - name: Check initial node count (Aura Free)
        if: github.event.inputs.use_docker != 'true'
        continue-on-error: true
        run: |
          echo "Checking Neo4j Aura Free node count before tests..."
          uv run python - <<'PY'
          import os
          import sys
          try:
              from neo4j import GraphDatabase
              uri = os.environ.get("NEO4J_URI", "")
              user = os.environ.get("NEO4J_USERNAME", "neo4j")
              password = os.environ.get("NEO4J_PASSWORD", "")

              if not uri or not password:
                  print("‚ö†Ô∏è  Skipping node count check - no Aura credentials")
                  sys.exit(0)

              driver = GraphDatabase.driver(uri, auth=(user, password))
              with driver.session() as session:
                  result = session.run("MATCH (n) RETURN count(n) as count")
                  count = result.single()["count"]
                  print(f"üìä Current node count: {count:,}")
                  if count > 95000:
                      print(f"‚ö†Ô∏è  WARNING: Node count ({count:,}) approaching Aura Free limit (100,000)")
                      print("   Consider cleaning up test data: MATCH (n) DETACH DELETE n")
              driver.close()
          except Exception as e:
              print(f"Note: Could not check node count: {e}")
          PY

      - name: Run fast tests
        env:
          # Pass through environment variables for test configuration
          NEO4J_URI: ${{ env.NEO4J_URI }}
          NEO4J_USERNAME: ${{ env.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}
        run: |
          # Use pytest-xdist for parallelization (auto-detects CPU count)
          uv run pytest -m fast --cov=src --cov-report=xml --cov-report=term-missing -n auto

      - name: Clean up Neo4j test data (Aura Free)
        # Only cleanup if we actually used Aura (not Docker) and have credentials
        if: always() && github.event.inputs.use_docker != 'true' && env.NEO4J_URI != '' && env.NEO4J_PASSWORD != ''
        continue-on-error: true
        run: |
          echo "Cleaning up Neo4j Aura Free test data..."
          uv run python scripts/ci/cleanup_neo4j_aura.py

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  container-build-test:
    needs: [test, detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Only run when Docker files change or on push to main/develop
    if: |
      always() &&
      needs.test.result == 'success' &&
      (github.event_name == 'push' ||
       (github.event_name == 'pull_request' && needs.detect-changes.outputs.docker == 'true') ||
       github.event_name == 'workflow_dispatch')
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx
        with:
          setup-qemu: "true"

      - name: Prepare .env for CI
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build image and load into Docker daemon
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile
          push: false
          load: true
          tags: sbir-etl:ci-${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Show built image
        run: docker images --format 'table {{.Repository}}:{{.Tag}}\t{{.ID}}\t{{.Size}}' | rg "sbir-etl" || true

      - name: Smoke test entrypoint
        run: |
          set -eux
          echo "Running smoke test: validate image entrypoint responds to 'dagster --version'"
          # Validate the built image can execute the dagster CLI. The image tag matches the one we built above.
          docker run --rm "sbir-etl:ci-${{ github.sha }}" dagster --version

      - name: Start test compose and run pytest
        env:
          # Provide the GITHUB_SHA used in the test compose overlay variable
          GITHUB_SHA: ${{ github.sha }}
        run: |
          set -eux
          # Use profile-based compose to bring up ephemeral neo4j + app (app configured to run pytest)
          docker compose --profile ci up --abort-on-container-exit --build neo4j app
        timeout-minutes: 30

      - name: Tear down test compose (cleanup)
        if: always()
        run: |
          set -eux
          docker compose --profile ci down --remove-orphans --volumes || true

      - name: Upload logs on failure
        if: failure()
        uses: ./.github/actions/upload-artifacts
        with:
          name: ci-compose-logs
          path: |
            logs
            reports || true
          if-no-files-found: ignore

  performance-check:
    needs: [test, detect-changes]
    # Run on PR only if performance files changed, always run on push to main/develop (unless docs-only)
    if: |
      always() &&
      needs.test.result == 'success' &&
      (github.event_name == 'push' || needs.detect-changes.outputs.performance == 'true') &&
      !(github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 30
    continue-on-error: true
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Cache baseline benchmark
        uses: actions/cache@v4
        with:
          path: reports/benchmarks/baseline.json
          key: benchmark-baseline-main
          restore-keys: |
            benchmark-baseline-

      - name: Check if baseline exists
        id: baseline_check
        run: |
          if [ -f "reports/benchmarks/baseline.json" ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "‚úì Baseline found"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "‚ö† No baseline found - this is first run or cache miss"
          fi

      - name: Run Performance Regression Detection
        id: regression
        continue-on-error: true
        run: |
          mkdir -p reports/benchmarks
          uv run python scripts/performance/detect_performance_regression.py \
            --sample-size 500 \
            --output-json /tmp/regression.json \
            --output-markdown /tmp/report.md \
            --output-html /tmp/report.html \
            --output-github-comment /tmp/pr_comment.md \
            --fail-on-regression

      - name: Generate summary for first run
        if: steps.baseline_check.outputs.baseline_exists == 'false'
        run: |
          echo "## üìä Performance Benchmark - Baseline Established" >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          echo "This is the first performance run. Baseline has been established." >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          if [ -f "/tmp/regression.json" ]; then
            echo "**Current Metrics:**" >> /tmp/pr_comment.md
            echo "" >> /tmp/pr_comment.md
            jq '.current_metrics | to_entries | .[] | "- **\(.key):** \(.value)"' /tmp/regression.json >> /tmp/pr_comment.md
          fi

      - name: Create baseline if needed
        if: steps.baseline_check.outputs.baseline_exists == 'false' && success()
        run: |
          export PYTHONPATH="${PYTHONPATH}:${GITHUB_WORKSPACE}"
          uv run python scripts/performance/benchmark_enrichment.py \
            --sample-size 500 \
            --save-as-baseline

      - name: Upload reports
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: performance-reports
          path: |
            /tmp/regression.json
            /tmp/report.md
            /tmp/report.html
          retention-days: "7"

      - name: Parse regression results
        id: parse_results
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity // "UNKNOWN"' /tmp/regression.json)
            DURATION_DELTA=$(jq -r '.duration_delta_percent // 0' /tmp/regression.json)
            MEMORY_DELTA=$(jq -r '.memory_delta_percent // 0' /tmp/regression.json)
            MATCH_RATE=$(jq -r '.match_rate // 0' /tmp/regression.json)
            echo "severity=$SEVERITY" >> $GITHUB_OUTPUT
            echo "duration_delta=$DURATION_DELTA" >> $GITHUB_OUTPUT
            echo "memory_delta=$MEMORY_DELTA" >> $GITHUB_OUTPUT
            echo "match_rate=$MATCH_RATE" >> $GITHUB_OUTPUT
          fi

      - name: Set check status - Success
        if: steps.regression.outcome == 'success' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'success',
              output: {
                title: '‚úì Performance Check Passed',
                summary: 'No performance regressions detected',
                text: 'All performance metrics are within acceptable thresholds'
              }
            });

      - name: Set check status - Failure
        if: steps.regression.outcome == 'failure' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'failure',
              output: {
                title: '‚ùå Performance Check Failed',
                summary: 'Performance regression detected',
                text: 'Review the PR comment and artifacts for detailed regression analysis'
              }
            });

      - name: Comment on PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            let comment_body = '';

            // Try to read generated comment
            if (fs.existsSync('/tmp/pr_comment.md')) {
              comment_body = fs.readFileSync('/tmp/pr_comment.md', 'utf8');
            } else {
              comment_body = '## ‚ö†Ô∏è Performance Check\n\nNo report generated. Please check the workflow logs.';
            }

            // Add alert summary if regression detected
            if (fs.existsSync('/tmp/regression.json')) {
              const results = JSON.parse(fs.readFileSync('/tmp/regression.json', 'utf8'));
              if (results.severity === 'FAILURE' || results.severity === 'WARNING') {
                comment_body += '\n\n## üö® Performance Alerts\n\n';
                if (results.issues && results.issues.length > 0) {
                  results.issues.forEach(issue => {
                    comment_body += `- **${issue.alert_type}:** ${issue.message}\n`;
                  });
                }
              }
            }

            // Add link to artifacts
            comment_body += '\n\n---\n\n';
            comment_body += '**üìé Artifacts:** [View Performance Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment_body
            });

      - name: Fail if FAILURE severity detected
        if: steps.regression.outcome == 'failure'
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity' /tmp/regression.json)
            if [ "$SEVERITY" = "FAILURE" ]; then
              echo "‚ùå Performance regression FAILURE detected"
              echo ""
              echo "## Alert Summary"
              jq '.issues' /tmp/regression.json
              echo ""
              echo "Regression thresholds exceeded. Please review and address before merging."
              exit 1
            fi
          fi

      - name: Print regression report
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            echo "## Regression Detection Summary"
            echo ""
            jq '.' /tmp/regression.json
          fi

  cet-tests:
    needs: test
    # Run on PR only if CET files changed, skip on push to main/develop (use cet-dev-e2e instead)
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'pull_request' &&
      needs.detect-changes.outputs.cet == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        test-type: [pipeline, smoke]
    
    services:
      neo4j:
        image: ${{ env.NEO4J_IMAGE }}
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/password
        options: >-
          --health-cmd="bash -c 'cypher-shell -u neo4j -p password \"RETURN 1\" || exit 1'"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=12
    
    env:
      # Neo4j connection for CET tests
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: ${{ env.NEO4J_USERNAME }}
      NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}  # pragma: allowlist secret
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "false"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"
      
      - name: Prepare CET configs and test data
        run: |
          set -eux
          mkdir -p config/cet data/processed
          
          # Generate taxonomy (pipeline needs 2 areas, smoke needs 1)
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
            - cet_id: quantum_information_science
              name: Quantum Information Science
              definition: Quantum computing and algorithms
              keywords: ["quantum", "qubit", "entanglement"]
              parent_cet_id: null
          YAML
          else
            cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
          YAML
          fi
          
          # Generate classification config
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > config/cet/classification.yaml << 'YAML'
          model_version: vtest
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 256}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          YAML
          else
            cat > config/cet/classification.yaml << 'YAML'
          model_version: "vtest"
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 64}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          tfidf:
            min_df: 1
            max_df: 1.0
            ngram_range: [1, 2]
            keyword_boost_factor: 2.0
          feature_selection: {enabled: false}
          calibration: {method: "sigmoid", cv: 2}
          YAML
          fi
          
          # Generate test data
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          {"award_id":"award_002","title":"Quantum sensor","abstract":"Research on qubit coherence and entanglement for sensors.","keywords":["quantum","qubit"]}
          NDJSON
          else
            cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"smoke_award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          NDJSON
          fi
      
      - name: Setup Neo4j environment
        uses: ./.github/actions/setup-neo4j-service
        with:
          username: ${{ env.NEO4J_USERNAME }}
          password: ${{ env.NEO4J_PASSWORD }}
          auth-mode: password
          uri: bolt://localhost:7687
      
      - name: Wait for Neo4j service
        uses: ./.github/actions/wait-for-neo4j
        with:
          method: tcp
          port: 7687
          timeout: "120"
      
      - name: Run CET pipeline test
        if: matrix.test-type == 'pipeline'
        env:
          SBIR_ETL__CET__NEO4J_OUTPUT_DIR: data/loaded/neo4j
        run: |
          set -eux
          uv run dagster job execute -m src.definitions -j cet_full_pipeline_job
      
      - name: Run CET smoke test
        if: matrix.test-type == 'smoke'
        env:
          NEO4J_URI: ${{ env.NEO4J_URI }}
          NEO4J_USERNAME: ${{ env.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}  # pragma: allowlist secret
        run: |
          set -eux
          uv run python scripts/ci/cet_smoke_test.py
      
      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" \) -print || true
      
      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-${{ matrix.test-type }}-artifacts
          retention-days: "14"
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/cet_*.*
            data/processed/*.checks.json
            data/loaded/neo4j/*.checks.json
          if-no-files-found: warn
      
      - name: Success note
        if: success()
        run: echo "CET ${{ matrix.test-type }} test completed successfully"

  cet-dev-e2e:
    name: CET Dev E2E (dev compose)
    needs: test
    # Only run on push to main/develop, skip on PRs
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'push'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    env:
      # Allow override via repository secrets; fall back to defaults for dev runs
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: ${{ secrets.NEO4J_USER || 'neo4j' }}
      NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD || 'password' }}
      IMAGE_NAME: sbir-etl
      IMAGE_TAG: dev-${{ github.sha }}
      STARTUP_TIMEOUT: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx

      - name: Prepare .env for dev compose
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build runtime image
        run: |
          set -eux
          docker build --target runtime -t "${IMAGE_NAME}:${IMAGE_TAG}" .

      - name: Start development compose stack (dev profile)
        run: |
          set -eux
          docker compose --env-file .env --profile dev up -d --build

      - name: Wait for Neo4j bolt port
        run: |
          set -eux
          sudo apt-get update -y
          sudo apt-get install -y --no-install-recommends netcat-openbsd
          TIMEOUT=${STARTUP_TIMEOUT}
          i=0
          until nc -z localhost 7687 || [ $i -ge $TIMEOUT ]; do
            i=$((i+5))
            echo "Waiting for Neo4j on 7687..."
            sleep 5
          done
          if ! nc -z localhost 7687; then
            echo "Neo4j did not become available on bolt port 7687"
            docker compose --env-file .env --profile dev ps || true
            docker logs $(docker ps --filter "name=sbir-neo4j" --format '{{.Names}}' | head -n 1) || true
            exit 1
          fi
          echo "Neo4j appears reachable on 7687"

      - name: Run CET full pipeline via Makefile helper
        env:
          # Ensure composer commands inside Make target use same .env
          DOCKER_BUILDKIT: 1
        run: |
          set -eux
          # Use the Make helper which runs the job inside the app container via docker compose
          make cet-pipeline-dev

      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          echo "Artifacts produced under data/ and reports/:"
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" -o -name "*.ndjson" \) -print || true
          find reports -maxdepth 4 -type f \( -name "*.json" -o -name "*.md" -o -name "*.html" \) -print || true

      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-dev-e2e-artifacts
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/*.checks.json
            data/processed/*.ndjson
            reports/analytics/**
            reports/alerts/**
            reports/benchmarks/**
          retention-days: "14"
          if-no-files-found: warn

      - name: Tear down dev compose stack (always)
        if: always()
        run: |
          set -eux
          docker compose --env-file .env --profile dev down --remove-orphans || true
          echo "Compose stack torn down"

      - name: Success note
        if: success()
        run: echo "CET Dev E2E completed successfully"

      - name: Failure diagnostics
        if: failure()
        run: |
          echo "Job failed ‚Äî gathering diagnostic information"
          docker compose --env-file .env --profile dev ps || true
          echo "Recent container logs (if available):"
          for name in $(docker ps --format '{{.Names}}'); do
            echo "==== logs: $name ===="
            docker logs --tail 200 "$name" || true
          done

  transition-mvp:
    name: Run Transition MVP (shim) and gate on validation summary
    needs: test
    # Run on PR only if transition files changed, always run on push to main/develop
    if: |
      always() &&
      needs.test.result == 'success' &&
      (github.event_name == 'push' || needs.detect-changes.outputs.transition == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 20
    permissions:
      contents: read

    concurrency:
      group: transition-mvp-${{ github.ref }}
      cancel-in-progress: true

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Run Transition MVP (shim)
        run: |
          make transition-mvp-run

      - name: Export precision audit sample (CSV)
        run: |
          uv run python scripts/transition/transition_precision_audit.py --export-csv reports/validation/vendor_resolution_audit_sample.csv

      - name: Upload precision audit sample
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-precision-audit-sample
          path: reports/validation/vendor_resolution_audit_sample.csv
          if-no-files-found: warn
          retention-days: "7"

      - name: Gate on validation summary
        id: gate
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_validation.py

      - name: Gate on evidence completeness
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_evidence.py

      - name: Gate on analytics checks
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_analytics.py

      - name: Upload validation summary
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-validation
          path: |
            reports/validation/transition_mvp.json
          if-no-files-found: warn
          retention-days: "7"

      - name: Upload processed artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-artifacts
          path: |
            data/processed/vendor_resolution.*
            data/processed/transitions.*
            data/processed/transitions_evidence.*
            data/processed/transition_analytics.*
            data/processed/contracts_sample.*
          if-no-files-found: warn
          retention-days: "7"
