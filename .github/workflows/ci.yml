name: CI

# Permissions: Minimal required permissions for security
permissions:
  contents: read
  pull-requests: read
  checks: write  # For Codecov and performance checks

on:
  pull_request:
    branches-ignore: []
    # Run on any PR to any branch
  push:
    branches: [main, develop]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Shared configuration values
  PYTHON_VERSION: "3.11"  # Python version for all jobs
  NEO4J_IMAGE: "neo4j:5"  # Neo4j Docker image version
  NEO4J_USERNAME: "neo4j"  # Default Neo4j username for test containers
  NEO4J_PASSWORD: "password"  # Default Neo4j password for test containers (pragma: allowlist secret)
  DEFAULT_TIMEOUT: 30  # Default job timeout in minutes
  # Performance test configuration
  PERFORMANCE_SAMPLE_SIZE: "500"  # Sample size for performance regression tests
  PERFORMANCE_BASELINE_CACHE_KEY: "benchmark-baseline-main"  # Cache key for performance baseline
  # Test configuration
  AURA_SAMPLE_LIMIT: "1000"  # Sample limit for Aura Free (stays within 100K node limit)
  DOCKER_SAMPLE_LIMIT: "10000"  # Sample limit for Docker Neo4j (no limit)
  # Artifact retention
  DEFAULT_RETENTION_DAYS: "7"  # Default artifact retention in days
  CET_ARTIFACT_RETENTION_DAYS: "14"  # CET artifacts retained longer for analysis

jobs:
  detect-changes:
    name: Detect File Changes
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Run change detection on all events to enable path-based filtering
    permissions:
      contents: read
      pull-requests: read
    outputs:
      performance: ${{ steps.filter.outputs.performance }}
      cet: ${{ steps.filter.outputs.cet }}
      transition: ${{ steps.filter.outputs.transition }}
      docker: ${{ steps.filter.outputs.docker }}
      docs-only: ${{ steps.filter.outputs.docs-only }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            performance:
              - 'src/enrichers/**'
              - 'src/assets/**'
              - 'src/utils/performance_monitor.py'
              - 'src/extractors/**'
              - 'scripts/performance/benchmark_enrichment.py'
              - 'scripts/performance/detect_performance_regression.py'
              - 'config/base.yaml'
            cet:
              - 'src/**'
              - 'tests/**'
              - 'pyproject.toml'
              - 'uv.lock'
            transition:
              - 'src/assets/transition/**'
              - 'src/assets/jobs/transition_job.py'
              - 'tests/e2e/transition/**'
              - 'tests/integration/transition/**'
              - 'docs/transition/**'
            docker:
              - 'Dockerfile'
              - 'docker-compose.yml'
              - '.dockerignore'
              - 'docker/**'
            docs-only:
              - '**/*.md'
              - 'docs/**'
              - '!**/*.py'
              - '!**/*.yaml'
              - '!**/*.yml'
              - '!**/*.json'
              - '!**/*.toml'
              - '!**/*.lock'
              - '!Makefile'
              - '!.github/**'

  test:
    name: Unit Tests
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Skip test job on PRs if only docs changed (but always run on push to main/develop)
    # On push events, we could skip detect-changes, but we need it for docs-only filtering
    if: |
      github.event_name == 'push' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && needs.detect-changes.outputs.docs-only != 'true')

    services:
      neo4j:
        image: neo4j:5
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/password
          NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 40s

    env:
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: neo4j
      NEO4J_PASSWORD: password
      SBIR_ETL__NEO4J__URI: bolt://localhost:7687
      SBIR_ETL__NEO4J__USERNAME: neo4j
      SBIR_ETL__NEO4J__PASSWORD: password
      ENVIRONMENT: dev
      SBIR_ETL__EXTRACTION__SAMPLE_LIMIT: "10000"

    strategy:
      matrix:
        python-version: ["3.11"]

    steps:
      - uses: actions/checkout@v4

      - name: Display test environment
        run: echo "ðŸ³ Using Dockerized Neo4j service for tests"

      - name: Wait for Neo4j
        run: |
          echo "Waiting for Neo4j service to be ready..."
          until curl -s -f -o /dev/null "http://localhost:7474"; do
            echo "Neo4j not ready, sleeping..."
            sleep 5
          done
          echo "Neo4j is ready!"

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ matrix.python-version }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "true"
          install-pyreadstat: "true"

      - name: Run taxonomy completeness checks
        # Only run taxonomy checks when CET-related files change or on push to main/develop
        if: |
          github.event_name == 'push' ||
          (github.event_name == 'pull_request' && needs.detect-changes.outputs.cet == 'true')
        run: |
          echo "Running CET taxonomy checks (write checks JSON; evaluation in next step)..."
          # Run checks but don't let this step fail the job immediately; we'll evaluate and comment in the next step.
          uv run python -m src.ml.config.taxonomy_checks --output data/processed/cet_taxonomy_checks.json || true

      - name: Evaluate taxonomy checks
        # Only evaluate when checks were run
        if: |
          github.event_name == 'push' ||
          (github.event_name == 'pull_request' && needs.detect-changes.outputs.cet == 'true')
        run: uv run python scripts/ci/evaluate_taxonomy_checks.py

      - name: Verify pandas, duckdb, and pyreadstat installed
        run: uv run python scripts/ci/verify_dependencies.py

      - name: Run fast tests
        env:
          # Pass through environment variables for test configuration
          NEO4J_URI: ${{ env.NEO4J_URI }}
          NEO4J_USERNAME: neo4j
          NEO4J_PASSWORD: password
        run: |
          # Use pytest-xdist for parallelization (auto-detects CPU count)
          uv run pytest -m fast --cov=src --cov-report=xml --cov-report=term-missing -n auto

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  container-build-test:
    name: Container Build and Test
    needs: [detect-changes]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Condition: Run on push, PR with docker changes, or manual dispatch
    # Note: Removed test dependency - this job can run in parallel with tests
    # The container build/test doesn't depend on unit test results
    if: |
      always() &&
      (github.event_name == 'push' ||
       (github.event_name == 'pull_request' && needs.detect-changes.outputs.docker == 'true') ||
       github.event_name == 'workflow_dispatch')
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx
        with:
          setup-qemu: "true"

      - name: Prepare .env for CI
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build image and load into Docker daemon
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile
          push: false
          load: true
          tags: sbir-analytics:ci-${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Show built image
        run: docker images --format 'table {{.Repository}}:{{.Tag}}\t{{.ID}}\t{{.Size}}' | rg "sbir-analytics" || true

      - name: Smoke test entrypoint
        run: |
          set -eux
          echo "Running smoke test: validate image entrypoint responds to 'dagster --version'"
          # Validate the built image can execute the dagster CLI. The image tag matches the one we built above.
          docker run --rm "sbir-analytics:ci-${{ github.sha }}" dagster --version

      - name: Start test compose and run pytest
        env:
          # Provide the GITHUB_SHA used in the test compose overlay variable
          GITHUB_SHA: ${{ github.sha }}
        run: |
          set -eux
          # Use profile-based compose to bring up ephemeral neo4j + app (app configured to run pytest)
          docker compose --profile ci up --abort-on-container-exit --build neo4j app
        timeout-minutes: 30

      - name: Tear down test compose (cleanup)
        if: always()
        run: |
          set -eux
          docker compose --profile ci down --remove-orphans --volumes || true

      - name: Upload logs on failure
        if: failure()
        uses: ./.github/actions/upload-artifacts
        with:
          name: ci-compose-logs
          path: |
            logs
            reports || true
          if-no-files-found: ignore

  performance-check:
    name: Performance Regression Check
    needs: [detect-changes]
    # Condition: Run only when performance-sensitive files change (or workflow_dispatch)
    if: needs.detect-changes.outputs.performance == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Cache baseline benchmark
        uses: actions/cache@v4
        with:
          path: reports/benchmarks/baseline.json
          key: benchmark-baseline-${{ github.ref_name }}-${{ hashFiles('scripts/performance/**/*.py') }}
          restore-keys: |
            benchmark-baseline-${{ github.ref_name }}-
            benchmark-baseline-main-
            benchmark-baseline-

      - name: Check if baseline exists
        id: baseline_check
        run: |
          if [ -f "reports/benchmarks/baseline.json" ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "âœ“ Baseline found"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "âš  No baseline found - this is first run or cache miss"
          fi

      - name: Run Performance Regression Detection
        id: regression
        continue-on-error: true
        run: |
          mkdir -p reports/benchmarks
          uv run python scripts/performance/detect_performance_regression.py \
            --sample-size ${{ env.PERFORMANCE_SAMPLE_SIZE }} \
            --output-json /tmp/regression.json \
            --output-markdown /tmp/report.md \
            --output-html /tmp/report.html \
            --output-github-comment /tmp/pr_comment.md \
            --fail-on-regression

      - name: Generate summary for first run
        if: steps.baseline_check.outputs.baseline_exists == 'false'
        run: |
          echo "## ðŸ“Š Performance Benchmark - Baseline Established" >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          echo "This is the first performance run. Baseline has been established." >> /tmp/pr_comment.md
          echo "" >> /tmp/pr_comment.md
          if [ -f "/tmp/regression.json" ]; then
            echo "**Current Metrics:**" >> /tmp/pr_comment.md
            echo "" >> /tmp/pr_comment.md
            jq '.current_metrics | to_entries | .[] | "- **\(.key):** \(.value)"' /tmp/regression.json >> /tmp/pr_comment.md
          fi

      - name: Create baseline if needed
        if: steps.baseline_check.outputs.baseline_exists == 'false' && success()
        run: |
          export PYTHONPATH="${PYTHONPATH}:${GITHUB_WORKSPACE}"
          uv run python scripts/performance/benchmark_enrichment.py \
            --sample-size ${{ env.PERFORMANCE_SAMPLE_SIZE }} \
            --save-as-baseline

      - name: Upload reports
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: performance-reports
          path: |
            /tmp/regression.json
            /tmp/report.md
            /tmp/report.html
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Parse regression results
        id: parse_results
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity // "UNKNOWN"' /tmp/regression.json)
            DURATION_DELTA=$(jq -r '.duration_delta_percent // 0' /tmp/regression.json)
            MEMORY_DELTA=$(jq -r '.memory_delta_percent // 0' /tmp/regression.json)
            MATCH_RATE=$(jq -r '.match_rate // 0' /tmp/regression.json)
            echo "severity=$SEVERITY" >> $GITHUB_OUTPUT
            echo "duration_delta=$DURATION_DELTA" >> $GITHUB_OUTPUT
            echo "memory_delta=$MEMORY_DELTA" >> $GITHUB_OUTPUT
            echo "match_rate=$MATCH_RATE" >> $GITHUB_OUTPUT
          fi

      - name: Set check status - Success
        if: steps.regression.outcome == 'success' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'success',
              output: {
                title: 'âœ“ Performance Check Passed',
                summary: 'No performance regressions detected',
                text: 'All performance metrics are within acceptable thresholds'
              }
            });

      - name: Set check status - Failure
        if: steps.regression.outcome == 'failure' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Performance Check',
              head_sha: context.payload.pull_request.head.sha,
              status: 'completed',
              conclusion: 'failure',
              output: {
                title: 'âŒ Performance Check Failed',
                summary: 'Performance regression detected',
                text: 'Review the PR comment and artifacts for detailed regression analysis'
              }
            });

      - name: Comment on PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            let comment_body = '';

            // Try to read generated comment
            if (fs.existsSync('/tmp/pr_comment.md')) {
              comment_body = fs.readFileSync('/tmp/pr_comment.md', 'utf8');
            } else {
              comment_body = '## âš ï¸ Performance Check\n\nNo report generated. Please check the workflow logs.';
            }

            // Add alert summary if regression detected
            if (fs.existsSync('/tmp/regression.json')) {
              const results = JSON.parse(fs.readFileSync('/tmp/regression.json', 'utf8'));
              if (results.severity === 'FAILURE' || results.severity === 'WARNING') {
                comment_body += '\n\n## ðŸš¨ Performance Alerts\n\n';
                if (results.issues && results.issues.length > 0) {
                  results.issues.forEach(issue => {
                    comment_body += `- **${issue.alert_type}:** ${issue.message}\n`;
                  });
                }
              }
            }

            // Add link to artifacts
            comment_body += '\n\n---\n\n';
            comment_body += '**ðŸ“Ž Artifacts:** [View Performance Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment_body
            });

      - name: Fail if FAILURE severity detected
        if: steps.regression.outcome == 'failure'
        run: |
          if [ -f "/tmp/regression.json" ]; then
            SEVERITY=$(jq -r '.severity' /tmp/regression.json)
            if [ "$SEVERITY" = "FAILURE" ]; then
              echo "âŒ Performance regression FAILURE detected"
              echo ""
              echo "## Alert Summary"
              jq '.issues' /tmp/regression.json
              echo ""
              echo "Regression thresholds exceeded. Please review and address before merging."
              exit 1
            fi
          fi

      - name: Print regression report
        if: always()
        run: |
          if [ -f "/tmp/regression.json" ]; then
            echo "## Regression Detection Summary"
            echo ""
            jq '.' /tmp/regression.json
          fi

  cet-tests:
    name: CET Tests (Pipeline & Smoke)
    needs: test
    # Condition: PR only, test passed, CET files changed
    # Note: On push events, use cet-dev-e2e job instead
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'pull_request' &&
      needs.detect-changes.outputs.cet == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        test-type: [pipeline, smoke]
    
    services:
      neo4j:
        image: neo4j:5
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/password
        options: >-
          --health-cmd="bash -c 'cypher-shell -u neo4j -p password \"RETURN 1\" || exit 1'"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=12
    
    env:
      # Neo4j connection for CET tests
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: neo4j
      NEO4J_PASSWORD: password  # pragma: allowlist secret
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "false"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"
      
      - name: Prepare CET configs and test data
        run: |
          set -eux
          mkdir -p config/cet data/processed
          
          # Generate taxonomy (pipeline needs 2 areas, smoke needs 1)
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
            - cet_id: quantum_information_science
              name: Quantum Information Science
              definition: Quantum computing and algorithms
              keywords: ["quantum", "qubit", "entanglement"]
              parent_cet_id: null
          YAML
          else
            cat > config/cet/taxonomy.yaml << 'YAML'
          version: "TEST-2025Q1"
          last_updated: "2025-01-01"
          cet_areas:
            - cet_id: artificial_intelligence
              name: Artificial Intelligence
              definition: AI and ML technologies
              keywords: ["machine learning", "neural network"]
              parent_cet_id: null
          YAML
          fi
          
          # Generate classification config
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > config/cet/classification.yaml << 'YAML'
          model_version: vtest
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 256}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          YAML
          else
            cat > config/cet/classification.yaml << 'YAML'
          model_version: "vtest"
          created_date: "2025-01-01"
          confidence_thresholds: {high: 70.0, medium: 40.0, low: 0.0}
          batch: {size: 64}
          evidence: {max_statements: 2, excerpt_max_words: 40}
          tfidf:
            min_df: 1
            max_df: 1.0
            ngram_range: [1, 2]
            keyword_boost_factor: 2.0
          feature_selection: {enabled: false}
          calibration: {method: "sigmoid", cv: 2}
          YAML
          fi
          
          # Generate test data
          if [ "${{ matrix.test-type }}" = "pipeline" ]; then
            cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          {"award_id":"award_002","title":"Quantum sensor","abstract":"Research on qubit coherence and entanglement for sensors.","keywords":["quantum","qubit"]}
          NDJSON
          else
            cat > data/processed/enriched_sbir_awards.ndjson << 'NDJSON'
          {"award_id":"smoke_award_001","title":"ML imaging","abstract":"This project uses machine learning for image classification.","keywords":["machine learning","imaging"]}
          NDJSON
          fi
      
      - name: Setup Neo4j environment
        uses: ./.github/actions/setup-neo4j-service
        with:
          username: neo4j
          password: password
          auth-mode: password
          uri: bolt://localhost:7687
      
      - name: Wait for Neo4j service
        uses: ./.github/actions/wait-for-neo4j
        with:
          method: tcp
          port: 7687
          timeout: "120"
      
      - name: Run CET pipeline test
        if: matrix.test-type == 'pipeline'
        env:
          SBIR_ETL__CET__NEO4J_OUTPUT_DIR: data/loaded/neo4j
        run: |
          set -eux
          uv run dagster job execute -m src.definitions -j cet_full_pipeline_job
      
      - name: Run CET smoke test
        if: matrix.test-type == 'smoke'
        env:
          NEO4J_URI: ${{ env.NEO4J_URI }}
          NEO4J_USERNAME: neo4j
          NEO4J_PASSWORD: password  # pragma: allowlist secret
        run: |
          set -eux
          uv run python scripts/ci/cet_smoke_test.py
      
      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" \) -print || true
      
      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-${{ matrix.test-type }}-artifacts
          retention-days: ${{ env.CET_ARTIFACT_RETENTION_DAYS }}
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/cet_*.*
            data/processed/*.checks.json
            data/loaded/neo4j/*.checks.json
          if-no-files-found: warn
      
      - name: Success note
        if: success()
        run: echo "CET ${{ matrix.test-type }} test completed successfully"

  cet-dev-e2e:
    name: CET Dev E2E (Docker Compose)
    needs: test
    # Only run on push to main/develop, skip on PRs
    if: |
      always() &&
      needs.test.result == 'success' &&
      github.event_name == 'push'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    env:
      # Allow override via repository secrets; fall back to defaults for dev runs
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: ${{ secrets.NEO4J_USER || 'neo4j' }}
      NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD || 'password' }}
      IMAGE_NAME: sbir-analytics
      IMAGE_TAG: dev-${{ github.sha }}
      STARTUP_TIMEOUT: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Docker Buildx
        uses: ./.github/actions/setup-docker-buildx

      - name: Prepare .env for dev compose
        uses: ./.github/actions/prepare-env-file
        with:
          neo4j-user: ${{ secrets.NEO4J_USER }}
          neo4j-password: ${{ secrets.NEO4J_PASSWORD }}

      - name: Build runtime image
        run: |
          set -eux
          docker build --target runtime -t "${IMAGE_NAME}:${IMAGE_TAG}" .

      - name: Start development compose stack (dev profile)
        run: |
          set -eux
          docker compose --env-file .env --profile dev up -d --build

      - name: Wait for Neo4j bolt port
        run: |
          set -eux
          sudo apt-get update -y
          sudo apt-get install -y --no-install-recommends netcat-openbsd
          TIMEOUT=${STARTUP_TIMEOUT}
          i=0
          until nc -z localhost 7687 || [ $i -ge $TIMEOUT ]; do
            i=$((i+5))
            echo "Waiting for Neo4j on 7687..."
            sleep 5
          done
          if ! nc -z localhost 7687; then
            echo "Neo4j did not become available on bolt port 7687"
            docker compose --env-file .env --profile dev ps || true
            docker logs $(docker ps --filter "name=sbir-neo4j" --format '{{.Names}}' | head -n 1) || true
            exit 1
          fi
          echo "Neo4j appears reachable on 7687"

      - name: Run CET full pipeline via Makefile helper
        env:
          # Ensure composer commands inside Make target use same .env
          DOCKER_BUILDKIT: 1
        run: |
          set -eux
          # Use the Make helper which runs the job inside the app container via docker compose
          make cet-pipeline-dev

      - name: List produced artifacts
        if: always()
        run: |
          set -eux
          echo "Artifacts produced under data/ and reports/:"
          find data -maxdepth 4 -type f \( -name "*.json" -o -name "*.parquet" -o -name "*.checks.json" -o -name "*.ndjson" \) -print || true
          find reports -maxdepth 4 -type f \( -name "*.json" -o -name "*.md" -o -name "*.html" \) -print || true

      - name: Upload CET artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: cet-dev-e2e-artifacts
          path: |
            data/processed/cet_*.parquet
            data/processed/cet_*.json
            data/processed/*.checks.json
            data/processed/*.ndjson
            reports/analytics/**
            reports/alerts/**
            reports/benchmarks/**
          retention-days: ${{ env.CET_ARTIFACT_RETENTION_DAYS }}
          if-no-files-found: warn

      - name: Tear down dev compose stack (always)
        if: always()
        run: |
          set -eux
          docker compose --env-file .env --profile dev down --remove-orphans || true
          echo "Compose stack torn down"

      - name: Success note
        if: success()
        run: echo "CET Dev E2E completed successfully"

      - name: Failure diagnostics
        if: failure()
        run: |
          echo "Job failed â€” gathering diagnostic information"
          docker compose --env-file .env --profile dev ps || true
          echo "Recent container logs (if available):"
          for name in $(docker ps --format '{{.Names}}'); do
            echo "==== logs: $name ===="
            docker logs --tail 200 "$name" || true
          done

  transition-mvp:
    name: Run Transition MVP (shim) and gate on validation summary
    needs: test
    # Run on PR only if transition files changed, always run on push to main/develop
    if: |
      always() &&
      needs.test.result == 'success' &&
      (github.event_name == 'push' || needs.detect-changes.outputs.transition == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 20
    permissions:
      contents: read

    concurrency:
      group: transition-mvp-${{ github.ref }}
      cancel-in-progress: true

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: "3.11"
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "false"

      - name: Run Transition MVP (shim)
        run: |
          make transition-mvp-run

      - name: Export precision audit sample (CSV)
        run: |
          uv run python scripts/transition/transition_precision_audit.py --export-csv reports/validation/vendor_resolution_audit_sample.csv

      - name: Upload precision audit sample
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-precision-audit-sample
          path: reports/validation/vendor_resolution_audit_sample.csv
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Gate on validation summary
        id: gate
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_validation.py

      - name: Gate on evidence completeness
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_evidence.py

      - name: Gate on analytics checks
        run: |
          set -euo pipefail
          uv run python scripts/ci/gate_transition_analytics.py

      - name: Upload validation summary
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-validation
          path: |
            reports/validation/transition_mvp.json
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}

      - name: Upload processed artifacts
        if: always()
        uses: ./.github/actions/upload-artifacts
        with:
          name: transition-mvp-artifacts
          path: |
            data/processed/vendor_resolution.*
            data/processed/transitions.*
            data/processed/transitions_evidence.*
            data/processed/transition_analytics.*
            data/processed/contracts_sample.*
          if-no-files-found: warn
          retention-days: ${{ env.DEFAULT_RETENTION_DAYS }}
