name: Weekly Comprehensive Tests

on:
  schedule:
    - cron: "0 2 * * 0"  # Sunday 2 AM UTC
  workflow_dispatch:

permissions:
  contents: read
  checks: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  NEO4J_IMAGE: "neo4j:5"
  NEO4J_USERNAME: "neo4j"
  NEO4J_PASSWORD: "password"  # pragma: allowlist secret

jobs:
  comprehensive-tests:
    name: Comprehensive Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"
          cache-pytest: "false"
          install-pyreadstat: "true"

      - name: Setup Neo4j service
        uses: ./.github/actions/setup-neo4j-service
        with:
          neo4j-image: ${{ env.NEO4J_IMAGE }}
          neo4j-username: ${{ env.NEO4J_USERNAME }}
          neo4j-password: ${{ env.NEO4J_PASSWORD }}

      - name: Wait for Neo4j
        uses: ./.github/actions/wait-for-neo4j
        with:
          neo4j-uri: "bolt://localhost:7687"
          neo4j-username: ${{ env.NEO4J_USERNAME }}
          neo4j-password: ${{ env.NEO4J_PASSWORD }}
          timeout-seconds: "60"

      - name: Run comprehensive test suite
        env:
          NEO4J_URI: "bolt://localhost:7687"
          NEO4J_USER: ${{ env.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}
          USE_REAL_SBIR_DATA: "1"
        run: |
          uv run pytest \
            -m "e2e or real_data or weekly" \
            --cov=src \
            --cov-report=html \
            --cov-report=xml \
            --cov-report=term \
            -n auto \
            --timeout=300 \
            -v
        timeout-minutes: 90

      - name: Upload coverage reports
        if: always()
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: weekly-comprehensive
          name: weekly-comprehensive-coverage

      - name: Upload HTML coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: weekly-coverage-report
          path: htmlcov/
          retention-days: 30

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: weekly-test-results
          path: |
            test-results/
            *.log
          retention-days: 30

  real-data-validation:
    name: Real Data Validation
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"
          install-pyreadstat: "true"

      - name: Run validation scripts
        run: |
          # Run taxonomy completeness checks
          uv run python tests/validation/test_categorization_validation.py

      - name: Upload validation reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-reports
          path: |
            validation-reports/
            *.csv
          retention-days: 30

  performance-profiling:
    name: Performance Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python and UV
        uses: ./.github/actions/setup-python-uv
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          install-dev-deps: "true"
          cache-venv: "true"

      - name: Run performance benchmarks
        run: |
          uv run pytest \
            -m "slow" \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            -v

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks
          path: benchmark-results.json
          retention-days: 90
